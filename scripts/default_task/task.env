# From speechless-code-mistral-7b-v1.0
# -------------------- Model --------------------
export MODELS_ROOT_DIR=/opt/local/llm_models/huggingface.co
# FIXME
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/mistralai/Mistral-7B-v0.1
export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/llm_agents/tora-code-7b-v1.0
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-tora-code-7b-v1.0
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-code-mistral-7b-v1.0

# FIXME
export MAX_TRAIN_SAMPLES=0
export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/$(basename ${PWD})-train${MAX_TRAIN_SAMPLES}
# export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/$(basename ${PWD})

# -------------------- Dataset --------------------
# FIXME
# export DATASET=/opt/local/datasets/jondurbin/airoboros-2.2/instructions-clean.jsonl
# export DATASET=/opt/local/datasets/Speechless/airoboros-orca-platypus-instructions.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-spider.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-thoughts-200k.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-reasoning-v0.1.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-reasoning-v0.2.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-agents-v0.2.jsonl
export DATASET=/opt/local/datasets/Speechless/speechless-coding-v0.2.jsonl

# -------------------- Environment --------------------
export OUTPUT_DIR=./outputs
# export TORCH_DISTRIBUTED_DEBUG=DETAIL
export RAY_memory_monitor_refresh_ms=0

# -------------------- Task --------------------
# FIXME
export TASK_NAME=$(basename ${TEST_MODEL_PATH})
export TASK_CHECKPOINT_DIR=${OUTPUT_DIR}
export WANDB_PROJECT=${TASK_NAME}

export SAVE_STEPS=10
export EVAL_STEPS=10
export WARMUP_STEPS=10
export MAX_EVAL_SAMPLES=200
export EVAL_DATASET_SIZE=0.005
export GROUP_BY_LENGTH=False

#export DEEPSPEED_STAGE2="--deepspeed deepspeed-stage2.json"

export LR_SCHEDULER_TYPE=cosine
# export LR_SCHEDULER_TYPE=constant_with_warmup

# export DATASET_FORMAT=instruction-input-response
export DATASET_FORMAT=sharegpt
export LEARNING_RATE=2e-4
# export LEARNING_RATE=2e-4

# export LORA_R=64
# export LORA_ALPHA=256
export LORA_R=32
export LORA_ALPHA=256

# Mistral
# export MODEL_MAX_LENGTH=32768
# export ROPE_THETA=10000
# export SLIDING_WINDOW=4096

# Tora
# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

export MODEL_MAX_LENGTH=32768
export ROPE_THETA=1000000
export SLIDING_WINDOW=16384

export NUM_GPUS=4
export NUM_TRAIN_EPOCHS=5

export SAVE_STRATEGY=epoch
export SAVE_TOTAL_LIMIT="--save_total_limit ${NUM_TRAIN_EPOCHS}"

# export DEEPSEED="--deepspeed deepspeed-stage2.json"
# export NEFTUNE="--neftune --noise_alpha 5.0"

# export MISC_PARAMS="--long_lora True"

export PER_DEVICE_TRAIN_BATCH_SIZE=2
export GRADIENT_ACCUMULATION_STEPS=16

# No more than 85% VRAM.
# A100(40GB) 32000, A40(48GB) 40000, A100(80GB) 70000
export MAX_MEMORY_MB=32000