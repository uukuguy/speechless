# From speechless-code-mistral-7b-v1.0
# -------------------- Model --------------------
export MODELS_ROOT_DIR=/opt/local/llm_models/huggingface.co
# FIXME
export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/mistralai/Mistral-7B-v0.1
# FIXME
export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-code-mistral-7b-v1.0
# export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-nl2sql-mistral-7b-v0.1
# export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-mistral-7b-v0.1

# -------------------- Dataset --------------------
# FIXME
# export DATASET=/opt/local/datasets/jondurbin/airoboros-2.2/instructions-clean.jsonl
# export DATASET=/opt/local/datasets/Speechless/airoboros-orca-platypus-instructions.jsonl
# export DATASET=/opt/local/datasets/Speechless/speechless-spider.jsonl
export DATASET=/opt/local/datasets/Speechless/speechless-thoughts-200k.jsonl

# -------------------- Environment --------------------
export OUTPUT_DIR=./outputs
# export TORCH_DISTRIBUTED_DEBUG=DETAIL
export RAY_memory_monitor_refresh_ms=0

# -------------------- Task --------------------
# FIXME
export TASK_NAME=$(basename ${TEST_MODEL_PATH})
export TASK_CHECKPOINT_DIR=${OUTPUT_DIR}
export WANDB_PROJECT=${TASK_NAME}

# -------------------- Task --------------------
export SAVE_STEPS=100
export EVAL_STEPS=100
export WARMUP_STEPS=20
export MAX_EVAL_SAMPLES=200
export EVAL_DATASET_SIZE=0.005
export GROUP_BY_LENGTH=False

#export DEEPSPEED_STAGE2="--deepspeed deepspeed-stage2.json"

# export LR_SCHEDULER_TYPE=cosine
export LR_SCHEDULER_TYPE=constant_with_warmup

export DATASET_FORMAT=instruction-input-response
export MODEL_MAX_LENGTH=4096
export LEARNING_RATE=2e-5
export LORA_R=64
export NUM_GPUS=2
export NUM_TRAIN_EPOCHS=1
# 40GB: 2 x 32, 80GB: 4 x 16
export PER_DEVICE_TRAIN_BATCH_SIZE=8
export GRADIENT_ACCUMULATION_STEPS=8

# No more than 85% VRAM.
# A100(40GB) 32000, A40(48GB) 40000, A100(80GB) 70000
export MAX_MEMORY_MB=40000