{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# speechless.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ROOT_DIR=\"/opt/local/llm_models\"\n",
    "# MODEL_PATH=os.path.join(MODEL_ROOT_DIR, \"huggingface.co/unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit\")\n",
    "# MODEL_PATH=os.path.join(MODEL_ROOT_DIR, \"huggingface.co/nvidia/Llama-3.1-Minitron-4B-Width-Base\")\n",
    "MODEL_PATH=os.path.join(MODEL_ROOT_DIR, \"huggingface.co/Qwen/Qwen2-7B\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:00<00:00, 15.46it/s]\n",
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs={'input_ids': tensor([[  840, 20772,   279,  7286,   315,  5662,  6832,   304,  4285,  3793,\n",
      "            25]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "outputs=tensor([[   840,  20772,    279,   7286,    315,   5662,   6832,    304,   4285,\n",
      "           3793,     25,  12960,   6832,    374,    264,   1616,    369,  18495,\n",
      "            311,   3960,    323,   7269,    504,   3139,   2041,   1660,  20975,\n",
      "          55068,     13,   1084,    594,   1075,  12629,    264,   1682,    311,\n",
      "          15282,   6171,    553,   9027,   1105,   1657,  10295,     13,    576,\n",
      "           6366,  46210,    504,    821,     11,  24588,  12624,    323,   3259,\n",
      "          11181,    476,  19898,   3118,    389,    429,    821,     13,   1084,\n",
      "            594,    264,   1376,   5440,   4815,   1657,   8357,    582,    990,\n",
      "           3351,     11,   1075,  27682,   5942,     11,  25328,  13406,     11,\n",
      "            323,    656,  59711,   9331,     13, 151643]])\n",
      "plain the concept of machine learning in simple terms: Machine learning is a way for computers to learn and improve from experience without being explicitly programmed. It's like teaching a child to recognize objects by showing them many examples. The computer learns from data, identifying patterns and making decisions or predictions based on that data. It's a key technology behind many applications we use today, like recommendation systems, spam filters, and self-driving cars.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "bnb_4bit_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\"\n",
    ") \n",
    "\n",
    "bnb_config = None # bnb_4bit_config\n",
    "\n",
    "model_kwargs = {\n",
    "    \"quantization_config\": bnb_config,\n",
    "    \"torch_dtype\": torch.bfloat16,\n",
    "    \"trust_remote_code\": True,\n",
    "}\n",
    "model_kwargs[\"attn_implementation\"] = \"flash_attention_2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, **model_kwargs) \n",
    "\n",
    "generate_kwargs = {\n",
    "    \"max_new_tokens\": 512,\n",
    "    \"temperature\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    # \"min_p\": 0.1,\n",
    "}\n",
    "\n",
    "# Function to generate text\n",
    "def generate_text(prompt: str, generate_kwargs: dict) -> str:\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].to(model.device)\n",
    "    attention_mask = inputs['attention_mask'].to(model.device)\n",
    "    outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, **generate_kwargs)\n",
    "    generated_text = tokenizer.decode(outputs[0][len(input_ids):], skip_special_tokens=True)\n",
    "    return generated_text\n",
    "\n",
    "prompt = \"Explain the concept of machine learning in simple terms:\"\n",
    "response = generate_text(prompt, generate_kwargs=generate_kwargs)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
