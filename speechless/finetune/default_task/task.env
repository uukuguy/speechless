# From speechless-code-mistral-7b-v1.0
# -------------------- Model --------------------
export MODELS_ROOT_DIR=/opt/local/llm_models/huggingface.co
# FIXME
export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/mistralai/Mistral-7B-v0.2-hf
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/meta-llama/Llama-2-7b-chat-hf
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/Qwen/Qwen1.5-1.8B
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/Qwen/Qwen1.5-7B
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/Qwen/Qwen1.5-MoE-A2.7B

# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/mistralai/Mistral-7B-v0.1
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/deepseek-ai/deepseek-coder-7b-base-v1.5
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/deepseek-ai/deepseek-coder-6.7b-base
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/deepseek-ai/deepseek-coder-1.3b-base
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/llm-agents/tora-code-7b-v1.0
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-tora-code-7b-v1.0
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/speechless-code-mistral-7b-v1.0

MODEL_BASENAME=$(basename ${PWD})

# FIXME
export MAX_TRAIN_SAMPLES=0
# export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/$(basename ${PWD})-train${MAX_TRAIN_SAMPLES}
export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/$(basename ${PWD})

# -------------------- Dataset --------------------
export SPEECHLESS_DATA_DIR=/opt/local/datasets/speechless_data
# FIXME
# export DATASET=${SPEECHLESS_DATA_DIR}/airoboros-orca-platypus-instructions.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-spider.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-thoughts-200k.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-reasoning-v0.1.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-reasoning-v0.2.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-agents-v0.2.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-orca.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-agents-v0.3.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-coding-v0.1.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-agent_instruct-v0.1.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-coding-v0.2.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-coding-16k.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-toolbench-v0.1.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-toolbench-multi-rounds.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-coding-8k.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-magicoder-oss-evol-dataset.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-nl2sql-18087.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-thoughts-252k.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/speechless-hermes-code-986k.jsonl
# export DATASET=${SPEECHLESS_DATA_DIR}/sggpt_data_0.71b_pt.jsonl
export DATASET=${SPEECHLESS_DATA_DIR}/GDC-0.84b.jsonl

# -------------------- Environment --------------------
export OUTPUT_DIR=./outputs
# export TORCH_DISTRIBUTED_DEBUG=DETAIL
export RAY_memory_monitor_refresh_ms=0

# -------------------- Task --------------------
# FIXME
export TASK_NAME=$(basename ${TEST_MODEL_PATH})
export TASK_CHECKPOINT_DIR=${OUTPUT_DIR}
export WANDB_PROJECT=${TASK_NAME}

export SAVE_STEPS=100
export EVAL_STEPS=10
export WARMUP_STEPS=20
# export MAX_EVAL_SAMPLES=200
# export EVAL_DATASET_SIZE=0.0005
export MAX_EVAL_SAMPLES=1000
export EVAL_DATASET_SIZE=1000
export GROUP_BY_LENGTH=False
export LOGGING_STEPS=1

#export DEEPSPEED_STAGE2="--deepspeed deepspeed-stage2.json"

export LR_SCHEDULER_TYPE=cosine
# export LR_SCHEDULER_TYPE=constant_with_warmup

# export DATASET_FORMAT=instruction-input-response
# export PROMPT_TYPE=raw
# export DATASET_FORMAT=input-output

export DATASET_FORMAT=conversations
export PROMPT_TYPE=alpaca

# export PROMPT_TYPE=chatlm
# export PROMPT_TYPE=llama2
# export PROMPT_TYPE=minicpm

export LEARNING_RATE=2e-4

export OPTIM=paged_adamw_8bit
# export OPTIM=adafactor

export BITS=4
export LORA_R=64
export LORA_ALPHA=16
# export LORA_R=32
# export LORA_ALPHA=256

export NUM_EXPERTS=8
export TOPK=2
export ADAPTER_DIM=64

# export NUM_EXPERTS=16
# export TOPK=4
# export ADAPTER_DIM=512

# export NUM_EXPERTS=4
# export TOPK=2
# export ADAPTER_DIM=32

# Mistral
# export MODEL_MAX_LENGTH=32768
# export ROPE_THETA=10000
# export SLIDING_WINDOW=4096

# Tora
# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

export MODEL_MAX_LENGTH=2048
export ROPE_THETA=1000000
export SLIDING_WINDOW=4096

# export NUM_GPUS=2
export NUM_GPUS=4
export NUM_TRAIN_EPOCHS=3
export NUM_EARLY_STOPPING_TRAIN_EPOCHS=${NUM_TRAIN_EPOCHS}

# export SAVE_STRATEGY=epoch
export SAVE_STRATEGY=steps
export SAVE_TOTAL_LIMIT="--save_total_limit ${NUM_TRAIN_EPOCHS}"

# export DEEPSEED="--deepspeed ./deepspeed/ds_z2_config.json"
# export DEEPSEED="--deepspeed deepspeed-stage2.json"
# export NEFTUNE="--neftune --noise_alpha 5.0"

# export MISC_PARAMS="--long_lora True"

export PER_DEVICE_TRAIN_BATCH_SIZE=4
export GRADIENT_ACCUMULATION_STEPS=64

# export PER_DEVICE_TRAIN_BATCH_SIZE=4
# export GRADIENT_ACCUMULATION_STEPS=128

# export PER_DEVICE_TRAIN_BATCH_SIZE=2
# export GRADIENT_ACCUMULATION_STEPS=32

# No more than 85% VRAM.
# A100(40GB) 32000, A40(48GB) 40000, A100(80GB) 70000
export MAX_MEMORY_MB=40000
