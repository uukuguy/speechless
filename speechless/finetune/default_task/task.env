# -------------------- Model --------------------
export MODELS_ROOT_DIR=/opt/local/llm_models/huggingface.co
# FIXME
export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/Qwen/Qwen2.5-7B-Instruct
# export BASE_MODEL_PATH=${MODELS_ROOT_DIR}/Qwen/Qwen2-7B-Instruct

VLLM_PORT=12345
TENSOR_PARALLEL_SIZE=4

SGLANG_PORT=30000

MODEL_BASENAME=$(basename ${PWD})

# FIXME
export MAX_TRAIN_SAMPLES=0
# export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/$(basename ${PWD})
export TEST_MODEL_PATH=${MODELS_ROOT_DIR}/speechlessai/toolcall-reasoning-intent

# -------------------- Dataset --------------------
export SPEECHLESS_DATA_DIR=/opt/local/datasets/speechless_data
# FIXME
export DATASET=./toolcall_data/6_3_1/toolcall-instructions-intent-train-1000-3000-6_3_1.jsonl

# -------------------- Environment --------------------
export OUTPUT_DIR=./outputs
# export TORCH_DISTRIBUTED_DEBUG=DETAIL
export RAY_memory_monitor_refresh_ms=0

# -------------------- Task --------------------
# FIXME
export TASK_NAME=$(basename ${TEST_MODEL_PATH})
export TASK_CHECKPOINT_DIR=${OUTPUT_DIR}
export WANDB_PROJECT=${TASK_NAME}

export SAVE_STEPS=100
export EVAL_STEPS=10
export WARMUP_STEPS=20
# export MAX_EVAL_SAMPLES=200
# export EVAL_DATASET_SIZE=0.0005
export MAX_EVAL_SAMPLES=200
export EVAL_DATASET_SIZE=200
export GROUP_BY_LENGTH=False
export LOGGING_STEPS=1

#export DEEPSPEED_STAGE2="--deepspeed deepspeed-stage2.json"

export LR_SCHEDULER_TYPE=cosine
# export LR_SCHEDULER_TYPE=constant_with_warmup

# export DATASET_FORMAT=instruction-input-response

export PROMPT_TYPE=raw
export DATASET_FORMAT=input-output

# export PROMPT_TYPE=alpaca
# export DATASET_FORMAT=conversations

# export PROMPT_TYPE=qwen2.5
# export DATASET_FORMAT=multi-rounds

# export PROMPT_TYPE=chatlm
# export PROMPT_TYPE=llama2
# export PROMPT_TYPE=minicpm

export LEARNING_RATE=2e-5
export OPTIM=paged_adamw_8bit
# export OPTIM=adafactor

export BITS=4
export LORA_R=32
export LORA_ALPHA=64

export NUM_EXPERTS=8
export TOPK=2
export ADAPTER_DIM=64

# export NUM_EXPERTS=16
# export TOPK=4
# export ADAPTER_DIM=512

# export NUM_EXPERTS=4
# export TOPK=2
# export ADAPTER_DIM=32

# Mistral
# export MODEL_MAX_LENGTH=32768
# export ROPE_THETA=10000
# export SLIDING_WINDOW=4096

# Tora
# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096
# export MODEL_MAX_LENGTH=16384
# export ROPE_THETA=1000000
# export SLIDING_WINDOW=4096

export MODEL_MAX_LENGTH=2048
export ROPE_THETA=1000000
export SLIDING_WINDOW=4096

export NUM_GPUS=4
export PER_DEVICE_TRAIN_BATCH_SIZE=4
export GRADIENT_ACCUMULATION_STEPS=4

export NUM_TRAIN_EPOCHS=3
export SAVE_STRATEGY=epoch
# export SAVE_STRATEGY=steps

export SAVE_TOTAL_LIMIT="--save_total_limit ${NUM_TRAIN_EPOCHS}"
export NUM_EARLY_STOPPING_TRAIN_EPOCHS=${NUM_TRAIN_EPOCHS}


# export DEEPSEED="--deepspeed ./deepspeed/ds_z2_config.json"
# export DEEPSEED="--deepspeed deepspeed-stage2.json"
# export NEFTUNE="--neftune --noise_alpha 5.0"

# export MISC_PARAMS="--long_lora True"
# export MISC_PARAMS="--custom_training_module_file custom_training.py --custom_trainer_name TaskTrainer"

# No more than 85% VRAM.
# A100(40GB) 32000, A40(48GB) 40000, A100(80GB) 70000
export MAX_MEMORY_MB=40000

# export ADD_REASONING_TOKENS="--add_reasoning_tokens"
