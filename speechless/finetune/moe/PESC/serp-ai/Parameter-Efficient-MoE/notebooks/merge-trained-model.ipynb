{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "\n",
    "parent_dir = \"..\"\n",
    "os.chdir(parent_dir)\n",
    "import json\n",
    "import torch\n",
    "import transformers\n",
    "from peft import PeftModel\n",
    "\n",
    "from sparsetral.configuration_sparsetral import SparsetralConfig\n",
    "from sparsetral.modeling_sparsetral import MistralForCausalLM\n",
    "\n",
    "trained_weights = \"output/checkpoint-7825\"\n",
    "output_dir = \"output/sparsetral-16x7B-v2\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = SparsetralConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model_config.pretraining_tp = 1  ## without tensor parallelism rank\n",
    "\n",
    "# Sparsetral Config\n",
    "model_config.moe_dtype = \"bfloat16\"\n",
    "model_config.adapter_dim = 512\n",
    "model_config.topk = 4\n",
    "model_config.moe_scaling = 1\n",
    "model_config.num_experts = 16\n",
    "model_config.output_router_logits = False\n",
    "\n",
    "moe_model = os.path.join(trained_weights, \"moe_model.bin\")\n",
    "adapter_model = os.path.join(trained_weights, \"adapter_model\")\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    config=model_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "model = PeftModel.from_pretrained(model, adapter_model)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "moe_state_dict = torch.load(moe_model, map_location=\"cpu\")\n",
    "new_moe_state_dict = {}\n",
    "for k, v in moe_state_dict.items():\n",
    "    new_moe_state_dict[k.replace(\"base_model.model.\", \"\")] = v\n",
    "\n",
    "model.load_state_dict(new_moe_state_dict, strict=False)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "config_path = os.path.join(output_dir, \"config.json\")\n",
    "config = json.load(open(config_path, \"r\"))\n",
    "config[\"architectures\"] = [\"modeling_sparsetral.MistralForCausalLM\"]\n",
    "config[\"auto_map\"] = {\n",
    "    \"AutoConfig\": \"configuration_sparsetral.SparsetralConfig\",\n",
    "    \"AutoModel\": \"modeling_sparsetral.MistralModel\",\n",
    "    \"AutoModelForCausalLM\": \"modeling_sparsetral.MistralForCausalLM\"\n",
    "  }\n",
    "config[\"model_type\"] = \"sparsetral\"\n",
    "config.pop(\"_name_or_path\", None)\n",
    "json.dump(config, open(config_path, \"w\"), indent=2)\n",
    "\n",
    "shutil.copy2(\"sparsetral/configuration_sparsetral.py\", os.path.join(output_dir, \"configuration_sparsetral.py\"))\n",
    "shutil.copy2(\"sparsetral/modeling_sparsetral.py\", os.path.join(output_dir, \"modeling_sparsetral.py\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Push to Hub (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "api.upload_folder(\n",
    "    folder_path=output_dir,\n",
    "    repo_id=\"\",\n",
    "    repo_type=\"model\",\n",
    "    token=\"\" # needs write access\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\"serpdotai/sparsetral-16x7B-v2\", torch_dtype=torch.bfloat16, device_map=\"cuda:0\", trust_remote_code=True)\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"serpdotai/sparsetral-16x7B-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_str = \"<|im_start|>system\\n{message}<|im_end|>\\n\"\n",
    "user_str = \"<|im_start|>user\\n{message}<|im_end|>\\n\"\n",
    "assistant_str = \"<|im_start|>assistant\\n{message}<|im_end|>\\n\"\n",
    "\n",
    "def construct_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"from\"] in [\"human\", \"user\"]:\n",
    "            prompt += user_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        elif message[\"from\"] in [\"gpt\", \"assistant\"]:\n",
    "            prompt += assistant_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        elif message[\"from\"] in [\"system\", \"instruction\"]:\n",
    "            prompt += system_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown message type: {message['from']}\"\n",
    "            )\n",
    "    return prompt + \"<|im_start|>assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a helpful assistant who will help the user to the best of their ability. If you don't know something, say \\\"I don't know\\\"\"\n",
    "user = \"Are you sentient?\"\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"system\", \"value\": system},\n",
    "    {\"from\": \"user\", \"value\": user},\n",
    "]\n",
    "\n",
    "prompt = construct_prompt(messages)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs, max_length=4096, do_sample=True, top_k=50, top_p=0.99, temperature=0.9, num_return_sequences=1)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
