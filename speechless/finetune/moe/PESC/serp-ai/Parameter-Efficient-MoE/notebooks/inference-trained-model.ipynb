{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "parent_dir = \"..\"\n",
    "os.chdir(parent_dir)\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from sparsetral.configuration_sparsetral import SparsetralConfig\n",
    "from sparsetral.modeling_sparsetral import MistralForCausalLM\n",
    "\n",
    "trained_weights = \"output/checkpoint-5000\" \n",
    "\n",
    "model_config = SparsetralConfig.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")\n",
    "model_config.pretraining_tp = 1  ## without tensor parallelism rank\n",
    "\n",
    "# Sparsetral Config\n",
    "model_config.moe_dtype = \"bfloat16\"\n",
    "model_config.lora_r = 64\n",
    "model_config.lora_alpha = 16\n",
    "model_config.adapter_dim = 512\n",
    "model_config.topk = 4\n",
    "model_config.moe_scaling = 1\n",
    "model_config.num_experts = 16\n",
    "model_config.output_router_logits = False\n",
    "\n",
    "model = MistralForCausalLM.from_pretrained(\n",
    "    \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    config=model_config,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"cuda:0\",\n",
    ")\n",
    "\n",
    "moe_model = os.path.join(trained_weights, \"moe_model.bin\")\n",
    "adapter_model = os.path.join(trained_weights, \"adapter_model\")\n",
    "\n",
    "moe_state_dict = torch.load(moe_model, map_location=\"cpu\")\n",
    "new_moe_state_dict = {}\n",
    "for k, v in moe_state_dict.items():\n",
    "    new_moe_state_dict[k.replace(\"base_model.model.\", \"\")] = v\n",
    "\n",
    "model.load_state_dict(new_moe_state_dict, strict=False)\n",
    "model.load_adapter(adapter_model)\n",
    "\n",
    "model.eval()\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_str = \"<|im_start|>system\\n{message}<|im_end|>\\n\"\n",
    "user_str = \"<|im_start|>user\\n{message}<|im_end|>\\n\"\n",
    "assistant_str = \"<|im_start|>assistant\\n{message}<|im_end|>\\n\"\n",
    "\n",
    "def construct_prompt(messages):\n",
    "    prompt = \"\"\n",
    "    for message in messages:\n",
    "        if message[\"from\"] in [\"human\", \"user\"]:\n",
    "            prompt += user_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        elif message[\"from\"] in [\"gpt\", \"assistant\"]:\n",
    "            prompt += assistant_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        elif message[\"from\"] in [\"system\", \"instruction\"]:\n",
    "            prompt += system_str.format(\n",
    "                message=message[\"value\"]\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                f\"Unknown message type: {message['from']}\"\n",
    "            )\n",
    "    return prompt + \"<|im_start|>assistant\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system = \"You are a helpful assistant who will help the user to the best of their ability. If you don't know something, say \\\"I don't know\\\"\"\n",
    "user = \"Are you sentient?\"\n",
    "\n",
    "messages = [\n",
    "    {\"from\": \"system\", \"value\": system},\n",
    "    {\"from\": \"user\", \"value\": user},\n",
    "]\n",
    "\n",
    "prompt = construct_prompt(messages)\n",
    "\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "inputs = inputs.to(model.device)\n",
    "pred = model.generate(**inputs, max_length=4096, do_sample=True, top_k=50, top_p=0.99, temperature=0.9, num_return_sequences=1)\n",
    "print(tokenizer.decode(pred.cpu()[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
