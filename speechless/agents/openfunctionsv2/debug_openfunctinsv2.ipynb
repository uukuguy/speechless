{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Gorilla OpenFunctions v2\n",
    "\n",
    "https://github.com/ShishirPatil/gorilla/tree/main/openfunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechless.agents.openfunctionsv2.utils.python_parser import parse_python_function_call\n",
    "# from speechless.agents.openfunctionsv2.utils.java_parser import parse_java_function_call\n",
    "# from speechless.agents.openfunctionsv2.utils.js_parser import parse_javascript_function_call\n",
    "\n",
    "FN_CALL_DELIMITER = \"<<function>>\"\n",
    "\n",
    "def strip_function_calls(content: str) -> list[str]:\n",
    "    \"\"\"\n",
    "    Split the content by the function call delimiter and remove empty strings\n",
    "    \"\"\"\n",
    "    # return [element.strip() for element in content.split(FN_CALL_DELIMITER)[2:] if element.strip()]\n",
    "    return [element.strip() for element in content.split(FN_CALL_DELIMITER)[1:] if element.strip()]\n",
    "\n",
    "def parse_function_call(call: str) -> dict[str, any]:\n",
    "    \"\"\"\n",
    "    This is temporary. The long term solution is to union all the \n",
    "    types of the parameters from the user's input function definition,\n",
    "    and check which language is a proper super set of the union type.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        return parse_python_function_call(call)\n",
    "    except:\n",
    "        return None\n",
    "    # except Exception as e:\n",
    "    #     # If Python parsing fails, try Java parsing\n",
    "    #     try:\n",
    "    #         java_result = parse_java_function_call(call)\n",
    "    #         if not java_result:\n",
    "    #             raise Exception(\"Java parsing failed\")\n",
    "    #         return java_result\n",
    "    #     except Exception as e:\n",
    "    #         # If Java parsing also fails, try JavaScript parsing\n",
    "    #         try:   \n",
    "    #             javascript_result = parse_javascript_function_call(call)\n",
    "    #             if not javascript_result:\n",
    "    #                 raise Exception(\"JavaScript parsing failed\")\n",
    "    #             return javascript_result\n",
    "    #         except:\n",
    "    #             return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_prompt(user_query: str, functions: list = []) -> str:\n",
    "    \"\"\"\n",
    "    Generates a conversation prompt based on the user's query and a list of functions.\n",
    "\n",
    "    Parameters:\n",
    "    - user_query (str): The user's query.\n",
    "    - functions (list): A list of functions to include in the prompt.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted conversation prompt.\n",
    "    \"\"\"\n",
    "    system = \"You are an AI programming assistant, utilizing the Gorilla LLM model, developed by Gorilla LLM, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer.\"\n",
    "    if len(functions) == 0:\n",
    "        return f\"{system}\\n### Instruction: <<question>> {user_query}\\n### Response: \"\n",
    "    functions_string = json.dumps(functions)\n",
    "    return f\"{system}\\n### Instruction: <<function>>{functions_string}\\n<<question>>{user_query}\\n### Response: \"\n",
    "\n",
    "\n",
    "def format_response(response: str):\n",
    "    \"\"\"\n",
    "    Formats the response from the OpenFunctions model.\n",
    "\n",
    "    Parameters:\n",
    "    - response (str): The response generated by the LLM.\n",
    "\n",
    "    Returns:\n",
    "    - str: The formatted response.\n",
    "    - dict: The function call(s) extracted from the response.\n",
    "\n",
    "    \"\"\"\n",
    "    function_call_dicts = None\n",
    "    try:\n",
    "        response = strip_function_calls(response)\n",
    "        print(f\"{response=}\")\n",
    "        # Parallel function calls returned as a str, list[dict]\n",
    "        if len(response) > 1: \n",
    "            function_call_dicts = []\n",
    "            for function_call in response:\n",
    "                function_call_dicts.append(parse_function_call(function_call))\n",
    "            response = \", \".join(response)\n",
    "        # Single function call returned as a str, dict\n",
    "        else:\n",
    "            function_call_dicts = parse_function_call(response[0])\n",
    "            response = response[0]\n",
    "    except Exception as e:\n",
    "        # Just faithfully return the generated response str to the user\n",
    "        pass\n",
    "    return response, function_call_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 273 tensors from /opt/local/llm_models/GGUF/function-calling/gorilla-openfunctions-v2.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 30\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = gpt2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,102400]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,102400]  = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,102400]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,99757]   = [\"Ġ Ġ\", \"Ġ t\", \"Ġ a\", \"i n\", \"h e...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 100000\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 100015\n",
      "llama_model_loader: - kv  19:            tokenizer.ggml.padding_token_id u32              = 100001\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   61 tensors\n",
      "llama_model_loader: - type q4_K:  183 tensors\n",
      "llama_model_loader: - type q6_K:   29 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 2387/102400 vs 2400/102400 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = BPE\n",
      "llm_load_print_meta: n_vocab          = 102400\n",
      "llm_load_print_meta: n_merges         = 99757\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 30\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = ?B\n",
      "llm_load_print_meta: model ftype      = Q4_K - Medium\n",
      "llm_load_print_meta: model params     = 6.91 B\n",
      "llm_load_print_meta: model size       = 3.93 GiB (4.88 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 100000 '<｜begin▁of▁sentence｜>'\n",
      "llm_load_print_meta: EOS token        = 100015 '<|EOT|>'\n",
      "llm_load_print_meta: PAD token        = 100001 '<｜end▁of▁sentence｜>'\n",
      "llm_load_print_meta: LF token         = 126 'Ä'\n",
      "llm_load_tensors: ggml ctx size =    0.21 MiB\n",
      "ggml_backend_metal_buffer_from_ptr: allocated buffer, size =  3798.92 MiB, ( 8051.80 / 98304.00)\n",
      "llm_load_tensors: offloading 30 repeating layers to GPU\n",
      "llm_load_tensors: offloading non-repeating layers to GPU\n",
      "llm_load_tensors: offloaded 31/31 layers to GPU\n",
      "llm_load_tensors:        CPU buffer size =   225.00 MiB\n",
      "llm_load_tensors:      Metal buffer size =  3798.91 MiB\n",
      ".........................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 512\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3 Max\n",
      "ggml_metal_init: picking default device: Apple M3 Max\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3 Max\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction support   = true\n",
      "ggml_metal_init: simdgroup matrix mul. support = true\n",
      "ggml_metal_init: hasUnifiedMemory              = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 103079.22 MB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   240.00 MiB, ( 8299.80 / 98304.00)\n",
      "llama_kv_cache_init:      Metal KV buffer size =   240.00 MiB\n",
      "llama_new_context_with_model: KV self size  =  240.00 MiB, K (f16):  120.00 MiB, V (f16):  120.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.39 MiB\n",
      "ggml_backend_metal_buffer_type_alloc_buffer: allocated buffer, size =   208.00 MiB, ( 8507.80 / 98304.00)\n",
      "llama_new_context_with_model:      Metal compute buffer size =   208.00 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =     9.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 966\n",
      "llama_new_context_with_model: graph splits = 2\n",
      "AVX = 0 | AVX_VNNI = 0 | AVX2 = 0 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 0 | NEON = 1 | ARM_FMA = 1 | F16C = 0 | FP16_VA = 1 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 0 | SSSE3 = 0 | VSX = 0 | MATMUL_INT8 = 0 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}\\n{% set add_generation_prompt = false %}\\n{% endif %}\\n{%- set ns = namespace(found=false) -%}\\n{%- for message in messages -%}\\n    {%- if message['role'] == 'system' -%}\\n        {%- set ns.found = true -%}\\n    {%- endif -%}\\n{%- endfor -%}\\n{{bos_token}}{%- if not ns.found -%}\\n{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\\\n'}}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if message['role'] == 'system' %}\\n{{ message['content'] }}\\n    {%- else %}\\n        {%- if message['role'] == 'user' %}\\n{{'### Instruction:\\\\n' + message['content'] + '\\\\n'}}\\n        {%- else %}\\n{{'### Response:\\\\n' + message['content'] + '\\\\n<|EOT|>\\\\n'}}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{% if add_generation_prompt %}\\n{{'### Response:'}}\\n{% endif %}\", 'tokenizer.ggml.padding_token_id': '100001', 'tokenizer.ggml.eos_token_id': '100015', 'tokenizer.ggml.bos_token_id': '100000', 'tokenizer.ggml.model': 'gpt2', 'llama.attention.head_count_kv': '32', 'llama.context_length': '4096', 'llama.attention.head_count': '32', 'llama.rope.freq_base': '10000.000000', 'llama.rope.dimension_count': '128', 'general.file_type': '15', 'llama.feed_forward_length': '11008', 'llama.embedding_length': '4096', 'llama.block_count': '30', 'general.architecture': 'llama', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'general.name': 'LLaMA v2'}\n",
      "Using gguf chat template: {% if not add_generation_prompt is defined %}\n",
      "{% set add_generation_prompt = false %}\n",
      "{% endif %}\n",
      "{%- set ns = namespace(found=false) -%}\n",
      "{%- for message in messages -%}\n",
      "    {%- if message['role'] == 'system' -%}\n",
      "        {%- set ns.found = true -%}\n",
      "    {%- endif -%}\n",
      "{%- endfor -%}\n",
      "{{bos_token}}{%- if not ns.found -%}\n",
      "{{'You are an AI programming assistant, utilizing the Deepseek Coder model, developed by Deepseek Company, and you only answer questions related to computer science. For politically sensitive questions, security and privacy issues, and other non-computer science questions, you will refuse to answer\\n'}}\n",
      "{%- endif %}\n",
      "{%- for message in messages %}\n",
      "    {%- if message['role'] == 'system' %}\n",
      "{{ message['content'] }}\n",
      "    {%- else %}\n",
      "        {%- if message['role'] == 'user' %}\n",
      "{{'### Instruction:\\n' + message['content'] + '\\n'}}\n",
      "        {%- else %}\n",
      "{{'### Response:\\n' + message['content'] + '\\n<|EOT|>\\n'}}\n",
      "        {%- endif %}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{% if add_generation_prompt %}\n",
      "{{'### Response:'}}\n",
      "{% endif %}\n",
      "Using chat eos_token: <|EOT|>\n",
      "Using chat bos_token: <｜begin▁of▁sentence｜>\n",
      "\n",
      "llama_print_timings:        load time =     347.12 ms\n",
      "llama_print_timings:      sample time =       3.37 ms /    16 runs   (    0.21 ms per token,  4750.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =     347.03 ms /   181 tokens (    1.92 ms per token,   521.57 tokens per second)\n",
      "llama_print_timings:        eval time =     248.05 ms /    15 runs   (   16.54 ms per token,    60.47 tokens per second)\n",
      "llama_print_timings:       total time =     653.31 ms /   196 tokens\n",
      "ggml_metal_free: deallocating\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output_1={'id': 'cmpl-db812936-6cde-4e2b-80c1-5ead6ca5697b', 'object': 'text_completion', 'created': 1713494467, 'model': '/opt/local/llm_models/GGUF/function-calling/gorilla-openfunctions-v2.Q4_K_M.gguf', 'choices': [{'text': \" <<function>>get_current_weather(location='San Francisco, CA')\", 'index': 0, 'logprobs': None, 'finish_reason': 'length'}], 'usage': {'prompt_tokens': 181, 'completion_tokens': 16, 'total_tokens': 197}}\n",
      " <<function>>get_current_weather(location='San Francisco, CA')\n",
      "response=[\"get_current_weather(location='San Francisco, CA')\"]\n",
      "--------------------\n",
      "Function call strings 1(s): get_current_weather(location='San Francisco, CA')\n",
      "--------------------\n",
      "OpenAI compatible `function_call`: {'name': 'get_current_weather', 'arguments': {'location': 'San Francisco, CA'}}\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def test_demo():\n",
    "    # # Device setup\n",
    "    # device : str = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "    # torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "    # # Model and tokenizer setup\n",
    "    # model_id : str = \"gorilla-llm/gorilla-openfunctions-v2\"\n",
    "    # tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    # model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch_dtype, low_cpu_mem_usage=True)\n",
    "\n",
    "    # # Move model to device\n",
    "    # model.to(device)\n",
    "\n",
    "    # # Pipeline setup\n",
    "    # pipe = pipeline(\n",
    "    #     \"text-generation\",\n",
    "    #     model=model,\n",
    "    #     tokenizer=tokenizer,\n",
    "    #     max_new_tokens=128,\n",
    "    #     batch_size=16,\n",
    "    #     torch_dtype=torch_dtype,\n",
    "    #     device=device,\n",
    "    # )\n",
    "\n",
    "    from llama_cpp import Llama\n",
    "    from llama_cpp.llama_tokenizer import LlamaHFTokenizer\n",
    "    FUNCTION_CALLING_MODELS_DIR = \"/opt/local/llm_models/GGUF/function-calling\"\n",
    "    # ---------- Gorilla OpenFunctions v2 ----------\n",
    "    model_path = f\"{FUNCTION_CALLING_MODELS_DIR}/gorilla-openfunctions-v2.Q4_K_M.gguf\"\n",
    "    # ---------- functionary-small-v2.4-GGUF ----------\n",
    "    # FUNCTIONARY_MODELS_DIR=f\"{FUNCTION_CALLING_MODELS_DIR}/meetkai/functionary-small-v2.4-GGUF\"\n",
    "    # model_path=f\"{FUNCTIONARY_MODELS_DIR}/functionary-small-v2.4.Q8_0.gguf\"\n",
    "    # ---------- functionary-medium-v2.4-GGUF ----------\n",
    "    # FUNCTIONARY_MODELS_DIR=f\"{FUNCTION_CALLING_MODELS_DIR}/meetkai/functionary-medium-v2.4-GGUF\"\n",
    "    # model_path=f\"{FUNCTIONARY_MODELS_DIR}/functionary-medium-v2.4.Q4_0.gguf\"\n",
    "\n",
    "    llm = Llama(\n",
    "        model_path=model_path,\n",
    "        # chat_format=\"functionary-v2\",\n",
    "        # tokenizer=LlamaHFTokenizer.from_pretrained(FUNCTIONARY_MODELS_DIR),\n",
    "        n_gpu_layers=-1\n",
    "    )\n",
    "\n",
    "    # Example usage 1\n",
    "    #  This should return 2 functions with the right argument\n",
    "    query_1: str = \"What's the weather like in the two cities of Boston and San Francisco?\"\n",
    "    functions_1 = [\n",
    "        {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The city and state, e.g. San Francisco, CA\",\n",
    "                    },\n",
    "                    \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            },\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Example usage 2\n",
    "    #  This should return an error since the function cann't help with the prompt\n",
    "    query_2: str = \"What is the freezing point of water at a pressure of 10 kPa?\"\n",
    "    functions_2 = [{\"name\": \"thermodynamics.calculate_boiling_point\", \"description\": \"Calculate the boiling point of a given substance at a specific pressure.\", \"parameters\": {\"type\": \"object\", \"properties\": {\"substance\": {\"type\": \"string\", \"description\": \"The substance for which to calculate the boiling point.\"}, \"pressure\": {\"type\": \"number\", \"description\": \"The pressure at which to calculate the boiling point.\"}, \"unit\": {\"type\": \"string\", \"description\": \"The unit of the pressure. Default is 'kPa'.\"}}, \"required\": [\"substance\", \"pressure\"]}}]\n",
    "\n",
    "    # Generate prompt and obtain model output\n",
    "    prompt_1 = get_prompt(query_1, functions=functions_1)\n",
    "\n",
    "    # output_1 = pipe(prompt_1)\n",
    "    output_1 = llm(prompt_1)\n",
    "    print(f\"{output_1=}\")\n",
    "    print(f\"{output_1['choices'][0]['text']}\")\n",
    "\n",
    "    # fn_call_string, function_call_dict = format_response(output_1[0]['generated_text'])\n",
    "    fn_call_string, function_call_dict = format_response(output_1['choices'][0]['text'])\n",
    "    print(\"--------------------\")\n",
    "    print(f\"Function call strings 1(s): {fn_call_string}\")\n",
    "    print(\"--------------------\")\n",
    "    print(f\"OpenAI compatible `function_call`: {function_call_dict}\")\n",
    "    print(\"--------------------\")\n",
    "\n",
    "test_demo()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "m1max-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
