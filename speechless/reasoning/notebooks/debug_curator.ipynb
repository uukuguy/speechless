{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug Curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from datasets import load_dataset\n",
    "from pydantic import BaseModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from speechless.reasoning.utils.code_execution_taco import process_dataset_parallel\n",
    "from speechless.reasoning.utils.prompt import SKY_T1_SYSTEM_PROMPT, generate_prompt\n",
    "\n",
    "from bespokelabs import curator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TACOCurator(curator.LLM):\n",
    "    \"\"\"Curator class for processing TACO (Testing Algorithmic Coding prOblems) dataset.\n",
    "\n",
    "    Handles prompting the LLM and parsing responses for code generation.\n",
    "    \"\"\"\n",
    "\n",
    "    return_completions_object = True\n",
    "\n",
    "    def prompt(self, problem):\n",
    "        \"\"\"Parse test cases and starter code from problem to create a prompt for the LLM.\"\"\"\n",
    "        test_case = json.loads(problem[\"input_output\"])\n",
    "        starter_code = problem[\"starter_code\"]\n",
    "        # Generate prompt text using test case, question and starter code\n",
    "        prompt_text = generate_prompt(test_case, problem[\"question\"], starter_code)\n",
    "        return [{\"role\": \"system\", \"content\": SKY_T1_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "    def parse(self, input, response):\n",
    "        \"\"\"Parse the LLM response to extract reasoning and solution.\"\"\"\n",
    "        input[\"reasoning\"] = response[\"choices\"][0][\"message\"][\"reasoning_content\"]\n",
    "        input[\"deepseek_solution\"] = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class APPSCurator(curator.LLM):\n",
    "    \"\"\"Curator class for processing APPS (Automated Programming Problems Solutions) dataset.\"\"\"\n",
    "\n",
    "    return_completions_object = True\n",
    "\n",
    "    def prompt(self, problem):\n",
    "        \"\"\"Parse test cases and starter code from problem to create a prompt for the LLM.\"\"\"\n",
    "        test_case = json.loads(problem[\"input_output\"])\n",
    "        starter_code = problem[\"starter_code\"]\n",
    "        prompt_text = generate_prompt(test_case, problem[\"question\"], starter_code)\n",
    "        return [{\"role\": \"system\", \"content\": SKY_T1_SYSTEM_PROMPT}, {\"role\": \"user\", \"content\": prompt_text}]\n",
    "\n",
    "    def parse(self, input, response):\n",
    "        \"\"\"Parse the LLM response to extract reasoning and solution.\"\"\"\n",
    "        input[\"reasoning\"] = response[\"choices\"][0][\"message\"][\"reasoning_content\"]\n",
    "        input[\"deepseek_solution\"] = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class JudgeResult(BaseModel):\n",
    "    \"\"\"Result of the judge's evaluation.\"\"\"\n",
    "    correct: bool\n",
    "    reasoning: str\n",
    "\n",
    "\n",
    "class Numina162KJudge(curator.LLM):\n",
    "    \"\"\"Curator class for processing Numina dataset.\"\"\"\n",
    "\n",
    "    response_format = JudgeResult\n",
    "\n",
    "    def prompt(self, input):\n",
    "        \"\"\"Create a prompt for the judge to evaluate the correctness of a solution.\"\"\"\n",
    "        return f\"\"\"\n",
    "        You are a judge that evaluates the correctness of a solution.\n",
    "        You will be given a solution and a ground truth solution.\n",
    "        You will need to determine if the solution is correct.\n",
    "        Answers are in the format of \\\\boxed{{}}.\n",
    "\n",
    "        SOLUTION: {input[\"deepseek_solution\"]}\n",
    "        GROUND TRUTH SOLUTION: {input[\"ground_truth_solution\"]}\n",
    "        \"\"\"\n",
    "\n",
    "    def parse(self, input, response):\n",
    "        \"\"\"Parse the judge's response to extract correctness and reasoning.\"\"\"\n",
    "        return {**input, \"correct\": response.correct, \"judge_reasoning\": response.reasoning}\n",
    "\n",
    "\n",
    "def extract_boxed_answer(text):\n",
    "    \"\"\"Extract the boxed answer from the text.\"\"\"\n",
    "    from speechless.reasoning.testing.math import extract_answer, strip_answer_string\n",
    "    text = strip_answer_string(text)\n",
    "    return extract_answer(text)\n",
    "\n",
    "class Numina162K(curator.LLM):\n",
    "    \"\"\"Curator class for processing Numina dataset.\"\"\"\n",
    "\n",
    "    return_completions_object = True\n",
    "\n",
    "    def prompt(self, input):\n",
    "        \"\"\"Create a prompt for the LLM to reason about the problem.\"\"\"\n",
    "        return [\n",
    "            {\"role\": \"system\", \"content\": SKY_T1_SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\", \"content\": input[\"problem\"]},\n",
    "        ]\n",
    "\n",
    "    def parse(self, input, response):\n",
    "        \"\"\"Parse the LLM response to extract reasoning and solution.\"\"\"\n",
    "        return [\n",
    "            {\n",
    "                \"problem\": input[\"problem\"],\n",
    "                \"reasoning\": response[\"choices\"][0][\"message\"][\"reasoning_content\"],\n",
    "                \"deepseek_solution\": response[\"choices\"][0][\"message\"][\"content\"],\n",
    "                \"ground_truth_solution\": input[\"solution\"],\n",
    "                \"deepseek_final_answer\": extract_boxed_answer(response[\"choices\"][0][\"message\"][\"content\"]),\n",
    "                \"ground_truth_final_answer\": extract_boxed_answer(input[\"solution\"]),\n",
    "            }\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
