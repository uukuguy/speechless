{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ToolCall Experiment Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, re, os, sys\n",
    "from tqdm import tqdm\n",
    "from loguru import logger\n",
    "import random\n",
    "from typing import List, Dict, Any, Tuple, Union\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "from textwrap import dedent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = os.path.dirname(os.path.abspath(os.getcwd()))\n",
    "if current_dir not in sys.path:\n",
    "    sys.path.insert(0, current_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import build_instruction, build_response_from_functioncalls, toolcall_to_functions, format_chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APIs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5991, 887, 5188)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round_a_apis_file = \"./round_a_apis.json\"\n",
    "round_b_apis_file = \"./round_b_apis.json\"\n",
    "round_a_apis = json.load(open(round_a_apis_file))\n",
    "round_b_apis = json.load(open(round_b_apis_file))\n",
    "toolcall_apis = {}\n",
    "toolcall_apis.update(round_a_apis)\n",
    "toolcall_apis.update(round_b_apis)\n",
    "len(toolcall_apis), len(round_a_apis), len(round_b_apis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 5991 toolcall apis to toolcall_apis.json\n"
     ]
    }
   ],
   "source": [
    "toolcall_apis_file = \"toolcall_apis.json\"\n",
    "with open(toolcall_apis_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(toolcall_apis, f, ensure_ascii=False, indent=2)\n",
    "print(f\"Saved {len(toolcall_apis)} toolcall apis to {toolcall_apis_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synthetic Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "synthetic_data_single_api_file = \"./synthetic_data/synthetic_data_native-single_api-v6_0314.jsonl\"\n",
    "synthetic_data_multi_api_file = \"./synthetic_data/synthetic_data_native-multi_apis-0316.jsonl\"\n",
    "synthetic_data_dev_file = \"./synthetic_data/synthetic_data_native-multi_apis-0318.jsonl\"\n",
    "synthetic_data_multi_api_0315_file = \"./synthetic_data/synthetic_data_native-multi_apis-0315.jsonl\"\n",
    "\n",
    "synthetic_data_single_api = [json.loads(line) for line in open(synthetic_data_single_api_file)]\n",
    "synthetic_data_multi_apis = [json.loads(line) for line in open(synthetic_data_multi_api_file)]\n",
    "synthetic_data_multi_apis_0315 = [json.loads(line) for line in open(synthetic_data_multi_api_0315_file)]\n",
    "synthetic_data_dev = [json.loads(line) for line in open(synthetic_data_dev_file)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/13287 [00:00<?, ?it/s]\u001b[32m2025-03-24 14:25:01.318\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"11111\",\\n                        \"scholarship_type\": \"科技创新奖\",\\n                        \"amount\": 8000,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": true,\\n                        \"student_grade\": \"A+\",\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                },\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"22222\",\\n                        \"scholarship_type\": \"社会服务奖\",\\n                        \"amount\": 5000,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": false,\\n                        \"reason\": \"缺乏相关证明\",\\n                        \"student_grade\": \"B\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.319\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"77777\",\\n                        \"scholarship_type\": \"文化交流奖学金\",\\n                        \"amount\": 7000,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": true,\\n                        \"student_grade\": \"A+\",\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                },\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"88888\",\\n                        \"scholarship_type\": \"环境保护奖学金\",\\n                        \"amount\": 5500,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": false,\\n                        \"reason\": \"申请材料不全\",\\n                        \"student_grade\": \"B\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.319\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"99999\",\\n                        \"scholarship_type\": \"创新创业奖学金\",\\n                        \"amount\": 9000,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": true,\\n                        \"student_grade\": \"A+\",\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                },\\n                {\\n                    \"name\": \"student_scholarship_allocation\",\\n                    \"arguments\": {\\n                        \"student_id\": \"10101\",\\n                        \"scholarship_type\": \"国际交流奖学金\",\\n                        \"amount\": 6500,\\n                        \"academic_year\": \"2023-2024\",\\n                        \"approved\": false,\\n                        \"reason\": \"缺乏相关证明\",\\n                        \"student_grade\": \"B+\",\\n                        \"distribution_date\": \"2023-09-01\"\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.336\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"create_event\",\\n                    \"arguments\": {\\n                        \"event_name\": \"春季促销\",\\n                        \"start_time\": \"2024-03-01T10:00:00Z\",\\n                        \"end_time\": \"2024-03-15T18:00:00Z\",\\n                        \"description\": \"春季特卖，折扣高达50%\",\",\\n                        \"location\": \"上海\",\\n                        \"capacity\": 100,\\n                        \"is_virtual\": false\\n                    }\\n                },\\n                {\\n                    \"name\": \"create_event\",\\n                    \"arguments\": {\\n                        \"event_name\": \"夏季音乐节\",\\n                        \"start_time\": \"2024-06-01T12:00:00Z\",\\n                        \"end_time\": \"2024-06-10T22:00:00Z\",\\n                        \"description\": \"享受音乐与美食的完美结合\",\\n                        \"location\": \"北京\",\\n                        \"capacity\": 200,\\n                        \"is_virtual\": false\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.338\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"A001\",\\n                        \"goal_title\": \"提升销售额\",\\n                        \"goal_description\": \"在下个季度内将销售额提高20%\",\",\\n                        \"start_date\": \"2023-11-01\",\\n                        \"end_date\": \"2024-01-31\",\\n                        \"priority\": 1\\n                    }\\n                },\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"A001\",\\n                        \"goal_title\": \"客户满意度\",\\n                        \"goal_description\": \"在下个季度内将客户满意度提升到90%以上\",\\n                        \"start_date\": \"2023-11-01\",\\n                        \"end_date\": \"2024-01-31\",\\n                        \"priority\": 2\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.338\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"C003\",\\n                        \"goal_title\": \"提升市场份额\",\\n                        \"goal_description\": \"在下个财年内将市场份额提高5%\",\",\\n                        \"start_date\": \"2023-12-01\",\\n                        \"end_date\": \"2024-11-30\",\\n                        \"priority\": 1\\n                    }\\n                },\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"C003\",\\n                        \"goal_title\": \"员工培训\",\\n                        \"goal_description\": \"在下个季度内完成至少两次培训\",\\n                        \"start_date\": \"2023-12-01\",\\n                        \"end_date\": \"2024-03-31\",\\n                        \"priority\": 2\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.339\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"D004\",\\n                        \"goal_title\": \"提升客户转化率\",\\n                        \"goal_description\": \"在下个季度内将客户转化率提高10%\",\",\\n                        \"start_date\": \"2023-11-15\",\\n                        \"end_date\": \"2024-02-15\",\\n                        \"priority\": 1\\n                    }\\n                },\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"D004\",\\n                        \"goal_title\": \"优化产品功能\",\\n                        \"goal_description\": \"在下个季度内完成产品功能的优化\",\\n                        \"start_date\": \"2023-11-15\",\\n                        \"end_date\": \"2024-02-15\",\\n                        \"priority\": 2\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.339\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"E005\",\\n                        \"goal_title\": \"提升品牌知名度\",\\n                        \"goal_description\": \"在下个财年内将品牌知名度提高15%\",\",\\n                        \"start_date\": \"2023-10-01\",\\n                        \"end_date\": \"2024-09-30\",\\n                        \"priority\": 1\\n                    }\\n                },\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"E005\",\\n                        \"goal_title\": \"增强社交媒体互动\",\\n                        \"goal_description\": \"在下个季度内提高社交媒体的互动率\",\\n                        \"start_date\": \"2023-10-01\",\\n                        \"end_date\": \"2024-01-01\",\\n                        \"priority\": 2\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "\u001b[32m2025-03-24 14:25:01.339\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36m<module>\u001b[0m:\u001b[36m80\u001b[0m - \u001b[33m\u001b[1mfunctioncall=' [\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"H008\",\\n                        \"goal_title\": \"提升销售团队的业绩\",\\n                        \"goal_description\": \"在下个季度内将销售额提高15%\",\",\\n                        \"start_date\": \"2023-11-01\",\\n                        \"end_date\": \"2024-01-31\",\\n                        \"priority\": 1\\n                    }\\n                },\\n                {\\n                    \"name\": \"set_performance_goals\",\\n                    \"arguments\": {\\n                        \"employee_id\": \"H008\",\\n                        \"goal_title\": \"增强客户关系\",\\n                        \"goal_description\": \"在下个季度内与至少10个重要客户进行沟通\",\\n                        \"start_date\": \"2023-11-01\",\\n                        \"end_date\": \"2024-01-31\",\\n                        \"priority\": 2\\n                    }\\n                }\\n            ]'\u001b[0m\n",
      "100%|██████████| 13287/13287 [00:00<00:00, 56520.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2834 10329 67 10414 13221 -23578\n",
      "Saved 2834 single-apis to ./synthetic_data/synthetic_data-single_api-0315.jsonl\n",
      "Saved 10414 multi-apis to ./synthetic_data/synthetic_data-multi_apis-0315.jsonl\n",
      "Saved 1295 empty-apis to ./synthetic_data/synthetic_data-empty_apis-0315.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "num_single_api = 0\n",
    "num_2_apis = 0\n",
    "num_3_apis = 0\n",
    "num_multi_apis = 0\n",
    "num_dup_functionalls = 0\n",
    "num_empty_apis = 0\n",
    "\n",
    "# output_single_file = \"./synthetic_data/synthetic_data-single_api-0314.jsonl\"\n",
    "# output_multi_file = \"./synthetic_data/synthetic_data-multi_apis-0314.jsonl\"\n",
    "# output_empty_file = \"./synthetic_data/synthetic_data-empty_apis-0314.jsonl\"\n",
    "# synthetic_data = synthetic_data_single_api\n",
    "# empty_threshold = 0.0\n",
    "\n",
    "output_single_file = \"./synthetic_data/synthetic_data-single_api-0315.jsonl\"\n",
    "output_multi_file = \"./synthetic_data/synthetic_data-multi_apis-0315.jsonl\"\n",
    "output_empty_file = \"./synthetic_data/synthetic_data-empty_apis-0315.jsonl\"\n",
    "synthetic_data = synthetic_data_multi_apis_0315\n",
    "empty_threshold = 0.9\n",
    "\n",
    "# output_single_file = \"./synthetic_data/synthetic_data-single_api-0316.jsonl\"\n",
    "# output_multi_file = \"./synthetic_data/synthetic_data-multi_apis-0316.jsonl\"\n",
    "# output_empty_file = \"./synthetic_data/synthetic_data-empty_apis-0316.jsonl\"\n",
    "# empty_threshold = 0.9\n",
    "# synthetic_data = synthetic_data_multi_apis\n",
    "\n",
    "\n",
    "# output_single_file = \"./synthetic_data/synthetic_data-single_api-0318.jsonl\"\n",
    "# output_multi_file = \"./synthetic_data/synthetic_data-multi_apis-0318.jsonl\"\n",
    "# output_empty_file = \"./synthetic_data/synthetic_data-empty_apis-0318.jsonl\"\n",
    "# empty_threshold = 0.9\n",
    "# synthetic_data = synthetic_data_dev\n",
    "\n",
    "\n",
    "single_apis = []\n",
    "multi_apis = []\n",
    "empty_apis = []\n",
    "for data in tqdm(synthetic_data):\n",
    "    # print(data)\n",
    "    if \"api_name\" in data:\n",
    "        _ = data.pop(\"api_name\")\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    # print(dialogue)\n",
    "\n",
    "    if not isinstance(dialogue, list):\n",
    "        continue\n",
    "    bad = False\n",
    "    for turn in dialogue:\n",
    "        if not isinstance(turn, dict):\n",
    "            bad = True\n",
    "            break\n",
    "        if \"from\" not in turn:\n",
    "            bad = True\n",
    "            break\n",
    "        if \"value\" not in turn:\n",
    "            bad = True\n",
    "            break\n",
    "    if bad:\n",
    "        continue\n",
    "\n",
    "    # 找到最后一个assistant的回复内容，可能是functioncall，也可能是其他内容\n",
    "    has_functioncall = False\n",
    "    has_empty = False\n",
    "    for i, turn in enumerate(reversed(dialogue)):\n",
    "        if turn[\"from\"] == \"ASSISTANT\":\n",
    "            if \"value\" not in turn:\n",
    "                break\n",
    "            content = turn[\"value\"]\n",
    "            # print(content)\n",
    "            if \"<functioncall>\" in content:\n",
    "                if len(re.findall(r\"<functioncall>\", content)) > 1 or len(re.findall(r\"</functioncall>\", content)) > 1:\n",
    "                    logger.warning(f\"Must only 1 <functioncall> pair in {content=}\")\n",
    "                    break\n",
    "                # if has_functioncall:\n",
    "                #     num_dup_functionalls += 1\n",
    "                #     continue\n",
    "                functioncall = content.split(\"<functioncall>\")[1].split(\"</functioncall>\")[0]\n",
    "                try:\n",
    "                    functions = json.loads(functioncall)\n",
    "                except:\n",
    "                    logger.warning(f\"{functioncall=}\")\n",
    "                    break\n",
    "\n",
    "                if not isinstance(functions, list):\n",
    "                    functions = [functions]\n",
    "\n",
    "\n",
    "                if not all([isinstance(function, dict) for function in functions]):\n",
    "                    break\n",
    "\n",
    "\n",
    "                if len(functions) == 0:\n",
    "                    print(functions)\n",
    "                    pass\n",
    "                if len(functions) == 1:\n",
    "                    if not isinstance(functions[0], dict):\n",
    "                        break\n",
    "                    has_functioncall = True\n",
    "                    num_single_api += 1\n",
    "                    turn[\"value\"] = (\"<functioncall>\" + json.dumps(functions, ensure_ascii=False) + \"</functioncall>\")\n",
    "                    new_data = deepcopy(data)\n",
    "                    new_data[\"dialogue\"] = dialogue[:len(dialogue) - i]\n",
    "                    single_apis.append(new_data)\n",
    "                elif len(functions) >= 2:\n",
    "                    has_functioncall = True\n",
    "                    num_multi_apis += 1\n",
    "                    turn[\"value\"] = (\"<functioncall>\" + json.dumps(functions, ensure_ascii=False) + \"</functioncall>\")\n",
    "                    new_data = deepcopy(data)\n",
    "                    new_data[\"dialogue\"] = dialogue[:len(dialogue) - i]\n",
    "                    multi_apis.append(new_data)\n",
    "                    if len(functions) == 2:\n",
    "                        num_2_apis += 1\n",
    "                    elif len(functions) == 3:\n",
    "                        num_3_apis += 1\n",
    "                else:\n",
    "                    raise ValueError(f\"{functions=}\")\n",
    "                break\n",
    "            else:\n",
    "                if not has_empty:\n",
    "                    if random.random() > empty_threshold:\n",
    "                        empty_apis.append(data)\n",
    "                    num_empty_apis += 1\n",
    "                    has_empty = True\n",
    "                # logger.debug(f\"{data=}\")\n",
    "\n",
    "# Only keep USER and ASSISTANT turns\n",
    "for api_data in [single_apis, multi_apis, empty_apis]:\n",
    "    for data in api_data:\n",
    "        dialogue = data[\"dialogue\"]\n",
    "        new_dialogue = []\n",
    "        if isinstance(dialogue[0], list):\n",
    "            dialogue = dialogue[0]\n",
    "        # print(dialogue)\n",
    "        for turn in dialogue:\n",
    "            if turn[\"from\"] in [\"USER\", \"ASSISTANT\"]:\n",
    "                new_dialogue.append(turn)\n",
    "        data[\"dialogue\"] = new_dialogue\n",
    "                \n",
    "print(num_dup_functionalls)\n",
    "print(num_single_api, num_2_apis, num_3_apis, num_multi_apis, num_empty_apis, len(synthetic_data) - num_single_api - num_2_apis - num_3_apis - num_multi_apis - num_empty_apis)\n",
    "\n",
    "with open(output_single_file, \"w\") as f:\n",
    "    for data in single_apis:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(single_apis)} single-apis to {output_single_file}\")\n",
    "with open(output_multi_file, \"w\") as f:\n",
    "    for data in multi_apis:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(multi_apis)} multi-apis to {output_multi_file}\")\n",
    "with open(output_empty_file, \"w\") as f:\n",
    "    for data in empty_apis:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(empty_apis)} empty-apis to {output_empty_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 25451 single-apis to ./synthetic_data/0324/synthetic_data-single_api-0324.jsonl\n",
      "Saved 14181 multi-apis to ./synthetic_data/0324/synthetic_data-multi_apis-0324.jsonl\n",
      "Saved 4397 empty-apis to ./synthetic_data/0324/synthetic_data-empty_apis-0324.jsonl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "synthetic_single_files = [\"./synthetic_data/synthetic_data-single_api-0314.jsonl\",\n",
    "                          \"./synthetic_data/synthetic_data-single_api-0315.jsonl\",  \n",
    "                          \"./synthetic_data/synthetic_data-single_api-0316.jsonl\",  \n",
    "                          \"./synthetic_data/synthetic_data-single_api-0318.jsonl\"]\n",
    "synthetic_multi_files = [\"./synthetic_data/synthetic_data-multi_apis-0314.jsonl\",\n",
    "                         \"./synthetic_data/synthetic_data-multi_apis-0315.jsonl\",\n",
    "                         \"./synthetic_data/synthetic_data-multi_apis-0316.jsonl\",\n",
    "                         \"./synthetic_data/synthetic_data-multi_apis-0318.jsonl\"]\n",
    "synthethic_empty_files = [\"./synthetic_data/synthetic_data-empty_apis-0314.jsonl\",\n",
    "                          \"./synthetic_data/synthetic_data-empty_apis-0315.jsonl\",\n",
    "                          \"./synthetic_data/synthetic_data-empty_apis-0316.jsonl\",\n",
    "                          \"./synthetic_data/synthetic_data-empty_apis-0318.jsonl\"]\n",
    "os.makedirs(\"./synthetic_data/0324\", exist_ok=True)\n",
    "output_synthetic_single_api_file = \"./synthetic_data/0324/synthetic_data-single_api-0324.jsonl\"\n",
    "output_sythetic_multi_apis_file = \"./synthetic_data/0324/synthetic_data-multi_apis-0324.jsonl\"\n",
    "output_sythetic_empty_apis_file = \"./synthetic_data/0324/synthetic_data-empty_apis-0324.jsonl\"\n",
    "\n",
    "\n",
    "synthetic_single_api_data = [json.loads(line) for file in synthetic_single_files for line in open(file) ]\n",
    "synthetic_multi_apis_data = [json.loads(line) for file in synthetic_multi_files for line in open(file) ]\n",
    "synthetic_empty_apis_data = [json.loads(line) for file in synthethic_empty_files for line in open(file) ]\n",
    "# print(f\"{len(synthetic_empty_apis_data)}\")\n",
    "\n",
    "synthetic_single_api_data = random.sample(synthetic_single_api_data, len(synthetic_single_api_data))\n",
    "synthetic_multi_apis_data = random.sample(synthetic_multi_apis_data, len(synthetic_multi_apis_data))\n",
    "synthetic_empty_apis_data = random.sample(synthetic_empty_apis_data, len(synthetic_empty_apis_data))\n",
    "# print(f\"{len(synthetic_empty_apis_data)}\")\n",
    "\n",
    "with open(output_synthetic_single_api_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in synthetic_single_api_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(synthetic_single_api_data)} single-apis to {output_synthetic_single_api_file}\")\n",
    "with open(output_sythetic_multi_apis_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in synthetic_multi_apis_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(synthetic_multi_apis_data)} multi-apis to {output_sythetic_multi_apis_file}\")\n",
    "with open(output_sythetic_empty_apis_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in synthetic_empty_apis_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(synthetic_empty_apis_data)} empty-apis to {output_sythetic_empty_apis_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### toolcall-single_api-0324.jsonl & toocall-multi_apis-0324.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing synthetic data:   0%|          | 0/4397 [00:00<?, ?it/s]\u001b[32m2025-03-24 15:08:05.596\u001b[0m | \u001b[31m\u001b[1mERROR   \u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mget_functions_from_synthetic_data\u001b[0m:\u001b[36m24\u001b[0m - \u001b[31m\u001b[1mfunctioncall=' {\"name\": \"calculate_employee_engagement_score\", \"arguments\": {\"employee_id\": \"E789\", \"survey_results\": \"{\\\\\"satisfaction\\\\\": 6, \\\\\"motivation\\\\\": 5}\", \"peer_feedback\": \\\\\"很努力\\\\\", \"manager_feedback\": \\\\\"表现良好\\\\\", \"job_satisfaction_level\": 7}}'\u001b[0m\n",
      "Processing synthetic data: 100%|██████████| 4397/4397 [00:00<00:00, 172912.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 3785 to ./toolcall-empty_apis-0324.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "synthetic_single_api_file = \"./synthetic_data/0324/synthetic_data-single_api-0324.jsonl\"\n",
    "sythetic_multi_apis_file = \"./synthetic_data/0324/synthetic_data-multi_apis-0324.jsonl\"\n",
    "sythetic_empty_apis_file = \"./synthetic_data/0324/synthetic_data-empty_apis-0324.jsonl\"\n",
    "\n",
    "synthetic_single_api_data = [json.loads(line) for line in open(synthetic_single_api_file)]\n",
    "synthetic_multi_apis_data = [json.loads(line) for line in open(sythetic_multi_apis_file)]\n",
    "synthetic_empty_apis_data = [json.loads(line) for line in open(sythetic_empty_apis_file)]\n",
    "\n",
    "toolcall_apis = json.load(open(toolcall_apis_file))\n",
    "toolcall_apis_keys = list(toolcall_apis.keys())\n",
    "\n",
    "def get_functions_from_synthetic_data(data) -> List:\n",
    "    selected_functions = []\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    bad = False\n",
    "    for turn in reversed(dialogue):\n",
    "        if turn[\"from\"] == \"ASSISTANT\":\n",
    "            content = turn[\"value\"]\n",
    "            if \"<functioncall>\" in content:\n",
    "                functioncall = content.split(\"<functioncall>\")[1].split(\"</functioncall>\")[0]\n",
    "                try:\n",
    "                    functions = json.loads(functioncall)\n",
    "                except:\n",
    "                    logger.error(f\"{functioncall=}\")\n",
    "                    bad = True\n",
    "                    break\n",
    "\n",
    "                if not isinstance(functions, list):\n",
    "                    functions = [functions]\n",
    "\n",
    "                if not all([isinstance(function, dict) for function in functions]):\n",
    "                    bad = True\n",
    "                    break\n",
    "\n",
    "                for function in functions:\n",
    "                    selected_functions.append(function)\n",
    "                break\n",
    "\n",
    "    last_response = selected_functions\n",
    "    if not bad:\n",
    "        last_turn = dialogue[-1]\n",
    "        if last_turn[\"from\"] != \"ASSISTANT\":\n",
    "            bad = True\n",
    "        else:\n",
    "            content = last_turn[\"value\"]\n",
    "            if not \"<functioncall>\" in content:\n",
    "                last_response = content\n",
    "\n",
    "\n",
    "    return selected_functions, last_response, bad\n",
    "\n",
    "def get_user_messages_from_synthetic_data(data) -> List:\n",
    "    user_messages = []\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    for turn in dialogue:\n",
    "        if turn[\"from\"] == \"USER\":\n",
    "            content = turn[\"value\"]\n",
    "            user_messages.append(content)\n",
    "    return user_messages\n",
    "\n",
    "def get_llm_responses_from_synthetic_data(data) -> List:\n",
    "    llm_responses = []\n",
    "    dialogue = data[\"dialogue\"]\n",
    "    for turn in dialogue:\n",
    "        if turn[\"from\"] == \"ASSISTANT\":\n",
    "            content = turn[\"value\"]\n",
    "            llm_responses.append(content)\n",
    "    return llm_responses\n",
    "\n",
    "toolcall_single_api_data = []\n",
    "toolcall_multi_apis_data = []\n",
    "toolcall_empty_apis_data = []\n",
    "\n",
    "output_single_api_file = \"./toolcall-single_api-0324.jsonl\"\n",
    "output_multi_apis_file = \"./toolcall-multi_apis-0324.jsonl\"\n",
    "output_empty_apis_file = \"./toolcall-empty_apis-0324.jsonl\"\n",
    "\n",
    "# 25451\n",
    "# synthetic_datas = synthetic_single_api_data\n",
    "# toolcall_datas = toolcall_single_api_data \n",
    "# output_file = output_single_api_file\n",
    "# prefix = \"s\"\n",
    "\n",
    "# 14181\n",
    "# synthetic_datas = synthetic_multi_apis_data\n",
    "# toolcall_datas = toolcall_multi_apis_data \n",
    "# output_file = output_multi_apis_file\n",
    "# prefix = \"m\"\n",
    "\n",
    "# 3785\n",
    "synthetic_datas = synthetic_empty_apis_data\n",
    "toolcall_datas = toolcall_empty_apis_data\n",
    "output_file = output_empty_apis_file\n",
    "prefix = \"e\"\n",
    "\n",
    "\n",
    "for i, s_data in enumerate(tqdm(synthetic_datas, desc=\"Processing synthetic data\")):\n",
    "    selected_functions, functions, bad = get_functions_from_synthetic_data(s_data)\n",
    "    if bad:\n",
    "        continue\n",
    "\n",
    "    assert isinstance(selected_functions, list), f\"{selected_functions=}\"\n",
    "    if len(selected_functions) == 0:\n",
    "        continue\n",
    "        \n",
    "\n",
    "    user_messages = get_user_messages_from_synthetic_data(s_data)\n",
    "    llm_responses = get_llm_responses_from_synthetic_data(s_data)\n",
    "\n",
    "    id = f\"{prefix}-{len(toolcall_datas)}\"\n",
    "    if not isinstance(functions, list):\n",
    "        data = {\n",
    "            \"id\": id,\n",
    "            \"user_messages\": user_messages,\n",
    "            \"functions\": [[] * len(user_messages)],\n",
    "            \"llm_responses\": llm_responses,\n",
    "            \"apis\": selected_apis,\n",
    "        }\n",
    "        toolcall_datas.append(data)\n",
    "\n",
    "    else:\n",
    "        num_functions = len(functions)\n",
    "        if num_functions == 0:\n",
    "            continue\n",
    "\n",
    "\n",
    "        function_names = [function[\"name\"] for function in selected_functions]\n",
    "        # Random select 4 - 6 apis that are different from the function_names\n",
    "        num_toolcall_apis = random.randint(4, 6)\n",
    "        toolcall_api_names = random.sample(toolcall_apis_keys, num_toolcall_apis + len(function_names))\n",
    "        toolcall_api_names = [name for name in toolcall_api_names if name not in function_names][:num_toolcall_apis]\n",
    "\n",
    "        selected_apis = [toolcall_apis[name] for name in toolcall_api_names + function_names]\n",
    "        selected_apis = random.sample(selected_apis, len(selected_apis))\n",
    "\n",
    "        data = {\n",
    "            \"id\": id,\n",
    "            \"user_messages\": user_messages,\n",
    "            \"functions\": [[] * (len(user_messages) -1)] + [[ {\"type\": \"function\", \"function\": function} for function in functions]],\n",
    "            \"llm_responses\": llm_responses,\n",
    "            \"apis\": selected_apis,\n",
    "        }\n",
    "\n",
    "        toolcall_datas.append(data)\n",
    "\n",
    "toolcall_datas = random.sample(toolcall_datas, len(toolcall_datas))\n",
    "with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in toolcall_datas:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(toolcall_datas)} to {output_file}\")  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ratios = [6, 3, 1]\n",
    "# ratios = [5, 4, 1]\n",
    "tag = \"_\".join([f\"{num}\" for num in ratios])\n",
    "total_train_data = 3000\n",
    "total_test_data = 1000\n",
    "data_dir = \"./\" + tag\n",
    "full_train_file = os.path.join(data_dir, f\"toolcall-train-{total_train_data}-{tag}.jsonl\")\n",
    "full_test_file = os.path.join(data_dir, f\"toolcall-test-{total_test_data}-{tag}.jsonl\")    \n",
    "\n",
    "full_train_data = [json.loads(line) for line in open(full_train_file).readlines()]\n",
    "full_test_data = [json.loads(line) for line in open(full_test_file).readlines()]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "single_api_file = \"./toolcall-single_api-0324.jsonl\"\n",
    "multi_apis_file = \"./toolcall-multi_apis-0324.jsonl\"\n",
    "empty_apis_file = \"./toolcall-empty_apis-0324.jsonl\"\n",
    "single_api_data = [json.loads(line) for line in open(single_api_file).readlines()]\n",
    "multi_apis_data = [json.loads(line) for line in open(multi_apis_file).readlines()]\n",
    "empty_apis_data = [json.loads(line) for line in open(empty_apis_file).readlines()]\n",
    "print(f\"{len(single_api_data)}, {len(multi_apis_data)}, {len(empty_apis_data)}\")\n",
    "\n",
    "weights = list(np.array(ratios) * 0.1)\n",
    "train_data = single_api_data[:int(total_train_data * weights[0])] + multi_apis_data[:int(total_train_data * weights[1])] + empty_apis_data[:int(total_train_data * weights[2])]\n",
    "test_data = single_api_data[int(-total_test_data * weights[0]):] + multi_apis_data[int(-total_test_data * weights[1]):] + empty_apis_data[int(-total_test_data * weights[2]):]\n",
    "full_train_data = random.sample(train_data, len(train_data))\n",
    "full_test_data = random.sample(test_data, len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.makedirs(data_dir, exist_ok=True)\n",
    "with open(full_train_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in full_train_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(train_data)} to {full_train_file}\")\n",
    "with open(full_test_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in full_test_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(test_data)} to {full_test_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Adjust these numbers to change the number of training and testing data\n",
    "num_train_data = 1000\n",
    "num_test_data = 1000\n",
    "\n",
    "train_data = full_train_data[:num_train_data]\n",
    "test_data = full_test_data[:num_test_data]\n",
    "train_data_file = os.path.join(data_dir, f\"toolcall-instructions-intent-train-{num_train_data}-{total_train_data}-{tag}.jsonl\")\n",
    "test_data_file = os.path.join(data_dir, f\"toolcall-instructions-intent-test-{num_test_data}-{total_test_data}-{tag}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from modules.intent.intent_prompt import system_content as intent_system_content\n",
    "\n",
    "def build_instruction_response_from_data(data):\n",
    "    instruction = \"\"\n",
    "    response = \"\"\n",
    "    user_messages = data['user_messages']\n",
    "    apis = data['apis']\n",
    "    toolcalls = data['functions']\n",
    "    output_format = \"json\"\n",
    "    system_content = intent_system_content\n",
    "    instruction = build_instruction(user_messages=user_messages, tools=apis, output_format=output_format, system_content=system_content, include_toolcall_example=False)\n",
    "    # response = build_response_from_functioncalls(functions, output_format=output_format)\n",
    "    # logger.info(f\"{functions=}\")\n",
    "\n",
    "    toolcall = toolcalls[-1]\n",
    "    function_names = [ function[\"function\"][\"name\"] for function in toolcall]\n",
    "    response = \"<function-names>[\" + \", \".join(function_names) + \"]</function-names>\"\n",
    "\n",
    "    # functions = [ function[\"function\"] for function in toolcall]\n",
    "    # function_names = [ function[\"name\"] for function in functions]\n",
    "    # response = \"<function-names>[\" + \", \".join(function_names) + \"]</function-names>\"\n",
    "\n",
    "    return instruction, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toolcall-instructions-intent-train-3000-6_3_1.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "with open(train_data_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in tqdm(train_data, desc=\"Building training data\"):\n",
    "        instruction, response = build_instruction_response_from_data(data)\n",
    "        f.write(json.dumps({\n",
    "                \"id\": data[\"id\"],\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": response,\n",
    "                # \"user_messages\": data['user_messages'],\n",
    "                # \"llm_responses\": data[\"llm_responses\"], \n",
    "                # \"functions\": data['functions'],\n",
    "                # \"apis\": data['apis'],\n",
    "            }, \n",
    "            ensure_ascii=False) + \"\\n\")\n",
    "\n",
    "print(f\"Saved {len(train_data)} to {train_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toolcall-instructions-intent-test-1000-6_3_1.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 1000/1000 [00:00<00:00, 11488.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 to ./6_3_1/toolcall-instructions-intent-test-1000-1000-6_3_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(test_data_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in tqdm(test_data, desc=\"Building test data\"):\n",
    "        instruction, response = build_instruction_response_from_data(data)\n",
    "        f.write(json.dumps({\n",
    "                \"id\": data[\"id\"],\n",
    "                \"instruction\": instruction,\n",
    "                \"response\": response,\n",
    "                \"user_messages\": data['user_messages'],\n",
    "                \"llm_responses\": data[\"llm_responses\"], \n",
    "                \"functions\": data['functions'],\n",
    "                \"apis\": data['apis'],\n",
    "            }, \n",
    "            ensure_ascii=False) + \"\\n\")\n",
    "print(f\"Saved {len(test_data)} to {test_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust these numbers to change the number of training and testing data\n",
    "num_train_data = 3000\n",
    "num_test_data = 1000\n",
    "\n",
    "train_data = full_train_data[:num_train_data]\n",
    "test_data = full_test_data[:num_test_data]\n",
    "train_data_file = os.path.join(\n",
    "    data_dir, f\"toolcall-instructions-ner-train-{num_train_data}-{total_train_data}-{tag}.jsonl\"\n",
    ")\n",
    "test_data_file = os.path.join(data_dir, f\"toolcall-instructions-ner-test-{num_test_data}-{total_test_data}-{tag}.jsonl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from modules.ner.ner_prompt import system_content as ner_system_content\n",
    "\n",
    "def build_instruction_response_from_data(data):\n",
    "    instruction = \"\"\n",
    "    response = \"\"\n",
    "    user_messages = data['user_messages']\n",
    "    apis = data['apis']\n",
    "    apis_dict = {\n",
    "        api[\"name\"]: api\n",
    "        for api in apis\n",
    "    }\n",
    "    toolcalls = data['functions']\n",
    "    output_format = \"json\"\n",
    "    system_content = ner_system_content\n",
    "\n",
    "    instruction = build_instruction(\n",
    "        user_messages=user_messages,\n",
    "        tools=apis,\n",
    "        output_format=output_format,\n",
    "        system_content=system_content,\n",
    "        include_toolcall_example=False\n",
    "    )\n",
    "    # response = build_response_from_functioncalls(functions, output_format=output_format)\n",
    "    # logger.info(f\"{functions=}\")\n",
    "\n",
    "    toolcall = toolcalls[-1]\n",
    "    functions = [function[\"function\"] for function in toolcall]\n",
    "\n",
    "    response = \"\"\n",
    "    for function in functions:\n",
    "        func_name = function[\"name\"]\n",
    "        func_arguments = function[\"arguments\"]\n",
    "        api = apis_dict[func_name]\n",
    "        api_description = api['description']\n",
    "        api_parameters = api['parameters']['properties']\n",
    "\n",
    "        func_arguments_list = []\n",
    "        for arg_name, arg_value in func_arguments.items():\n",
    "            if len(str(arg_value)) == 0:\n",
    "                continue\n",
    "            api_param = api_parameters.get(arg_name)\n",
    "            if api_param is None:\n",
    "                continue\n",
    "            arg_type = api_param['type']\n",
    "            arg_description = api_param['description']\n",
    "\n",
    "            data = f\"| {arg_name} | {str(arg_value)} | {arg_type} | {arg_description} |\"\n",
    "            func_arguments_list.append(data)\n",
    "        func_arguments_str = \"\\n\".join(func_arguments_list)\n",
    "        response += f\"<function-name>| {func_name} | {api_description} |</function-name>\\n<function-arguments>\\n{func_arguments_str}\\n</function-arguments>\"\n",
    "\n",
    "    return instruction, response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toolcall-instructions-ner-train-3000-6_3_1.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(train_data_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in tqdm(train_data, desc=\"Building training data\"):\n",
    "        instruction, response = build_instruction_response_from_data(data)\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": data[\"id\"],\n",
    "                    \"instruction\": instruction,\n",
    "                    \"response\": response,\n",
    "                    # \"user_messages\": data['user_messages'],\n",
    "                    # \"llm_responses\": data[\"llm_responses\"],\n",
    "                    # \"functions\": data['functions'],\n",
    "                    # \"apis\": data['apis'],\n",
    "                },\n",
    "                ensure_ascii=False\n",
    "            ) + \"\\n\"\n",
    "        )\n",
    "\n",
    "print(f\"Saved {len(train_data)} to {train_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### toolcall-instructions-ner-test-1000-6_3_1.jsonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building test data: 100%|██████████| 1000/1000 [00:00<00:00, 10558.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 1000 to ./6_3_1/toolcall-instructions-ner-test-1000-1000-6_3_1.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "with open(test_data_file, \"w\", encoding=\"utf-8\") as f:\n",
    "    for data in tqdm(test_data, desc=\"Building test data\"):\n",
    "        instruction, response = build_instruction_response_from_data(data)\n",
    "        f.write(\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"id\": data[\"id\"],\n",
    "                    \"instruction\": instruction,\n",
    "                    \"response\": response,\n",
    "                    \"user_messages\": data['user_messages'],\n",
    "                    \"llm_responses\": data[\"llm_responses\"],\n",
    "                    \"functions\": data['functions'],\n",
    "                    \"apis\": data['apis'],\n",
    "                },\n",
    "                ensure_ascii=False\n",
    "            ) + \"\\n\"\n",
    "        )\n",
    "print(f\"Saved {len(test_data)} to {test_data_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intent Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file = \"\"\n",
    "results_file = \"\"\n",
    "test_datas = [json.loads(line) for line in open(test_file).readlines()]\n",
    "results_data = [json.loads(line) for line in open(results_file).readlines()]\n",
    "assert len(test_datas) == len(results_data)\n",
    "\n",
    "from collections import Counter\n",
    "def get_function_names_from_data(data):\n",
    "    function_names = data.split(\"<function-names>\")[1].split(\"</function-names>\")[0].strip()\n",
    "    function_names = function_names.split(\",\")\n",
    "    function_names = [name.strip() for name in function_names]\n",
    "    function_names = sorted(function_names)\n",
    "\n",
    "    return function_names\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "for result_data, test_data  in tqdm(zip(results_data, test_datas), desc=\"Evaluating results\"):\n",
    "    test_response = test_data['response']\n",
    "    test_function_names = get_function_names_from_data(test_response)\n",
    "\n",
    "    result_response = result_data['response']\n",
    "    result_function_names = get_function_names_from_data(result_response)\n",
    "\n",
    "    if test_function_names == result_function_names:\n",
    "        score = 1\n",
    "        all_scores.append(score)\n",
    "        continue\n",
    "\n",
    "    mc_test_function_names = Counter(test_function_names).most_common()\n",
    "    test_names_count = {name: count for name, count in mc_test_function_names}\n",
    "\n",
    "    mc_result_function_names = Counter(result_function_names).most_common()\n",
    "    result_name_count = { name: count for name, count in mc_result_function_names}\n",
    "\n",
    "    common_names = set(test_function_names) & set(result_function_names)\n",
    "    TP = [ min(test_names_count.get(name, 0), result_name_count.get(name, 0)) for name in common_names]\n",
    "    FP = [ min(result_name_count.get(name, 0) - test_names_count.get(name, 0), 0) for name in set(test_function_names)]\n",
    "    # FN = [ min(result_name_count.get(name, 0) - test_names_count.get(name, 0), 0) for name in set(result_function_names) - set(test_function_names)]\n",
    "    # TP, FP, FN = sum(TP), sum(FP), sum(FN)\n",
    "    # # score = TP / (TP + FP + FN)\n",
    "    # acc = TP / (TP + FN)\n",
    "    # callback = TP / (TP + FP)\n",
    "    # F1 = 2 * acc * callback / (acc + callback)\n",
    "    # score = F1 \n",
    "    acc = TP / (TP + FP)\n",
    "    score = acc \n",
    "\n",
    "    all_scores.append(score)\n",
    "\n",
    "score = sum(all_scores) / len(all_scores)\n",
    "print(f\"{score=}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NER Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "test_file = \"\"\n",
    "results_file = \"\"\n",
    "test_datas = [json.loads(line) for line in open(test_file).readlines()]\n",
    "results_data = [json.loads(line) for line in open(results_file).readlines()]\n",
    "assert len(test_datas) == len(results_data)\n",
    "\n",
    "from collections import Counter\n",
    "def get_function_arguments_from_data(data):\n",
    "    function_names = data.split(\"<function-names>\")[1].split(\"</function-names>\")[0].strip()\n",
    "    function_names = function_names.split(\",\")\n",
    "    function_names = [name.strip() for name in function_names]\n",
    "    function_names = sorted(function_names)\n",
    "\n",
    "    return function_names\n",
    "\n",
    "\n",
    "all_scores = []\n",
    "for result_data, test_data  in tqdm(zip(results_data, test_datas), desc=\"Evaluating results\"):\n",
    "    test_response = test_data['response']\n",
    "    test_function_names = get_function_names_from_data(test_response)\n",
    "\n",
    "    result_response = result_data['response']\n",
    "    result_function_arguments = get_function_arguments_from_data(result_response)\n",
    "\n",
    "    if test_function_names == result_function_names:\n",
    "        score = 1\n",
    "    else:\n",
    "        mc_test_function_names = Counter(test_function_names).most_common()\n",
    "        test_names_count = {name: count for name, count in mc_test_function_names}\n",
    "\n",
    "        mc_result_function_names = Counter(result_function_names).most_common()\n",
    "        result_name_count = { name: count for name, count in mc_result_function_names}\n",
    "\n",
    "        common_names = set(test_function_names) & set(result_function_names)\n",
    "        TP = [ min(test_names_count.get(name, 0), result_name_count.get(name, 0)) for name in common_names]\n",
    "        FP = [ min(test_names_count.get(name, 0) - result_name_count.get(name, 0), 0) for name in set(test_function_names) - set(result_function_names)]\n",
    "        # FN = [ min(result_name_count.get(name, 0) - test_names_count.get(name, 0), 0) for name in set(result_function_names) - set(test_function_names)]\n",
    "        # TP, FP, FN = sum(TP), sum(FP), sum(FN)\n",
    "        # # score = TP / (TP + FP + FN)\n",
    "        # acc = TP / (TP + FN)\n",
    "        # callback = TP / (TP + FP)\n",
    "        # F1 = 2 * acc * callback / (acc + callback)\n",
    "        # score = F1 \n",
    "        acc = TP / (TP + FP)\n",
    "        score = acc \n",
    "        \n",
    "\n",
    "    all_scores.append(score)\n",
    "\n",
    "score = sum(all_scores) / len(all_scores)\n",
    "print(f\"{score=}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
