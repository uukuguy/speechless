{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary Generation Round B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from knowledge_base import KnowledgeBase\n",
    "kb = KnowledgeBase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to get data: response.status_code=422, response=<Response [422]>\n"
     ]
    }
   ],
   "source": [
    "kb.keywords_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 454846909877948052,\n",
       " 'distance': 0.6174427270889282,\n",
       " 'entity': {'paper_id': '658254cf939a5f4082bc95a5',\n",
       "  'paper_title': 'Point Cloud Part Editing: Segmentation, Generation, Assembly, and Selection',\n",
       "  'chunk_id': 3,\n",
       "  'chunk_text': '# Loss Functions\\nWe adopt the loss function introduced in Wasserstein GAN (Arjovsky, Chintala, and Bottou 2017) with gradient penalty (Gulrajani et al. 2017). Network $E$ ${\\\\vec{\\\\mathfrak{z}}},\\\\,G_{p i},i=1...n,$ ,$F_{p i},i=1...n$ , and $F$ need training. The losses are given as:  \\n\\n$$\\n\\\\begin{array}{c c}{\\\\displaystyle{\\\\mathcal{L}_{G}=-\\\\alpha\\\\ast\\\\frac{1}{n}\\\\sum_{i=1}^{n}\\\\mathbb{E}_{z\\\\sim Z}[F_{p i}(G_{p i}(z))]}}\\\\\\\\ {\\\\displaystyle}\\\\\\\\ {\\\\displaystyle}\\\\\\\\ {\\\\displaystyle}\\\\\\\\ {\\\\displaystyle{\\\\mathcal{L}_{F_{p}}=\\\\frac{1}{n}\\\\sum_{i=1}^{n}(\\\\mathbb{E}_{z\\\\sim Z}[F(\\\\bigcup_{i=1}^{n}G_{p i}(z))]}}\\\\\\\\ {\\\\displaystyle}\\\\\\\\ {\\\\displaystyle{\\\\mathcal{L}_{F_{p}}=\\\\frac{1}{n}\\\\sum_{i=1}^{n}(\\\\mathbb{E}_{z\\\\sim Z}[F_{p i}(G_{p i}(z))]-\\\\mathbb{E}_{x_{i}\\\\sim R_{x i}}[F_{p i}(x_{i})]}}\\\\\\\\ {\\\\displaystyle}\\\\\\\\ {\\\\displaystyle{+\\\\lambda_{g p}\\\\mathbb{E}_{\\\\hat{x}_{i}}[(\\\\lVert\\\\nabla_{\\\\hat{x}_{i}}F_{p i}(\\\\hat{x}_{i})\\\\rVert_{2}-1)^{2}])}}\\\\end{array}\\n$$  \\n\\n$$\\n\\\\begin{array}{r}{\\\\mathcal{L}_{F}=\\\\mathbb{E}_{z\\\\sim Z,x_{i}\\\\sim P_{x i}}[F(\\\\bigcup_{i=1}^{n}M_{i}G_{p i}(z))+(1-M_{i})E_{p i}(x_{i})]}\\\\\\\\ {-\\\\mathbb{E}_{x_{i}\\\\sim R_{x i}}[F(\\\\bigcup_{i=1}^{n}x_{i})]+\\\\lambda_{g p}\\\\mathbb{E}_{\\\\hat{x}_{i}}[(\\\\|\\\\nabla_{\\\\hat{x}_{i}}F(\\\\bigcup_{i=1}^{n}\\\\hat{x}_{i})\\\\|_{2}-1)^{2}]}\\\\\\\\ {\\\\underset{i=1}{\\\\overset{n}{-}}\\\\times\\\\relax_{k}[F(\\\\bigcup_{i=1}^{n}x_{i})]+\\\\lambda_{g p}\\\\mathbb{E}_{\\\\hat{x}_{i}}[(\\\\|\\\\nabla_{\\\\hat{x}_{i}}F(\\\\bigcup_{i=1}^{n}\\\\hat{x}_{i})\\\\|_{2}-1)^{2}]}\\\\end{array}\\n$$  \\n\\nwhere the part-level generator, the part-level discriminator, and the $\\\\mathcal{L}_{G},\\\\;\\\\mathcal{L}_{F_{p}}$ , and ${\\\\mathcal{L}}_{F}$ represent the loss functions of global discriminator respectively. $\\\\alpha$ and $\\\\beta$ are hyperparameters that control the proportion of the part-level to the global GAN. $N$ is the number of parts. $x_{i},i=1...n$ are parts. The formulas after $\\\\lambda_{g p}$ are the gradient penalty terms proposed by Gulrajani et a 7). $\\\\boldsymbol{Z}=\\\\mathcal{N}(\\\\boldsymbol{\\\\mu},\\\\boldsymbol{\\\\Sigma^{2}})$ , where $\\\\mu$ and $\\\\Sigma$ are from encoder $E,\\\\,M$ is determined by the unedited parts.\\n\\n# Experiments\\n\\n# Datasets and Implementation Details\\nWe evaluate SGAS on PartNet (Mo et al. 2019b) dataset. By merging fine-grained semantic labels and removing some special objects, we create a new dataset called PartNet.v0.Merged for point cloud part editing. Following previous works (Gal et al. 2021; Li, Liu, and Walder 2022), we perform unsupervised part-aware point cloud generation on ShapeNet-Partseg (Yi et al. 2016) dataset. We do not use semantic labels in the ShapeNet-Partseg dataset.  \\n\\nAdam optimizers are used for SGAS with a learning rate of $\\\\alpha=0.0005$ , coefficients $\\\\beta_{1}=0.5$ and $\\\\beta_{2}=0.99$ . All the experiments are performed on a single NVIDIA TITAN Xp for 2000 epochs with a batch size of 200. In loss functions, $\\\\alpha$ and $\\\\beta$ are set to 1 and 1. $\\\\lambda_{g p}$ is set to 10. The threshold in Part Select is set to 0.5. We update the discriminator 5 times for each update of the generator. Each shape has 2048 points while each part has $\\\\left\\\\lfloor{\\\\frac{2048}{n}}\\\\right\\\\rfloor$ \\x05points. $n$ is the number of parts. During training, the input unedited parts for SGAS are obtained by randomly removing 1 to $n-1$ parts of objects in PartNet.v0.Merged dataset.  \\n\\nTable 1: Diversity part editing performance. MMD and TMD measure the quality and diversity respectively.   \\n\\n\\n<html><body><table><tr><td></td><td>Model</td><td>Chair</td><td>Lamp</td><td>Table</td><td>Average</td></tr><tr><td rowspan=\"2\">MMD ←</td><td>MSC-cGAN</td><td>1.62</td><td>3.41</td><td>1.39</td><td>2.14</td></tr><tr><td>SGAS</td><td>1.33</td><td>2.26</td><td>1.06</td><td>1.55</td></tr><tr><td></td><td>MSC-cGAN</td><td>5.45</td><td>3.94</td><td>5.14</td><td>4.84</td></tr><tr><td>TMD ↑</td><td>SGAS</td><td>4.36</td><td>4.48</td><td>8.04</td><td>5.63</td></tr></table></body></html>  \\n\\n  \\nFigure 3: Performance on the new metric Total Mutual Difference Surface (TMDS). Red represents SGAS; blue represents MSC-cGAN. The smaller the thresholds of MMD and UHD are, the more referential the calculated TMD is.',\n",
       "  'original_filename': 'Conf_Paper_Meta_Data_AAAI2024_with_whole_text.db',\n",
       "  'year': 2024}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/search_papers\n",
    "基于文本内容搜索相关论文片段。\n",
    "[\n",
    "    {\n",
    "        'id': 454846909877948052, \n",
    "        'distance': 0.6174427270889282,\n",
    "        'entity': {\n",
    "            'paper_id': '658254cf939a5f4082bc95a5',\n",
    "            'paper_title': 'Point Cloud Part Editing: Segmentation, Generation, Assembly, and Selection',\n",
    "            'chunk_id': 3,\n",
    "            'chunk_text': '# Loss Functions\\nWe adopt the loss function introduced in Wasserstein GAN ...',\n",
    "            'original_filename': 'Conf_Paper_Meta_Data_AAAI2024_with_whole_text.db',\n",
    "            'year': 2024\n",
    "        }\n",
    "    },\n",
    "]\n",
    "\"\"\"\n",
    "query = \"损失函数\"\n",
    "chunks = kb.search_papers(query, top_k=1)\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/query_whole_text_by_id\n",
    "通过论文ID查询论文全文。\n",
    "\"\"\"\n",
    "paper_id = '658254cf939a5f4082bc95a5'\n",
    "paper_text = kb.query_whole_text_by_id(paper_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "/query_by_paper_id\n",
    "根据论文ID查询相关的论文片段。\n",
    "\"\"\"\n",
    "paper_id = '658254cf939a5f4082bc95a5'\n",
    "chunks = kb.query_by_paper_id(paper_id, top_k=1000)\n",
    "chunks = sorted(chunks, key=lambda x: ['chunk_id'])\n",
    "chunks_text = \"\\n\".join([chunk['chunk_text'] for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 454845485404913716,\n",
       " 'paper_id': '6350bc6d90e50fcafdecf289',\n",
       " 'paper_title': 'RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses.',\n",
       " 'chunk_id': 0,\n",
       " 'chunk_text': '# RankT5: Fine-Tuning T5 for Text Ranking with Ranking Losses\\nHonglei Zhuang, Zhen Qin, Rolf Jagerman, Kai Hui, Ji Ma, Jing Lu, Jianmo Ni, Xuanhui Wang and Michael Berdersky Google Research  \\n\\n{hlz,zhenqin,jagerman,kaihuibj,maji,ljwinnie,jianmon, xuanhui,bemike}@google.com\\n\\n# Abstract\\nRecently, substantial progress has been made in text ranking based on pretrained language models such as BERT. However, there are limited studies on how to leverage more powerful sequence-to-sequence models such as T5. Existing attempts usually formulate text ranking as classification and rely on postprocessing to obtain a ranked list. In this paper, we propose RankT5 and study two T5-based ranking model structures, an encoder-decoder and an encoder-only one, so that they not only can directly output ranking scores for each querydocument pair, but also can be fine-tuned with “pairwise” or “listwise” ranking losses to optimize ranking performances. Our experiments show that the proposed models with ranking losses can achieve substantial ranking performance gains on different public text ranking data sets. Moreover, when fine-tuned with listwise ranking losses, the ranking model appears to have better zero-shot ranking performance on out-of-domain data sets compared to the model fine-tuned with classification losses.\\n\\n# 1 Introduction\\nText ranking refers to the task of ranking a set of textual documents based on their relevance to a given query or context. Learning a text ranking model is a fundamental component of countless real world applications such as search and question answering. Earliest explorations ( Liu ,2009 )mostly rely on handcrafted numerical features to represent each query-document pair ( Qin and Liu ,2013 ;Chapelle and Chang ,2011 ) and put more emphasis on the learning algorithms such as ranking losses ( Qin et al. ,2021 ). Progress on pretrained language models in the past few years ( Devlin et al. ,2019 ) and the release of large-scale public data sets ( Bajaj et al. ,2016 ;Kwiatkowski et al. ,2019 )enable a series of work ( Lin et al. ,2020 ;Nogueira et al. ,2019 ;Han et al. ,2020 ) on text ranking models which directly encode textual query and document using pretrained language models, noticeably BERT ( Devlin et al. ,2019 ).  \\n\\nRecently, pretrained language models such as T5 ( Raffel et al. ,2020 ) and GPT3 ( Brown et al. ,2020 ) have shown superior performance in various NLP tasks including sentiment analysis, coreference resolution, and translation. Such models often have much larger size available than previous models such as BERT ( Devlin et al. ,2019 ) to store more hidden knowledge. They also mostly have a sequence-to-sequence interface to unify different NLP tasks from classification to text generation.  \\n\\nWhile BERT-based models have been well explored for text ranking ( Lin et al. ,2020 ;Nogueira et al. ,2019 ;Han et al. ,2020 ), how to leverage T5 for text ranking is still under-explored and challenging. First, while many classification and text generation tasks fit into the sequence-to-sequence framework, it is more tricky for text ranking tasks: a text ranking model is often expected to output a numerical ranking score $\\\\hat{y}\\\\in\\\\mathbb R$ for each querydocument pair. Second, it is important to train a text ranking model with ranking losses to optimize its ranking performance, where the losses take into account the ranking scores from multiple documents for each query. This is different from the typical T5 fine-tuning strategy where the objective is often formulated into a text generation loss for each single input sequence independently.  \\n\\nA typical approach to use T5 for text ranking is to convert the problem into a token generation problem. For example, Nogueira et al. (2020 ) finetune the T5 model to predict a “ true ” or “ false ”token for a relevant or irrelevant query-document pair and then use a postprocessing step during inference to derive ranking scores to rank candidate documents. Such an approach can be considered a “pointwise” classification formulation. How to extend this approach to fine-tune T5 with ranking losses is unclear.  \\n\\nIn this paper, we propose RankT5 with the goal to support text ranking more natively with T5 by outputting real numbers, instead of text tokens. We first adapt the encoder-decoder structure for this goal. In addition, we also propose an alternative structure which omits the decoder from T5 and outputs real numbers based on the encoder, called the encoder-only structure. These two structure variants allow us to fine-tune T5 with various ranking losses to directly optimize ranking performance.  \\n\\nExperiments on MS MARCO and Natural Question data sets show that our RankT5 models finetuned with specialized ranking losses can significantly outperform other T5 ranking models finetuned with classification losses and previously proposed T5 adaptations for ranking ( Nogueira et al. ,2020 ). We also discover that models fine-tuned with some ranking losses tend to have better zeroshot performance than models fine-tuned with classification losses.\\n\\n# 2 Related Work\\nLeveraging pretrained language models for text ranking tasks is becoming the state-of-the-art ( Lin et al. ,2020 ). We review recent literature on utilizing such models for text ranking tasks.  \\n\\nModel structure. Pretrained language models accept a text sequence as input. A typical model structure design is the cross-attention model structure, where a query and a candidate document is concatenated into a sequence and fed into the model. It allows the attention mechanism to fully capture the query-document interaction. This model structure has been explored for BERT-like encoder-only model ( Han et al. ,2020 ;Nogueira et al. ,2019 ;Gao et al. ,2021 ). This is also explored in the T5-like model ( Nogueira et al. ,2020 ;Ju et al. ,2021 ), but the model is not directly finetuned with ranking losses for the optimal ranking performance. Instead, the model is fine-tuned to generate “ true ” or “ false ” tokens. The ranking scores are later derived in postprocessing from the predicted logits. There are also model structures that take a triple with one query and two documents as input ( Pradeep et al. ,2021 ), but these models can only be deployed in the late ranking stage to work with tens of documents because they require scoring all possible input document pairs, which is not scalable. Our contributions are in the early ranking stage which scores thousands of input documents; they are thus complementary with this work.  \\n\\nThere are also works exploring ranking models based on generative likelihood from language models. For example, existing works in ( Zhuang and Zuccon ,2021 ;Zhuang et al. ,2021 ;Sachan et al. ,2022 ;dos Santos et al. ,2020 ) take a candidate document as input and estimates the likelihood of generating the query. Then they rank documents based on the query-likelihood or use it as an auxiliary task ( Ju et al. ,2021 ). These models convert ranking into a generation task in order to leverage generative models such as T5, instead of directly enabling T5 to output numerical scores.  \\n\\nThere are a few other model structures proposed for retrieval tasks, where the model needs to score hundreds of millions of documents in the entire corpus almost instantly for each arriving query. These model structures emphasize more on model effi- ciency. Typical examples for BERT-like models are dual-encoder models ( Karpukhin et al. ,2020 ) and encoders with late interaction ( Khattab and Zaharia ,2020 ;Gao et al. ,2020 ;MacAvaney et al. ,2020 ), where the model computation of queries and documents are decomposed to allow precomputations for documents in the corpus. Some other works also utilize the predictive likelihood from generative T5-like models for retrieval. Examples include using the likelihood of generating queries from documents ( Hui et al. ,2022 ) or using the likelihood of generative document IDs from queries ( Tay et al. ,2022 ). However, our paper focuses on the ranking task, where a small set of candidates documents are already retrieved for each query. This allows us to focus on cross-attention model structure and push for better ranking performance.  \\n\\nFine-tuning with ranking losses. Early explorations ( Nogueira et al. ,2019 ,2020 ) of applying pretrained language models on the document reranking task mainly use “pointwise” losses, where the loss is calculated for each querydocument pair independently. There are some recent works that use a “listwise” loss ( Han et al. ,2020 ;Gao et al. ,2021 ;Ren et al. ,2021 ) which takes one positive and multiple negative documents for each query and calculates the softmax loss over their ranking scores. But they only fine-tune BERT, RoBERTa etc. There is no existing work finetuning sequence-to-sequence models like T5 with ranking losses .  \\n\\nSome retriever models are fine-tuned using pairwise loss or softmax loss ( Xiong et al. ,2020 ;Karpukhin et al. ,2020 ;Lu et al. ,2021 ). However, in this work we only focus on reranking models.',\n",
       " 'original_filename': 'Conf_Paper_Meta_Data_SIGIR2023_with_whole_text.db',\n",
       " 'year': 2023}"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/query_by_title_contain\n",
    "搜索标题中包含特定关键词的论文片段。\n",
    "严格匹配，区分大小写。\n",
    "\"\"\"\n",
    "chunks = kb.query_by_title_contain(\"Loss\")\n",
    "chunks[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/query_by_title_like\n",
    "搜索标题中包含特定关键词的论文片段。\n",
    "模糊匹配，不区分大小写。\n",
    "返回值: 嵌套的论文片段信息列表。 外层列表对应每个相似标题，内层列表是与该相似标题相关的论文片段信息列表 (结构同 /search_papers)。\n",
    "\"\"\"\n",
    "chunks = kb.query_by_title_like(\"loSs\", top_k=1)\n",
    "len(chunks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': 454845515631169384,\n",
       " 'paper_id': '6432998690e50fcafd467a44',\n",
       " 'paper_title': 'Cross-Modal Retrieval with Partially Mismatched Pairs',\n",
       " 'chunk_id': 7,\n",
       " 'chunk_text': '# $D$ . Comparison With Rectifying Method\\nIn this section, we compare our RCL with the most related method NCR [43] to investigate the effectiveness and efficiency of the proposed learning paradigm. First, NCR requires simultaneously training two individual cross-modal models with GMM in a co-teaching manner, which will take a relatively high computational cost. In contrast, our method does not introduce extra training costs into the original cross-modal method, thus embracing higher efficiency. Second, we conduct some comparisons with NCR in Table V. From the experiments, one could see that both NCR and our RCL achieve comparable retrieval performance in low mismatching rates (e.g., 0.2 and 0.4). However, the performance of NCR will fast degrade with high mismatching rates (e.g., 0.6 and 0.8) because NCR cannot correctly distinguish true positives from false positives when the PMPs dominate in the training data. Furthermore, one could find that NCR achieves worse performance under PMPs comparing the results reported in [43] , which demonstrates that our PMP injection approach is more challenging than that used in NCR.  \\n\\nTABLE V COMPARISONWITHNCR[43]UNDERDIFFERENTMISMATCHINGRATES(MRATE)ONMS-COCOANDFLICKR30K  \\n\\n\\n<html><body><table><tr><td rowspan=\"3\">MRate</td><td rowspan=\"3\">Method</td><td colspan=\"6\">MS-COCO</td><td colspan=\"6\"></td><td colspan=\"3\"></td></tr><tr><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td><td colspan=\"2\">rSum</td><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td></tr><tr><td>R@1</td><td>R@5</td><td>R@10</td><td></td><td>R@1 R@5</td><td>R@10</td><td></td><td>R@1</td><td>R@5</td><td></td><td>R@10</td><td>Flickr30K R@1</td><td>R@5 R@10</td><td></td><td>rSum</td></tr><tr><td rowspan=\"5\">0.2</td><td>NCR* [43]</td><td>73.7</td><td>94.5</td><td>97.7</td><td>58.3</td><td>88.7</td><td>94.0</td><td>506.9</td><td>69.9</td><td>92.0</td><td>95.4</td><td>52.6</td><td>79.4</td><td>86.8</td><td>476.1</td></tr><tr><td>RCL-SAF</td><td>77.1</td><td>95.5</td><td>98.2</td><td>61.0</td><td>88.8</td><td>94.6</td><td>515.2</td><td>72.0</td><td>91.7</td><td>95.8</td><td>53.6</td><td>79.9</td><td>86.7</td><td>479.7</td></tr><tr><td>RCL-SGR</td><td>77.0</td><td>95.5</td><td>98.1</td><td>61.3</td><td>88.8</td><td>94.8</td><td>515.5</td><td>74.2</td><td>91.8</td><td>96.9</td><td>55.6</td><td>81.2</td><td>87.5</td><td>487.2</td></tr><tr><td>NCR[43]</td><td>76.6</td><td>95.6</td><td>98.2</td><td>60.8</td><td>88.8</td><td>95.0</td><td>515.0</td><td>73.5</td><td>93.2</td><td>96.6</td><td>56.9</td><td>82.4</td><td>88.5</td><td>491.1</td></tr><tr><td>RCL-SGRAF</td><td>78.9</td><td>96.0</td><td>98.4</td><td>62.8</td><td>89.9</td><td>95.4</td><td>521.4</td><td>75.9</td><td>94.5</td><td>97.3</td><td>57.9</td><td>82.6</td><td>88.6</td><td>496.8</td></tr><tr><td rowspan=\"5\">0.4</td><td>NCR* [43]</td><td>71.7</td><td>93.9</td><td>97.5</td><td>56.7</td><td>86.8</td><td>94.0</td><td>500.6</td><td>61.6</td><td>88.3</td><td>92.8</td><td>46.9</td><td>74.5</td><td>82.3</td><td>446.4</td></tr><tr><td>RCL-SAF</td><td>74.8</td><td>94.8</td><td>97.8</td><td>59.0</td><td>87.1</td><td>93.9</td><td>507.4</td><td>68.8</td><td>89.8</td><td>95.0</td><td>51.0</td><td>76.7</td><td>84.8</td><td>466.1</td></tr><tr><td>RCL-SGR</td><td>73.9</td><td>94.9</td><td>97.9</td><td>59.0</td><td>87.4</td><td>93.9</td><td>507.0</td><td>71.3</td><td>91.1</td><td>95.3</td><td>51.4</td><td>78.0</td><td>85.2</td><td>472.3</td></tr><tr><td>NCR [43]</td><td>74.7</td><td>94.6</td><td>98.0</td><td>59.6</td><td>88.1</td><td>94.7</td><td>509.7</td><td>68.1</td><td>89.6</td><td>94.8</td><td>51.4</td><td>78.4</td><td>84.8</td><td>467.1</td></tr><tr><td>RCL-SGRAF</td><td>77.0</td><td>95.5</td><td>98.3</td><td>61.2</td><td>88.5</td><td>94.8</td><td>515.3</td><td>72.7</td><td>92.7</td><td>96.1</td><td>54.8</td><td>80.0</td><td>87.1</td><td>483.4</td></tr><tr><td rowspan=\"6\">0.6</td><td>NCR* [43]</td><td>0.1</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.5</td><td>1.0</td><td>2.4</td><td>13.7</td><td>34.7</td><td>46.9</td><td>10.1</td><td>27.4</td><td>38.4</td><td>171.2</td></tr><tr><td>RCL-SAF</td><td>70.1</td><td>93.1</td><td>96.8</td><td>54.5</td><td>84.4</td><td>91.9</td><td>490.8</td><td>63.9</td><td>84.8</td><td>91.7</td><td>43.0</td><td>71.2</td><td>79.4</td><td>434.0</td></tr><tr><td>RCL-SGR</td><td>71.4</td><td>93.2</td><td>97.1</td><td>55.4</td><td>84.7</td><td>92.3</td><td>494.1</td><td>62.3</td><td>86.3</td><td>92.9</td><td>45.1</td><td>71.3</td><td>80.2</td><td>438.1</td></tr><tr><td>NCR [43]</td><td>0.1</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.5</td><td>1.0</td><td>2.4</td><td>13.9</td><td>37.7</td><td>50.5</td><td>11.0</td><td>30.1</td><td>41.4</td><td>184.6</td></tr><tr><td>RCL-SGRAF</td><td>74.0</td><td>94.3</td><td>97.5</td><td>57.6</td><td>86.4</td><td>93.5</td><td>503.3</td><td>67.7</td><td>89.1</td><td>93.6</td><td>48.0</td><td>74.9</td><td>83.3</td><td>456.6</td></tr><tr><td>NCR* [43]</td><td>0.1</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.5</td><td>1.0</td><td>2.4</td><td>0.9</td><td>2.7</td><td>4.7</td><td>0.2</td><td>0.8</td><td>1.6</td><td>10.9</td></tr><tr><td rowspan=\"5\">0.8</td><td>RCL-SAF</td><td>62.9</td><td>89.3</td><td>94.9</td><td>47.1</td><td>77.9</td><td>87.4</td><td>459.5</td><td>45.0</td><td>72.8</td><td>80.8</td><td>30.7</td><td>56.5</td><td>67.3</td><td>353.1</td></tr><tr><td>RCL-SGR</td><td>63.2</td><td>89.3</td><td>95.2</td><td>47.6</td><td>78.7</td><td>88.0</td><td>462.0</td><td>47.1</td><td>70.5</td><td>79.4</td><td>30.3</td><td>56.1</td><td>66.3</td><td>349.7</td></tr><tr><td>NCR [43]</td><td>0.1</td><td>0.3</td><td>0.4</td><td>0.1</td><td>0.5</td><td>1.0</td><td>2.4</td><td>1.5</td><td>6.2</td><td>9.9</td><td>0.3</td><td>1.0</td><td>2.1</td><td>21.0</td></tr><tr><td>RCL-SGRAF</td><td>67.4</td><td>90.8</td><td>96.0</td><td>50.6</td><td>81.0</td><td>90.1</td><td>475.9</td><td>51.7</td><td>75.8</td><td>84.4</td><td>34.5</td><td>61.2</td><td>70.7</td><td>378.3</td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td><td></td></tr></table></body></html>  \\n\\n  \\nFig. 6. The ability of our RCL to capture latent semantics for cross-modal retrieval with MRate $_{:=0.6}$ . The figure shows some retrieved examples of the image-to-text (as shown in (a)–(c)) and text-to-image (as shown in (d)–(f)) for RCL-SGR on the validation set of MS-COCO dataset. We show the top-3 retrieved texts and images for each given image and text query, respectively. The correctly matched ones are marked in green, and incorrectly matched in red. Specifically, the correctly matched sentences are with green check marks, and the incorrectly matched ones are with red words and X marks. The ground-truth matched images are outlined in green boxes and unmatched in red boxes.  \\n\\n  \\nFig. 7. Parameter analysis of RCL-SAF in terms of average scores $@@1$ ,$R@5$ , and $R@10)$ ) for image-text matching with MRate $=0.6$ on the validation set of Flickr30K.  \\n\\nTABLE VI COMPARISON OF SGR [5] WITH DIFFERENT PRESENTED LOSS FUNCTIONS UNDER THEMISMATCHINGRATES(MRATE)OF0.6ONMS-COCO  \\n\\n\\n<html><body><table><tr><td rowspan=\"2\">Loss</td><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td><td rowspan=\"2\">rSum</td></tr><tr><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td></tr><tr><td>C* \\'mae</td><td>0.1</td><td>0.5</td><td>1.0</td><td>0.1</td><td>0.5</td><td>1.0</td><td>3.2</td></tr><tr><td>Lmae</td><td>67.8</td><td>93.3</td><td>97.2</td><td>55.4</td><td>85.8</td><td>92.9</td><td>492.4</td></tr><tr><td>Lexp</td><td>72.0</td><td>92.9</td><td>97.2</td><td>54.9</td><td>85.0</td><td>92.7</td><td>494.7</td></tr><tr><td>Llog</td><td>71.4</td><td>93.2</td><td>97.1</td><td>55.4</td><td>84.7</td><td>92.3</td><td>494.1</td></tr><tr><td>C gce</td><td>72.6</td><td>93.7</td><td>97.3</td><td>55.4</td><td>84.6</td><td>92.1</td><td>495.7</td></tr><tr><td>C tan</td><td>72.2</td><td>93.7</td><td>97.3</td><td>55.5</td><td>84.7</td><td>92.5</td><td>495.9</td></tr></table></body></html>  \\n\\nTABLE VII I MAGE -T EXT MATCHING WITH THE MISMATCHING RATE OF 0.6 ON MS-COCO 1K AND FLICKR 30K   \\n\\n\\n<html><body><table><tr><td rowspan=\"3\">Method</td><td rowspan=\"3\">LoSS</td><td colspan=\"6\">MS-COCO</td><td rowspan=\"2\"></td><td colspan=\"6\">Flickr30K</td></tr><tr><td colspan=\"2\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td><td rowspan=\"2\">rSum</td><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td></tr><tr><td>R@1</td><td>R@5 R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td></td><td>R@1 R@5</td><td></td><td>R@10</td><td>R@1 R@5</td><td>R@10</td><td>rSum</td></tr><tr><td rowspan=\"5\">SAF</td><td>TR</td><td>28.7</td><td>61.7</td><td>77.4</td><td>26.0 59.1</td><td>74.8</td><td>327.7</td><td>28.4 0.1</td><td>51.6</td><td>64.2</td><td>16.1</td><td>37.9</td><td>48.9</td><td>247.1</td></tr><tr><td>TR-HN</td><td>0.5</td><td>1.3</td><td>2.4</td><td>1.4</td><td>5.4</td><td>8.8 19.8</td><td></td><td>1.2</td><td>2.1</td><td>0.5</td><td>1.1</td><td>2.1</td><td>7.1</td></tr><tr><td>NL</td><td>0.1</td><td>0.9</td><td>1.3</td><td>0.1</td><td>0.5</td><td>1.0</td><td>3.9</td><td>0.0</td><td>0.5</td><td>1.3</td><td>0.1 0.5</td><td>1.0</td><td>3.4</td></tr><tr><td>CL</td><td>68.0</td><td>92.1</td><td>96.7</td><td>52.0</td><td>83.5</td><td>91.5</td><td>483.8</td><td>53.8 81.3</td><td>88.4</td><td>39.6</td><td>66.6</td><td>75.7</td><td>405.4</td></tr><tr><td>CCL</td><td>70.1</td><td>93.1</td><td>96.8</td><td>54.5</td><td>84.4</td><td>91.9 490.8</td><td></td><td>64.5 86.6</td><td>91.6</td><td>43.9</td><td>70.0</td><td>79.2</td><td>435.8</td></tr><tr><td rowspan=\"5\">SGR</td><td>TR</td><td>33.7</td><td>66.9</td><td>80.3</td><td>26.2 59.1</td><td>73.2</td><td>339.4</td><td>37.9</td><td>64.7</td><td>75.0</td><td>24.1</td><td>47.7</td><td>58.3</td><td>307.7</td></tr><tr><td>TR-HN</td><td>0.1</td><td>0.6</td><td>1.1</td><td>0.1</td><td>0.5</td><td>1.0</td><td>3.4</td><td>0.3 1.4</td><td>3.1</td><td>0.2</td><td>1.0</td><td>1.8</td><td>7.8</td></tr><tr><td>NL</td><td>0.0</td><td>0.5</td><td>0.9</td><td>0.1</td><td>0.5</td><td>1.0</td><td>3.0</td><td>0.2 0.4</td><td>0.6</td><td>0.1</td><td>0.5</td><td>1.0</td><td>2.8</td></tr><tr><td>CL</td><td>68.7</td><td>91.6</td><td>96.6</td><td>52.3</td><td>83.3</td><td>91.1</td><td>483.6</td><td>56.8 81.0</td><td>88.4</td><td>39.4</td><td>66.6</td><td>75.8</td><td>408.0</td></tr><tr><td>CCL</td><td>71.4</td><td>93.2</td><td>97.1</td><td>55.4</td><td>84.7</td><td>92.3</td><td>494.1</td><td>65.1 86.1</td><td>92.0</td><td>44.3</td><td>71.2</td><td>79.7</td><td>438.4</td></tr></table></body></html>  \\n\\nTABLE VIII FILTERING-BASEDBASELINESUNDERDIFFERENTMISMATCHINGRATES(MRATE)ONMS-COCO 1KANDFLICKR30K  \\n\\n\\n<html><body><table><tr><td rowspan=\"3\">Noise</td><td rowspan=\"3\">Methods</td><td colspan=\"6\">MS-COCO</td><td colspan=\"6\">Flickr30K</td><td colspan=\"3\"></td></tr><tr><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td><td colspan=\"3\">rSum</td><td colspan=\"3\">Image-to-Text</td><td colspan=\"3\">Text-to-Image</td></tr><tr><td>R@1</td><td>R@5</td><td>R@10</td><td>R@1</td><td>R@5</td><td>R@10</td><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td></td><td>R@1</td><td>R@5</td><td>R@10</td><td>rSum</td></tr><tr><td rowspan=\"8\">0.2</td><td>SAF [5] SGR [5]</td><td>71.5 25.7</td><td>94.0</td><td>97.5 75.1</td><td>57.8 23.5</td><td>86.4 58.9</td><td>91.9 75.1</td><td>499.1 317.1</td><td>62.8 55.9</td><td>88.7 81.5</td><td>93.9 88.9</td><td>49.7 40.2</td><td></td><td>73.6</td><td>78.0 75.3</td><td>446.7 408.6</td></tr><tr><td></td><td>74.0</td><td>58.8</td><td></td><td></td><td></td><td></td><td></td><td>68.1</td><td>90.1</td><td></td><td></td><td></td><td>66.8</td><td></td><td></td></tr><tr><td>SAF[5]+CLIP [9]</td><td></td><td>95.2</td><td>98.0</td><td>58.7</td><td>88.0</td><td>94.4</td><td>508.3</td><td></td><td></td><td></td><td>94.0</td><td>49.6</td><td>76.6</td><td>83.6</td><td>462.0</td></tr><tr><td>SGR [5]+CLIP [9]</td><td>74.7</td><td>94.9</td><td>98.1</td><td>58.9</td><td>87.8</td><td>94.3</td><td>508.7</td><td></td><td>69.1 90.1</td><td></td><td>94.2</td><td>50.3</td><td>76.1</td><td>83.8</td><td>463.6</td></tr><tr><td>SAF-C</td><td>74.9</td><td>94.8</td><td>98.0</td><td>58.7</td><td>88.0</td><td>94.4</td><td>508.8</td><td></td><td>68.3</td><td>90.6</td><td>95.0</td><td>51.1</td><td>77.5</td><td>84.7</td><td>467.2</td></tr><tr><td>SGR-C</td><td>74.4</td><td>95.1</td><td>98.1</td><td>58.6</td><td>87.6</td><td>94.2</td><td></td><td>508.0</td><td>72.2</td><td>91.3</td><td>95.5</td><td>51.5</td><td>76.3</td><td>82.0</td><td>468.8</td></tr><tr><td>RCL-SAF RCL-SGR</td><td>77.1</td><td>95.5</td><td>98.2</td><td>61.0</td><td>88.8</td><td>94.6</td><td>515.2</td><td></td><td>72.0 91.7</td><td></td><td>95.8</td><td>53.6</td><td>79.9</td><td>86.7</td><td>479.7</td></tr><tr><td></td><td>77.0</td><td>95.5</td><td>98.1</td><td>61.3</td><td>88.8</td><td>94.8</td><td>515.5</td><td>74.2</td><td>91.8</td><td></td><td>96.9</td><td>55.6 4.4</td><td>81.2 12.0</td><td>87.5 17.0</td><td>487.2</td></tr><tr><td rowspan=\"8\">0.4</td><td>SAF[5]</td><td>13.5 1.3</td><td>43.8</td><td>48.2</td><td>16.0</td><td>39.0</td><td>50.8</td><td>211.3 18.4</td><td>7.4 4.1</td><td>19.6 16.6</td><td>26.7 24.1</td><td></td><td></td><td></td><td></td><td>87.1 81.8</td></tr><tr><td>SGR [5]</td><td>71.4</td><td>3.7</td><td>6.3</td><td>0.5</td><td>2.5</td><td>4.1</td><td></td><td></td><td></td><td></td><td></td><td>4.1</td><td>13.2</td><td>19.7</td><td></td></tr><tr><td>SAF [5]+CLIP [9]</td><td>72.7</td><td>94.3</td><td>66</td><td>57.1</td><td>86.8</td><td>94.0</td><td>501.5</td><td>61.5</td><td>85.6</td><td></td><td>92.1</td><td>44.5</td><td>72.0</td><td>81.1</td><td>436.8</td></tr><tr><td>SGR [5]+CLIP [9]</td><td></td><td>94.3</td><td>97.9</td><td>56.8</td><td>86.5</td><td>93.2</td><td>501.4</td><td></td><td>62.2</td><td>86.0</td><td>92.1</td><td>44.6</td><td>71.4</td><td>78.6</td><td>434.9</td></tr><tr><td>SAF-C</td><td>72.4</td><td>94.3</td><td>97.8</td><td>57.5</td><td>86.9</td><td>93.8</td><td>502.7</td><td></td><td>63.9</td><td>88.7</td><td>93.2</td><td>46.7</td><td>73.5</td><td>81.4</td><td>447.4</td></tr><tr><td>SGR-C</td><td>72.7</td><td>94.2</td><td>97.9</td><td>57.5</td><td>87.0</td><td>93.8</td><td>503.1</td><td></td><td>67.1</td><td>89.6</td><td>93.7</td><td>47.6</td><td>73.5</td><td>81.1</td><td>452.6</td></tr><tr><td>RCL-SAF</td><td>74.8</td><td>94.8</td><td>97.8</td><td>59.0</td><td>87.1</td><td>93.9</td><td>507.4</td><td>68.8</td><td>89.8</td><td></td><td>95.0</td><td>51.0</td><td>76.7</td><td>84.8</td><td>466.1</td></tr><tr><td>RCL-SGR</td><td>73.9</td><td>94.9</td><td>97.9</td><td>59.0 0.8</td><td>87.4 3.5</td><td>93.9</td><td>507.0</td><td>71.3 0.1</td><td>91.1</td><td></td><td>95.3</td><td>51.4</td><td>78.0</td><td>85.2</td><td>472.3</td></tr><tr><td rowspan=\"8\">0.6 0.8</td><td>SAF [5]</td><td>0.1 0.1</td><td>0.5</td><td>0.7</td><td></td><td>0.5</td><td>6.3</td><td>11.9</td><td></td><td>1.5 6.6</td><td>2.8</td><td></td><td>0.4</td><td>1.2</td><td>2.3</td><td>8.3 24.5</td></tr><tr><td>SGR [5]</td><td></td><td>0.6</td><td>1.0</td><td>0.1</td><td></td><td>1.1</td><td></td><td>3.4</td><td>1.5</td><td></td><td>9.6</td><td>0.3</td><td>2.3</td><td>4.2</td><td></td></tr><tr><td>SAF [5]+CLIP [9]</td><td>68.4 56.5</td><td>93.0</td><td>96.8</td><td>54.3</td><td>85.0 77.1</td><td>92.7</td><td>490.2</td><td></td><td>21.9 53.8</td><td>69.1</td><td></td><td>16.2</td><td>40.3</td><td>53.3</td><td>254.6</td></tr><tr><td>SGR [5]+CLIP [9]</td><td></td><td>85.6</td><td>93.6</td><td>42.9</td><td></td><td>87.4</td><td></td><td>443.1</td><td>2.3</td><td>7.7</td><td>12.2</td><td>1.9</td><td>6.9</td><td>11.1</td><td>42.1</td></tr><tr><td>SAF-C</td><td>69.1</td><td>92.6</td><td>96.9</td><td>54.0</td><td>84.9</td><td>92.8</td><td></td><td>490.3</td><td>45.3</td><td>74.2</td><td>84.1</td><td>32.8</td><td>59.8</td><td>69.5</td><td>365.7</td></tr><tr><td>SGR-C</td><td>66.9</td><td>92.0</td><td>96.6</td><td>52.3</td><td>83.6</td><td>91.8</td><td>483.2</td><td></td><td>47.1 72.2</td><td></td><td>82.1</td><td>31.8</td><td>57.4</td><td>66.6</td><td>357.2</td></tr><tr><td>RCL-SAF</td><td>70.1</td><td>93.1</td><td>96.8</td><td>54.5</td><td>84.4</td><td>91.9</td><td>490.8</td><td>63.9</td><td>84.8</td><td>91.7 18.2</td><td>92.9 1.2 0.5 13.8 2.1 3.2 80.8 79.4</td><td>43.0 45.1 0.1 0.1 0.5 0.2 0.9 0.4 30.7 30.3</td><td>71.2 71.3 0.5 0.6 1.8 0.9 3.9 1.6 56.5 56.1</td><td>79.4 80.2 1.1 1.0 3.0 1.7 6.8 2.9 67.3 66.3</td><td>434.0 438.1 3.7 2.7 30.8 6.5 45.8 9.7 353.1 349.7</td></tr><tr><td>RCL-SGR SAF[5] SGR [5] SAF[5]+CLIP [9] SGR [5]+CLIP [9] SAF-C SGR-C RCL-SAF RCL-SGR</td><td>71.4 0.2 0.2 24.1 22.0 60.3 50.1 62.9 63.2</td><td>93.2 0.8 0.6 37.2 54.6 88.7 81.3 89.3 89.3</td><td>97.1 1.4 1.0 40.4 69.8 94.4 90.2 94.9 95.2</td><td>55.4 0.1 0.1 20.0 17.0 47.1 39.0 47.1 47.6</td><td>84.7 0.5 0.5 34.0 47.5 80.4 72.5 77.9 78.7</td><td>92.3 1.0 1.0 38.2 64.8 89.9 84.5 87.4 88.0</td><td>494.1 193.9 275.7 460.8 417.6 459.5 462.0</td><td>62.3 4.0 3.4 45.0 47.1</td><td>86.3 0.0 0.8 0.2 0.3 3.1 8.6 0.5 1.1 3.8 12.2 0.2 1.4 72.8 70.5</td></table></body></html>\\n\\n# E. Image-Text Matching With Different Upper Bounds\\nIn this section, we investigate the effectiveness of the variants of our framework with different upper bounds, i.e., different loss functions, as shown in Table VI . From the experimental results, one could see that the vanilla satisfactory performance, due to the underfitting issue faced by $\\\\mathcal{L}_{\\\\mathrm{mae}}$ cannot achieve complementary learning. Thanks to the proposed strategy of MAE treats each point equally and ignores hard samples, thus multiple negatives, $\\\\mathcal{L}_{\\\\mathrm{mae}}$ achieves comparable results. However, leading to performance degradation. To address such a problem, we optimize different upper bounds of MAE to improve the performance while preserving robustness. From the experimental res ould find that all upper bounds could improve $\\\\mathcal{L}_{\\\\mathrm{mae}}$ by $1.7\\\\sim3.5$ ∼in terms of the overall scores (i.e., rSum).\\n\\n# F. Ablation Study\\nTo comprehensively investigate the effectiveness of our CCL, we carry out some ablation studies by using the following five loss functions:  \\n\\n  \\nFig. 8. The robustness of our RCL against PMPs with MRate $_{=0.6}$ . This figure shows some mismatched and retrieved examples for our RCL-SGR on the training set of the MS-COCO dataset. (a)–(c) illustrate the mismatched (middle) and top-5 retrieved (right) textual examples for each given image (left). The correctly matched samples are marked by green check marks, and incorrectly matched ones are marked by red X marks.  \\n\\nTR [45] is the hinge-based triplet ranking loss.   \\nTR-HN [29] is the widely-used hinge-based triplet ranking loss with hard negatives.   \\nCL [23] is the contrastive learning loss, i.e., (8) .  \\nNL [17] is the negative learning (aka complementary learning) loss.  \\n\\nBesides the choice in the loss function, the experiments are conducted by training the same settings including but not limited to network structure, hyper-parameters, and optimizer. The ablation study is carried out on MS-COCO and Flickr30K in terms of image-text matching. As demonstrated in Table VII , Figs. 4 and 5 , one could see that TR-HN overfits the false positives because it focuses on the hardest pairs. With the soft relaxation, TR has achieved better performance than TR-HN because it could avoid overfitting PMPs. Different from TR-HN, NL only utilizes negative labels. However, as the negative labels are less informative, NLwillencountertheunderfittingissueaselaboratedinSections I and III . In a contrastive learning manner, although CL could be immune to the false positive in the early training stage, it also overfits the uncorrected supervision with further training, thus leading to performance degradation. Fortunately, our CCL could simultaneously address the overfitting and underfitting issues as claimed and achieve the best performance.',\n",
       " 'original_filename': 'Journal_Paper_Meta_Data_IEEE_Transactions_on_Pattern_Analysis_and_Machine_Intelligence_with_whole_text.db',\n",
       " 'year': 2023}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/query_by_chunk_contain\n",
    "搜索论文片段内容中包含特定关键词的论文片段。\n",
    "\"\"\"\n",
    "chunks = kb.query_by_chunk_contain(\"LoSS\", top_k=1)\n",
    "chunks[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Mesh Segmentation', 'Point Clouds']"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/query_keywords_by_id\n",
    "通过论文ID查询论文关键词。\n",
    "返回值: 关键词列表 (字符串列表) 或 None (如果未找到)。\n",
    "\"\"\"\n",
    "paper_id = '658254cf939a5f4082bc95a5'\n",
    "paper_keywords = kb.query_keywords_by_id(paper_id)\n",
    "paper_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['64c33dc33fda6d7f069586ec',\n",
       "  'Multi-Source Domain Adaptation Through Dataset Dictionary Learning in Wasserstein Space'],\n",
       " ['652d36a9939a5f4082496263',\n",
       "  'DTBS: Dual-Teacher Bi-Directional Self-Training for Domain Adaptation in Nighttime Semantic Segmentation'],\n",
       " ['622577a75aee126c0f008dd0',\n",
       "  'Continual Few-shot Relation Learning Via Embedding Space Regularization and Data Augmentation'],\n",
       " ['60dfc18591e01129379b371c',\n",
       "  'Combining Feature and Instance Attribution to Detect Artifacts'],\n",
       " ['623004305aee126c0f9b37d6',\n",
       "  'Continual Prompt Tuning for Dialog State Tracking'],\n",
       " ['6232a74d5aee126c0fe13f03',\n",
       "  'ConTinTin: Continual Learning from Task Instructions'],\n",
       " ['6075694591e0110f6fe68296',\n",
       "  'Meta-Learning for Fast Cross-Lingual Adaptation in Dependency Parsing'],\n",
       " ['62393e7e5aee126c0f125f36',\n",
       "  'Leveraging Expert Guided Adversarial Augmentation for Improving Generalization in Named Entity Recognition'],\n",
       " ['62393e7e5aee126c0f125ef5',\n",
       "  'Continual Sequence Generation with Adaptive Compositional Modules'],\n",
       " ['62393e7e5aee126c0f125e36',\n",
       "  'Parallel Instance Query Network for Named Entity Recognition'],\n",
       " ['624d05075aee126c0f4a75bf',\n",
       "  'Domain-Aware Contrastive Knowledge Transfer for Multi-domain Imbalanced Data'],\n",
       " ['627dbeda5aee126c0ff67b71',\n",
       "  'Multi Task Learning for Zero Shot Performance Prediction of Multilingual Models'],\n",
       " ['625e1a3d5aee126c0fecaf70',\n",
       "  'StableMoE: Stable Routing Strategy for Mixture of Experts'],\n",
       " ['626603225aee126c0f233886',\n",
       "  'Learning Functional Distributional Semantics with Visual Data'],\n",
       " ['628464625aee126c0faca606',\n",
       "  'When to Use Multi-Task Learning Vs Intermediate Fine-Tuning for Pre-Trained Encoder Transfer Learning'],\n",
       " ['6260bd7e5aee126c0fc6baca',\n",
       "  'DaLC: Domain Adaptation Learning Curve Prediction for Neural Machine Translation'],\n",
       " ['643e0ad50746dc40e341a2a9',\n",
       "  'MELT: Mutual Enhancement of Long-Tailed User and Item for Sequential Recommendation'],\n",
       " ['6110a0ac5244ab9dcb2fbc0f',\n",
       "  'Out-of-Domain Generalization from a Single Source: an Uncertainty Quantification Approach'],\n",
       " ['60d4022191e0112ca5d187b0',\n",
       "  'ImageNet Pre-training Also Transfers Non-robustness.'],\n",
       " ['622eb24d5aee126c0f62b481', 'Active Token Mixer'],\n",
       " ['62afe5495aee126c0f668b96',\n",
       "  'BridgeTower: Building Bridges Between Encoders in Vision-Language Representation Learning'],\n",
       " ['637ee0ee90e50fcafd0f70e1',\n",
       "  'TransVCL: Attention-enhanced Video Copy Localization Network with Flexible Supervision'],\n",
       " ['6385788690e50fcafdf4a0c0',\n",
       "  'RankDNN: Learning to Rank for Few-shot Learning'],\n",
       " ['6391560290e50fcafd9c51bc',\n",
       "  'Cyclically Disentangled Feature Translation for Face Anti-spoofing.'],\n",
       " ['6392a76d90e50fcafd8c35b7',\n",
       "  'Decorate the Newcomers: Visual Domain Prompt for Continual Test Time Adaptation'],\n",
       " ['6397ed4e90e50fcafdf43ffc',\n",
       "  'SRoUDA: Meta Self-training for Robust Unsupervised Domain Adaptation.'],\n",
       " ['63a1751690e50fcafd1f45f9',\n",
       "  'On the Connection Between Invariant Learning and Adversarial Training for Out-of-Distribution Generalization'],\n",
       " ['63bcd73690e50fcafdefa380',\n",
       "  'DeMT: Deformable Mixer Transformer for Multi-Task Learning of Dense Prediction.'],\n",
       " ['63bf7a6e90e50fcafd886668',\n",
       "  'Head-Free Lightweight Semantic Segmentation with Linear Transformer'],\n",
       " ['63a1756a90e50fcafd1fa4f6',\n",
       "  'DSI++: Updating Transformer Memory with New Documents'],\n",
       " ['6401669c90e50fcafd688c4b',\n",
       "  'UDAPDR: Unsupervised Domain Adaptation Via LLM Prompting and Distillation of Rerankers'],\n",
       " ['645c5e40d68f896efa22c84f',\n",
       "  'StrAE: Autoencoding for Pre-Trained Embeddings Using Explicit Structure'],\n",
       " ['646c3addd68f896efa5d195a',\n",
       "  'GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints.'],\n",
       " ['646c3addd68f896efa5d1966',\n",
       "  'TaskWeb: Selecting Better Source Tasks for Multi-task NLP'],\n",
       " ['646edca5d68f896efaddaf80',\n",
       "  'Meta-Learning Online Adaptation of Language Models.'],\n",
       " ['65275731939a5f4082a44ec1', 'Sparse Universal Transformer'],\n",
       " ['652dee70939a5f4082b435db',\n",
       "  'Lifelong Sequence Generation with Dynamic Module Expansion and Adaptation'],\n",
       " ['65309159939a5f4082843dbd',\n",
       "  'Prototype-based HyperAdapter for Sample-Efficient Multi-task Tuning'],\n",
       " ['6531e2ca939a5f4082f5d40b',\n",
       "  'LACMA: Language-Aligning Contrastive Learning with Meta-Actions for Embodied Instruction Following'],\n",
       " ['6535d747939a5f408295c4ed',\n",
       "  'Zero-Shot Sharpness-Aware Quantization for Pre-trained Language Models'],\n",
       " ['653728b8939a5f408235e5e6',\n",
       "  'Continual Named Entity Recognition Without Catastrophic Forgetting'],\n",
       " ['65387a3d939a5f4082980285',\n",
       "  'Adaptive End-to-End Metric Learning for Zero-Shot Cross-Domain Slot Filling'],\n",
       " ['6541f34a939a5f4082ce218a',\n",
       "  'Poisoning Retrieval Corpora by Injecting Adversarial Passages'],\n",
       " ['65430613939a5f40829d35fd',\n",
       "  'Text Rendering Strategies for Pixel Language Models'],\n",
       " ['655c19df939a5f4082c56052',\n",
       "  'Adapt in Contexts: Retrieval-Augmented Domain Adaptation Via In-Context Learning'],\n",
       " ['62b3da1f5aee126c0fb1b50d', 'Robust Universal Adversarial Perturbations'],\n",
       " ['64741c33d68f896efaa7b677',\n",
       "  'Selective Mixup Helps with Distribution Shifts, but Not (only) Because of Mixup'],\n",
       " ['647572d8d68f896efa7b737e',\n",
       "  'CrossGET: Cross-Guided Ensemble of Tokens for Accelerating Vision-Language Transformers'],\n",
       " ['648bde68d68f896efaf818f1',\n",
       "  'Graph Structure and Feature Extrapolation for Out-of-Distribution Generalization'],\n",
       " ['648bde68d68f896efaf81bf1',\n",
       "  'Differentially Private Domain Adaptation with Theoretical Guarantees'],\n",
       " ['651a282d3fda6d7f0600a292', 'Information Flow in Self-Supervised Learning'],\n",
       " ['653b1d19939a5f4082995142',\n",
       "  'OTMatch: Improving Semi-Supervised Learning with Optimal Transport'],\n",
       " ['6555813f939a5f4082e42579',\n",
       "  'ConvNet Vs Transformer, Supervised Vs CLIP: Beyond ImageNet Accuracy'],\n",
       " ['655d6752939a5f4082e03281',\n",
       "  'One Size Fits All for Semantic Shifts: Adaptive Prompt Tuning for Continual Learning'],\n",
       " ['6563ffd4939a5f408221770c',\n",
       "  'Learning in Deep Factor Graphs with Gaussian Belief Propagation'],\n",
       " ['6531e2ca939a5f4082f5d3bc',\n",
       "  'An Image is Worth Multiple Words: Discovering Object Level Concepts Using Multi-Concept Prompt Learning'],\n",
       " ['65727e67939a5f4082ffcc8b',\n",
       "  'Improving Gradient-guided Nested Sampling for Posterior Inference'],\n",
       " ['657a6a5f939a5f4082cf2bd9',\n",
       "  'Polynomial-based Self-Attention for Table Representation Learning'],\n",
       " ['658254d5939a5f4082bc98d7',\n",
       "  'Sparse is Enough in Fine-tuning Pre-trained Large Language Models'],\n",
       " ['6594cfba939a5f4082141d50',\n",
       "  'F-Divergence Based Classification: Beyond the Use of Cross-Entropy'],\n",
       " ['65977591939a5f4082b15297',\n",
       "  'Neural Collapse for Cross-entropy Class-Imbalanced Learning with Unconstrained ReLU Feature Model'],\n",
       " ['659f71f0939a5f40824232f9',\n",
       "  'An Information Theoretic Approach to Interaction-Grounded Learning'],\n",
       " ['60dfd82691e01129379b3856',\n",
       "  'Generalization and Robustness Implications in Object-Centric Learning'],\n",
       " ['6285b5995aee126c0f14da36',\n",
       "  \"Maslow's Hammer for Catastrophic Forgetting: Node Re-Use Vs Node Activation\"],\n",
       " ['62b2889f5aee126c0fbd474a',\n",
       "  'Robust Task Representations for Offline Meta-Reinforcement Learning Via Contrastive Learning.'],\n",
       " ['62c28ae45aee126c0f8a160c', 'Probabilistic Bilevel Coreset Selection'],\n",
       " ['62c28ae55aee126c0f8a1d6c',\n",
       "  'Removing Batch Normalization Boosts Adversarial Training.'],\n",
       " ['65252d8a939a5f40827e8496',\n",
       "  'TAIL: Task-specific Adapters for Imitation Learning with Large Pretrained Models'],\n",
       " ['652622f2939a5f4082bb3c69',\n",
       "  'Geometrically Aligned Transfer Encoder for Inductive Transfer in Regression Tasks'],\n",
       " ['6531e2ca939a5f4082f5d5e8',\n",
       "  'Model Merging by Uncertainty-Based Gradient Matching.'],\n",
       " ['65ea8a7c13fb2c6cf62f709d',\n",
       "  'Enhancing Transfer Learning with Flexible Nonparametric Posterior Sampling.'],\n",
       " ['62c64f265aee126c0f6ccbd4',\n",
       "  'Chairs Can Be Stood On: Overcoming Object Bias in Human-Object Interaction Detection'],\n",
       " ['62d7730e5aee126c0f9009f3',\n",
       "  'Incremental Task Learning with Incremental Rank Updates'],\n",
       " ['62d8c4555aee126c0f762a02',\n",
       "  'On the Versatile Uses of Partial Distance Correlation in Deep Learning.'],\n",
       " ['633ba44890e50fcafdfe4c8e', 'Long-Tailed Class Incremental Learning'],\n",
       " ['6350bc6e90e50fcafdecf2d6',\n",
       "  'Attaining Class-level Forgetting in Pretrained Model Using Few Samples'],\n",
       " ['64d9a6813fda6d7f061d309f',\n",
       "  'Cost-effective On-device Continual Learning over Memory Hierarchy with Miro'],\n",
       " ['63ed9f3090e50fcafd0f0b28',\n",
       "  'Same Same, but Different: Conditional Multi-Task Learning for Demographic-Specific Toxicity Detection.'],\n",
       " ['63eef09c90e50fcafda0d441',\n",
       "  'Fairly Adaptive Negative Sampling for Recommendations.'],\n",
       " ['640166a490e50fcafd68b35b',\n",
       "  'Distillation from Heterogeneous Models for Top-K Recommendation'],\n",
       " ['6407fd3e90e50fcafd27470f',\n",
       "  'Structure Pre-training and Prompt Tuning for Knowledge Graph Transfer'],\n",
       " ['61a444e15244ab9dcb6e77ec',\n",
       "  'SWAT: Spatial Structure Within and among Tokens'],\n",
       " ['6333bb7890e50fcafdb4c0d6',\n",
       "  'OSDP: Optimal Sharded Data Parallel for Distributed Deep Learning'],\n",
       " ['64671267d68f896efaf148ff', 'Levin Tree Search with Context Models'],\n",
       " ['64671269d68f896efaf14b3e',\n",
       "  'G2Pxy: Generative Open-Set Node Classification on Graphs with Proxy Unknowns'],\n",
       " ['64671269d68f896efaf14c07',\n",
       "  'DenseDINO: Boosting Dense Self-Supervised Learning with Token-Based Point-Level Consistency.'],\n",
       " ['6467126cd68f896efaf14d91',\n",
       "  'Meta-Tsallis-Entropy Minimization: a New Self-Training Approach for Domain Adaptation on Text Classification'],\n",
       " ['64671271d68f896efaf150b7',\n",
       "  'Semi-supervised Domain Adaptation in Graph Transfer Learning'],\n",
       " ['63520de890e50fcafd60f4d8',\n",
       "  'Similarity of Neural Architectures Using Adversarial Attack Transferability'],\n",
       " ['641a71fb90e50fcafd72035d',\n",
       "  'ViC-MAE: Self-Supervised Representation Learning from Images and Video with Contrastive Masked Autoencoders'],\n",
       " ['64377347eb3a372744c80c4f',\n",
       "  'WildRefer: 3D Object Localization in Large-scale Dynamic Scenes with Multi-modal Visual Data and Natural Language'],\n",
       " ['64702deed68f896efa520087',\n",
       "  'BK-SDM: A Lightweight, Fast, and Cheap Version of Stable Diffusion'],\n",
       " ['64741c33d68f896efaa7b73a',\n",
       "  'Improving Knowledge Distillation Via Regularizing Feature Direction and Norm'],\n",
       " ['651b7e003fda6d7f0630bdd5',\n",
       "  'ZeroI2V: Zero-Cost Adaptation of Pre-trained Transformers from Image to Video'],\n",
       " ['65275731939a5f4082a45106',\n",
       "  'Leveraging Hierarchical Feature Sharing for Efficient Dataset Condensation'],\n",
       " ['652c9d07939a5f40825c0c6e',\n",
       "  'Learning to Adapt SAM for Segmenting Cross-domain Point Clouds'],\n",
       " ['653f11c2939a5f4082888662',\n",
       "  'ControlLLM: Augment Language Models with Tools by Searching on Graphs'],\n",
       " ['65499d88939a5f4082be9bd6',\n",
       "  'Exploring Active Learning in Meta-Learning: Enhancing Context Set Labeling'],\n",
       " ['655ebf6a939a5f4082bbd637',\n",
       "  'Revisiting Supervision for Continual Representation Learning'],\n",
       " ['6566b085939a5f40827a9689',\n",
       "  'MobileDiffusion: Instant Text-to-Image Generation on Mobile Devices'],\n",
       " ['65696aa4939a5f4082968278',\n",
       "  'Improving Adversarial Transferability Via Model Alignment'],\n",
       " ['656e8f72939a5f408287c536',\n",
       "  'DiffiT: Diffusion Vision Transformers for Image Generation'],\n",
       " ['65712f62939a5f4082b533c1',\n",
       "  'DGInStyle: Domain-Generalizable Semantic Segmentation with Image Diffusion Models and Stylized Semantic Control'],\n",
       " ['6577c9dd939a5f408230a498',\n",
       "  'You Only Learn One Query: Learning Unified Human Query for Single-Stage Multi-Person Multi-Task Human-Centric Perception'],\n",
       " ['657bbe34939a5f4082f21607',\n",
       "  'Agent Attention: on the Integration of Softmax and Linear Attention'],\n",
       " ['657bc89d939a5f4082fab3d3',\n",
       "  'Weighted Ensemble Models Are Strong Continual Learners'],\n",
       " ['65810e5b939a5f4082fcfcf1',\n",
       "  'SeiT++: Masked Token Modeling Improves Storage-efficient Training'],\n",
       " ['65962383939a5f408292f295',\n",
       "  'De-Confusing Pseudo-Labels in Source-Free Domain Adaptation'],\n",
       " ['659b7361939a5f4082eda7f8',\n",
       "  'Open-Vocabulary SAM: Segment and Recognize Twenty-thousand Classes Interactively'],\n",
       " ['65a0acd2939a5f40828e4209',\n",
       "  'PartSTAD: 2D-to-3d Part Segmentation Task Adaptation'],\n",
       " ['65d6b188939a5f40827c04dd',\n",
       "  'YOLOv9: Learning What You Want to Learn Using Programmable Gradient Information'],\n",
       " ['65dd4f8013fb2c6cf66f9fde', 'One-stage Prompt-based Continual Learning'],\n",
       " ['65efbbe213fb2c6cf63f19bd',\n",
       "  'Semantic Residual Prompts for Continual Learning'],\n",
       " ['65f7a00f13fb2c6cf668dfa8',\n",
       "  'Few-Shot Class Incremental Learning with Attention-Aware Self-Adaptive Prompt'],\n",
       " ['6600da6313fb2c6cf6bea0ea',\n",
       "  'Preventing Catastrophic Forgetting Through Memory Networks in Continuous Detection'],\n",
       " ['661dec8213fb2c6cf6c8f952',\n",
       "  'Knowledge-enhanced Visual-Language Pretraining for Computational Pathology'],\n",
       " ['640fe64790e50fcafd9e2514',\n",
       "  'One Transformer Fits All Distributions in Multi-Modal Diffusion at Scale'],\n",
       " ['60ee3a7f91e01102f8efa536',\n",
       "  'CoBERL: Contrastive BERT for Reinforcement Learning'],\n",
       " ['615e65735244ab9dcbf2179e',\n",
       "  'VC Dimension of Partially Quantized Neural Networks in the Overparametrized Regime'],\n",
       " ['615fb6ee5244ab9dcb9c3ba4',\n",
       "  'Efficient Sharpness-aware Minimization for Improved Training of Neural Networks'],\n",
       " ['6164fcc15244ab9dcb24cfc8',\n",
       "  'X-model: Improving Data Efficiency in Deep Learning with A Minimax Model'],\n",
       " ['614012c65244ab9dcb816dd8',\n",
       "  'CDTrans: Cross-domain Transformer for Unsupervised Domain Adaptation'],\n",
       " ['6164fcc15244ab9dcb24d236',\n",
       "  'Self-supervised Learning is More Robust to Dataset Imbalance'],\n",
       " ['61664e625244ab9dcb455913',\n",
       "  'Open-Set Recognition: A Good Closed-Set Classifier is All You Need'],\n",
       " ['615e65745244ab9dcbf2183a',\n",
       "  'Decoupled Adaptation for Cross-Domain Object Detection'],\n",
       " ['60c7f69991e0110a2be23858',\n",
       "  'FedBABU: Towards Enhanced Representation for Federated Image Classification'],\n",
       " ['615e657b5244ab9dcbf21ec9',\n",
       "  'Focus on the Common Good: Group Distributional Robustness Follows'],\n",
       " ['616ce5a05244ab9dcbacface',\n",
       "  'Towards Understanding the Data Dependency of Mixup-style Training'],\n",
       " ['61552aed5244ab9dcb23eaae', 'IGLU: Efficient GCN Training Via Lazy Updates'],\n",
       " ['616e37445244ab9dcbd1ab3c', 'Learning Optimal Conformal Classifiers'],\n",
       " ['61722be25244ab9dcb6f0dae', 'Fast Model Editing at Scale'],\n",
       " ['616e37435244ab9dcbd1a70d',\n",
       "  'Mind the Gap: Domain Gap Control for Single Shot Domain Adaptation for Generative Adversarial Networks'],\n",
       " ['61a839665244ab9dcbb14d50',\n",
       "  'Pixelated Butterfly: Simple and Efficient Sparse Training for Neural Network Models'],\n",
       " ['617b66755244ab9dcbb6a4e0',\n",
       "  'Towards Evaluating the Robustness of Neural Networks Learned by Transduction'],\n",
       " ['61b1742e5244ab9dcb25aec6',\n",
       "  'Relating Transformers to Models and Neural Representations of the Hippocampal Formation'],\n",
       " ['61bff4295244ab9dcb79c999',\n",
       "  'Data Efficient Language-supervised Zero-shot Recognition with Optimal Transport Distillation'],\n",
       " ['61d269325244ab9dcbc5ea42',\n",
       "  'Constructing a Good Behavior Basis for Transfer Using Generalized Policy Updates'],\n",
       " ['61baae6a5244ab9dcb644316',\n",
       "  'Hierarchical Variational Memory for Few-shot Learning Across Domains'],\n",
       " ['61b2c57b5244ab9dcb1d71ff',\n",
       "  'DR3: Value-Based Deep Reinforcement Learning Requires Explicit Regularization'],\n",
       " ['61df98595244ab9dcbd2d530',\n",
       "  'Leveraging Unlabeled Data to Predict Out-of-Distribution Performance'],\n",
       " ['61f8a4c35aee126c0fee01f1',\n",
       "  'Continual Learning with Recursive Gradient Optimization'],\n",
       " ['6201df4a5aee126c0f64dcef',\n",
       "  'The Unreasonable Effectiveness of Random Pruning: Return of the Most Naive Baseline for Sparse Training'],\n",
       " ['61d3bae15244ab9dcba34a5f', 'Optimal Representations for Covariate Shift.'],\n",
       " ['6201df4a5aee126c0f64dd07',\n",
       "  'No Parameters Left Behind: Sensitivity Guided Adaptive Learning Rate for Training Large Transformer Models'],\n",
       " ['6209c8265aee126c0f1e7ef5',\n",
       "  'Domain Adversarial Training: A Game Perspective'],\n",
       " ['6209c8265aee126c0f1e7fbe', 'Conditional Contrastive Learning with Kernel'],\n",
       " ['620dbcfa5aee126c0f5db4de',\n",
       "  'Prospect Pruning: Finding Trainable Weights at Initialization Using Meta-Gradients'],\n",
       " ['6213029a5aee126c0f42a35d',\n",
       "  'When, Why, and Which Pretrained GANs Are Useful?'],\n",
       " ['620f0e735aee126c0fec47dd',\n",
       "  'Revisiting Over-smoothing in BERT from the Perspective of Graph'],\n",
       " ['616ce5a55244ab9dcbacff30',\n",
       "  'Multitask Prompted Training Enables Zero-Shot Task Generalization'],\n",
       " ['621454535aee126c0f2021f6',\n",
       "  'Fine-Tuning Can Distort Pretrained Features and Underperform Out-of-Distribution'],\n",
       " ['620c6b6a5aee126c0fe295b4',\n",
       "  'SQuant: On-the-Fly Data-Free Quantization Via Diagonal Hessian Approximation'],\n",
       " ['61f9f6455aee126c0f41edc8',\n",
       "  'Deconfounded Representation Similarity for Comparison of Neural Networks'],\n",
       " ['620b19c45aee126c0f7e692f',\n",
       "  'Wukong: A 100 Million Large-scale Chinese Cross-modal Pre-training Benchmark'],\n",
       " ['620c6b655aee126c0fe28fd0',\n",
       "  'Debiased Self-Training for Semi-Supervised Learning'],\n",
       " ['61834f695244ab9dcb55ce5f',\n",
       "  'VLMo: Unified Vision-Language Pre-Training with Mixture-of-Modality-Experts'],\n",
       " ['6205d3ef5aee126c0ff1d02b',\n",
       "  'Energy-Based Contrastive Learning of Visual Representations.'],\n",
       " ['61fc99465aee126c0fcdcc5e',\n",
       "  'Understanding Cross-Domain Few-Shot Learning Based on Domain Similarity and Few-Shot Difficulty'],\n",
       " ['622abdd15aee126c0f56bc5e',\n",
       "  'Projunn: Efficient Method for Training Deep Networks with Unitary Matrices.'],\n",
       " ['623004315aee126c0f9b38d9', 'RecursiveMix: Mixed Learning with History'],\n",
       " ['624278fa5aee126c0fd7955c',\n",
       "  'Continual Learning: a Feature Extraction Formalization, an Efficient Algorithm, and Fundamental Obstructions'],\n",
       " ['6204827e5aee126c0f77d99c',\n",
       "  'Generative Multitask Learning Mitigates Target-Causing Confounding'],\n",
       " ['621d8ecd5aee126c0f73b354',\n",
       "  'LobsDICE: Offline Learning from Observation Via Stationary Distribution Correction Estimation'],\n",
       " ['625e1a345aee126c0feca603',\n",
       "  'Unsupervised Cross-Task Generalization Via Retrieval Augmentation'],\n",
       " ['624e569e5aee126c0f7f8fe1',\n",
       "  'Beyond Separability: Analyzing the Linear Transferability of Contrastive Representations to Related Subpopulations'],\n",
       " ['62708f625aee126c0fa692a3',\n",
       "  'VICE: Variational Interpretable Concept Embeddings.'],\n",
       " ['6260bd7f5aee126c0fc6bcfc',\n",
       "  'A Fast Post-Training Pruning Framework for Transformers'],\n",
       " ['628707325aee126c0f78c011',\n",
       "  'Consistent Interpolating Ensembles Via the Manifold-Hilbert Kernel.'],\n",
       " ['628707335aee126c0f78c3f1',\n",
       "  'Learning Energy Networks with Generalized Fenchel-Young Losses.'],\n",
       " ['628afb4b5aee126c0f04e28a',\n",
       "  'Improving Multi-Task Generalization Via Regularizing Spurious Correlation'],\n",
       " ['628afb4c5aee126c0f04e35b',\n",
       "  'BayesPCN: A Continually Learnable Predictive Coding Associative Memory.'],\n",
       " ['628afb4c5aee126c0f04e471', 'Visual Concepts Tokenization.'],\n",
       " ['628afb515aee126c0f04ea32',\n",
       "  'Pre-Train Your Loss: Easy Bayesian Transfer Learning with Informative Priors.'],\n",
       " ['628c4ce15aee126c0ff597af',\n",
       "  'Knowledge Distillation from A Stronger Teacher'],\n",
       " ['628c4ce15aee126c0ff59859',\n",
       "  'Pessimism for Offline Linear Contextual Bandits Using $\\\\ell_p$ Confidence Sets.'],\n",
       " ['629041a75aee126c0fb5cadc',\n",
       "  'Tight Lower Bounds on Worst-Case Guarantees for Zero-Shot Learning with Attributes.'],\n",
       " ['629041a85aee126c0fb5cdde',\n",
       "  'TransBoost: Improving the Best ImageNet Performance Using Deep Transduction.'],\n",
       " ['629041ac5aee126c0fb5dbce',\n",
       "  'AdaptFormer: Adapting Vision Transformers for Scalable Visual Recognition'],\n",
       " ['6294359f5aee126c0f2fddf1',\n",
       "  'Learning to Reason with Neural Networks: Generalization, Unseen Data and Boolean Measures.'],\n",
       " ['6294359f5aee126c0f2fde40',\n",
       "  'FedAvg with Fine Tuning: Local Updates Lead to Representation Learning'],\n",
       " ['629435a05aee126c0f2fe408', 'Sharpness-Aware Training for Free'],\n",
       " ['629435a05aee126c0f2fe41d',\n",
       "  'Spartan: Differentiable Sparsity Via Regularized Transportation.'],\n",
       " ['629435a25aee126c0f2fecb3',\n",
       "  'FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness.'],\n",
       " ['629587475aee126c0fe14c42',\n",
       "  'Decoupling Knowledge from Memorization: Retrieval-augmented Prompt Learning.'],\n",
       " ['629587465aee126c0fe14b4a',\n",
       "  'The Missing Invariance Principle Found -- the Reciprocal Twin of Invariant Risk Minimization'],\n",
       " ['629587475aee126c0fe14cc3',\n",
       "  'Temporal Latent Bottleneck: Synthesis of Fast and Slow Processing Mechanisms in Sequence Learning.'],\n",
       " ['629587475aee126c0fe14d93',\n",
       "  'ACIL: Analytic Class-Incremental Learning with Absolute Memorization and Privacy Protection.'],\n",
       " ['629587475aee126c0fe14dc7',\n",
       "  'Multi-Agent Reinforcement Learning is a Sequence Modeling Problem.'],\n",
       " ['6295874a5aee126c0fe15291',\n",
       "  'Self-Supervised Visual Representation Learning with Semantic Grouping.'],\n",
       " ['6296d9145aee126c0f7312d4',\n",
       "  'A Reduction to Binary Approach for Debiasing Multiclass Datasets'],\n",
       " ['62982a9a5aee126c0f6f5efe',\n",
       "  'A Theoretical Framework for Inference Learning.'],\n",
       " ['629d70385aee126c0f302605', 'Optimal Weak to Strong Learning.'],\n",
       " ['62a013775aee126c0ff6918d',\n",
       "  'Signal Propagation in Transformers: Theoretical Perspectives and the Role of Rank Collapse.'],\n",
       " ['62a165485aee126c0f50a02f',\n",
       "  'Hub-Pathway: Transfer Learning from A Hub of Pre-trained Models.'],\n",
       " ['62a2b6955aee126c0f4d8e7c',\n",
       "  'Uni-Perceiver-MoE: Learning Sparse Generalist Models with Conditional MoEs.'],\n",
       " ['62a94e065aee126c0f9c02cd',\n",
       "  'LST: Ladder Side-Tuning for Parameter and Memory Efficient Transfer Learning'],\n",
       " ['62aa9fb55aee126c0fa5ca8a',\n",
       "  'Contrastive Learning As Goal-Conditioned Reinforcement Learning.'],\n",
       " ['62aa9fb55aee126c0fa5ca56',\n",
       "  'Unknown-Aware Domain Adversarial Learning for Open-Set Domain Adaptation.'],\n",
       " ['62abf1365aee126c0f475b7a',\n",
       "  'CARLANE: A Lane Detection Benchmark for Unsupervised Domain Adaptation from Simulation to Multiple Real-World Domains'],\n",
       " ['62abf13c5aee126c0f478563',\n",
       "  'Interaction-Grounded Learning with Action-inclusive Feedback'],\n",
       " ['62b2888c5aee126c0fbc769c',\n",
       "  'Provable Generalization of Overparameterized Meta-learning Trained with SGD.'],\n",
       " ['62b52c635aee126c0f459d64',\n",
       "  'Learning Viewpoint-Agnostic Visual Representations by Recovering Tokens in 3D Space.'],\n",
       " ['62bd156e5aee126c0f624b5d',\n",
       "  'Beyond Neural Scaling Laws: Beating Power Law Scaling Via Data Pruning.'],\n",
       " ['62c64f265aee126c0f6cca36',\n",
       "  'Cooperative Distribution Alignment Via JSD Upper Bound.'],\n",
       " ['62c64f265aee126c0f6ccc2b', 'PAC Prediction Sets for Meta-Learning'],\n",
       " ['62cce6655aee126c0f2a42bb',\n",
       "  'Models out of Line: A Fourier Lens on Distribution Shift Robustness.'],\n",
       " ['62cce6655aee126c0f2a431b',\n",
       "  'SInGE: Sparsity Via Integrated Gradients Estimation of Neuron Relevance'],\n",
       " ['62d620f65aee126c0fad4b7c',\n",
       "  'Towards Diverse and Faithful One-shot Adaption of Generative Adversarial Networks.'],\n",
       " ['62d8c4555aee126c0f76293b',\n",
       "  'Test-Time Adaptation Via Conjugate Pseudo-labels'],\n",
       " ['62e1fea85aee126c0f70100f',\n",
       "  'Unsupervised Learning under Latent Label Shift.'],\n",
       " ['62eb392a5aee126c0fb76398',\n",
       "  'Optimal Rates for Regularized Conditional Mean Embedding Learning.'],\n",
       " ['62eb392a5aee126c0fb764a3',\n",
       "  'The Power and Limitation of Pretraining-Finetuning for Linear Regression under Covariate Shift.'],\n",
       " ['62f1d08b90e50fcafd88e73e',\n",
       "  'Blackbox Attacks Via Surrogate Ensemble Search.'],\n",
       " ['632297f590e50fcafdc889a0',\n",
       "  'Revisiting Neural Scaling Laws in Language and Vision'],\n",
       " ['632297f390e50fcafdc879ca',\n",
       "  'Improving Self-Supervised Learning by Characterizing Idealized Representations.'],\n",
       " ['6323e96290e50fcafd8a241b',\n",
       "  'Learning from Future: A Novel Self-Training Framework for Semantic Segmentation'],\n",
       " ['6323e96890e50fcafd8a43e0',\n",
       "  'Test-Time Prompt Tuning for Zero-Shot Generalization in Vision-Language Models.'],\n",
       " ['6327dda690e50fcafd67dff9',\n",
       "  'Deliberated Domain Bridging for Domain Adaptive Semantic Segmentation.'],\n",
       " ['6327dda690e50fcafd67e09f',\n",
       "  'Expansion and Shrinkage of Localization for Weakly-Supervised Semantic Segmentation.'],\n",
       " ['6327dda790e50fcafd67e1bb',\n",
       "  'MetaMask: Revisiting Dimensional Confounder for Self-Supervised Learning.'],\n",
       " ['63292f6790e50fcafd2eb739',\n",
       "  'Learning Distinct and Representative Modes for Image Captioning.'],\n",
       " ['63292f6890e50fcafd2ebb1a',\n",
       "  'SegNeXt: Rethinking Convolutional Attention Design for Semantic Segmentation.'],\n",
       " ['632a810e90e50fcafd081ac0',\n",
       "  'Sparse Interaction Additive Networks Via Feature Interaction Detection and Sparse Selection.'],\n",
       " ['632d240290e50fcafd91b2c3',\n",
       "  'Layer Freezing Data Sieving: Missing Pieces of a Generic Framework for Sparse Training'],\n",
       " ['633269f690e50fcafd49002d',\n",
       "  'Optimal Transport-based Identity Matching for Identity-invariant Facial Expression Recognition.'],\n",
       " ['63350ce690e50fcafd350baa',\n",
       "  'An Embarrassingly Simple Approach to Semi-Supervised Few-Shot Learning'],\n",
       " ['63350ce790e50fcafd350d5e',\n",
       "  'Disentangling Transfer in Continual Reinforcement Learning.'],\n",
       " ['633e476790e50fcafde5952c',\n",
       "  'Revisiting Graph Contrastive Learning from the Perspective of Graph Spectrum.'],\n",
       " ['63438d2290e50fcafd4eb082',\n",
       "  'Polyhistor: Parameter-Efficient Multi-Task Adaptation for Dense Vision Tasks.'],\n",
       " ['6344dede90e50fcafd24d09f',\n",
       "  'Adaptive Distribution Calibration for Few-Shot Learning with Hierarchical Optimal Transport'],\n",
       " ['6344dede90e50fcafd24d28b',\n",
       "  'Semi-supervised Semantic Segmentation with Prototype-based Consistency Regularization.'],\n",
       " ['6344dede90e50fcafd24d2df',\n",
       "  'Asymmetric Temperature Scaling Makes Larger Networks Teach Well Again.'],\n",
       " ['6344dee590e50fcafd24e6b3',\n",
       "  'Association Graph Learning for Multi-Task Classification with Category Shifts.'],\n",
       " ['6346305690e50fcafda05668',\n",
       "  'Continual Learning with Evolving Class Ontologies'],\n",
       " ['6346305e90e50fcafda07490', 'Deep Fourier Up-Sampling.'],\n",
       " ['634781f790e50fcafd2bf51f',\n",
       "  'SegViT: Semantic Segmentation with Plain Vision Transformers.'],\n",
       " ['634781fe90e50fcafd2c1b6f',\n",
       "  'Decomposed Knowledge Distillation for Class-Incremental Semantic Segmentation.'],\n",
       " ['634781fe90e50fcafd2c1da0',\n",
       "  'Multi-Granularity Cross-modal Alignment for Generalized Medical Visual Representation Learning.'],\n",
       " ['634781fe90e50fcafd2c1e35',\n",
       "  'When Are Local Queries Useful for Robust Learning?'],\n",
       " ['634781fe90e50fcafd2c1f15',\n",
       "  'Transfer Learning on Heterogeneous Feature Spaces for Treatment Effects Estimation'],\n",
       " ['634781ff90e50fcafd2c2548',\n",
       "  'On the Effectiveness of Lipschitz-Driven Rehearsal in Continual Learning.'],\n",
       " ['634781ff90e50fcafd2c2584',\n",
       "  'Sampling in Constrained Domains with Orthogonal-Space Variational Gradient Descent'],\n",
       " ['6348d36990e50fcafd54622f',\n",
       "  'Evaluated CMI Bounds for Meta Learning: Tightness and Expressiveness.'],\n",
       " ['6348d36990e50fcafd5462c0',\n",
       "  'GULP: a Prediction-Based Metric Between Representations.'],\n",
       " ['6348d36990e50fcafd54631f',\n",
       "  'Task-Free Continual Learning Via Online Discrepancy Distance Learning.'],\n",
       " ['6348d36a90e50fcafd546525',\n",
       "  'Intermediate Prototype Mining Transformer for Few-Shot Semantic Segmentation'],\n",
       " ['6348d36b90e50fcafd5465f9',\n",
       "  'Feature-Proxy Transformer for Few-Shot Segmentation.'],\n",
       " ['634cc7a390e50fcafd162fb1',\n",
       "  'Mix and Reason: Reasoning over Semantic Topology with Data Mixing for Domain Generalization.'],\n",
       " ['634cc7a890e50fcafd163d33',\n",
       "  'Learnable Polyphase Sampling for Shift Invariant and Equivariant Convolutional Networks.'],\n",
       " ['634e194090e50fcafd24e3dc',\n",
       "  'Old Can Be Gold: Better Gradient Flow Can Make Vanilla-GCNs Great Again.'],\n",
       " ['634e194090e50fcafd24e5a3',\n",
       "  'How Mask Matters: Towards Theoretical Understandings of Masked Autoencoders'],\n",
       " ['634e194090e50fcafd24e663',\n",
       "  'Positive-Unlabeled Learning Using Random Forests Via Recursive Greedy Risk Minimization'],\n",
       " ['634e194790e50fcafd24f2d7',\n",
       "  'Signal Processing for Implicit Neural Representations'],\n",
       " ['634e194790e50fcafd24f3a7',\n",
       "  'HyperDomainNet: Universal Domain Adaptation for Generative Adversarial Networks'],\n",
       " ['634e194890e50fcafd24f63b',\n",
       "  'Marksman Backdoor: Backdoor Attacks with Arbitrary Target Class.'],\n",
       " ['634e194890e50fcafd24f66d',\n",
       "  'Self-Supervised Learning Through Efference Copies'],\n",
       " ['6350bc6690e50fcafdeceaf5',\n",
       "  'Curriculum Reinforcement Learning Using Optimal Transport Via Gradual Domain Adaptation'],\n",
       " ['63520de990e50fcafd60f592',\n",
       "  'Self-Supervised Learning Via Maximum Entropy Coding.'],\n",
       " ['6356022390e50fcafd33698b',\n",
       "  'Learning Robust Dynamics Through Variational Sparse Gating.'],\n",
       " ['6356022790e50fcafd337114',\n",
       "  'Graph Few-shot Learning with Task-specific Structures'],\n",
       " ['635753cd90e50fcafddddea7',\n",
       "  'Deep Equilibrium Approaches to Diffusion Models.'],\n",
       " ['635753cd90e50fcafddddf55',\n",
       "  'Revisiting Sparse Convolutional Model for Visual Recognition.'],\n",
       " ['60c1849f91e0112cf43c201d',\n",
       "  'Preservation of the Global Knowledge by Not-True Distillation in Federated Learning'],\n",
       " ['615e65735244ab9dcbf2176d',\n",
       "  'Spectral Bias in Practice: the Role of Function Frequency in Generalization'],\n",
       " ['621454535aee126c0f200087',\n",
       "  'Gradient Estimation with Discrete Stein Operators.'],\n",
       " ['616e374b5244ab9dcbd1b314',\n",
       "  'MEMO: Test Time Robustness Via Adaptation and Augmentation'],\n",
       " ['625f6bf65aee126c0ffb359c',\n",
       "  'ELEVATER: A Benchmark and Toolkit for Evaluating Language-Augmented Visual Models'],\n",
       " ['613ec1295244ab9dcb8168d0',\n",
       "  'Rapid Model Architecture Adaption for Meta-Learning'],\n",
       " ['619472d35244ab9dcbd2dccd',\n",
       "  'Learning with Convolution and Pooling Operations in Kernel Methods.'],\n",
       " ['61722bd25244ab9dcb6ef581', 'Learning Partial Equivariances from Data'],\n",
       " ['62c3abc65aee126c0fc9a406',\n",
       "  'Object Representations As Fixed Points: Training Iterative Refinement Algorithms with Implicit Differentiation'],\n",
       " ['635b486790e50fcafd32f9a0',\n",
       "  'Characterizing Datapoints Via Second-Split Forgetting.'],\n",
       " ['60f8f6be5244ab9dcb27385e',\n",
       "  'Self-Supervised Aggregation of Diverse Experts for Test-Agnostic Long-Tailed Recognition'],\n",
       " ['61b95d115244ab9dcbf12e3a',\n",
       "  'Continual Learning in Environments with Polynomial Mixing Times'],\n",
       " ['6358a56290e50fcafda58e99',\n",
       "  'OpenAUC: Towards AUC-Oriented Open-Set Recognition'],\n",
       " ['64af9a043fda6d7f065a69f5',\n",
       "  'Transferable Graph Structure Learning for Graph-based Traffic Forecasting Across Cities.'],\n",
       " ['61f360c45aee126c0f7d5d65',\n",
       "  'Gap Minimization for Knowledge Sharing and Transfer'],\n",
       " ['626754c95aee126c0fbcde79', 'Zero-Shot Logit Adjustment'],\n",
       " ['627dbeda5aee126c0ff67b57',\n",
       "  'Bootstrapping Informative Graph Augmentation Via A Meta Learning Approach'],\n",
       " ['61e781645244ab9dcbf9a200',\n",
       "  'Comparison Knowledge Translation for Generalizable Image Classification'],\n",
       " ['61ad7f4a5244ab9dcbc6cce8',\n",
       "  'Self-Supervised Material and Texture Representation Learning for Remote Sensing Tasks'],\n",
       " ['61a98aff5244ab9dcb955ef5',\n",
       "  'Uni-Perceiver: Pre-training Unified Architecture for Generic Perception for Zero-shot and Few-shot Tasks'],\n",
       " ['637aec2190e50fcafd928b76',\n",
       "  'Contrastive Credibility Propagation for Reliable Semi-Supervised Learning.'],\n",
       " ['640a9ff890e50fcafd03c699',\n",
       "  'MaskDiff: Modeling Mask Distribution with Diffusion Probabilistic Model for Few-Shot Instance Segmentation.'],\n",
       " ['642ce6fb90e50fcafde75b7c',\n",
       "  'Cross-Class Feature Augmentation for Class Incremental Learning'],\n",
       " ['645dad15d68f896efad9ddaa',\n",
       "  'WeditGAN: Few-Shot Image Generation Via Latent Space Relocation'],\n",
       " ['64c88ca43fda6d7f062689eb',\n",
       "  'MetaDiff: Meta-Learning with Conditional Diffusion for Few-Shot Learning'],\n",
       " ['64e432bf3fda6d7f0600af18',\n",
       "  'UniAP: Towards Universal Animal Perception in Vision Via Few-shot Learning'],\n",
       " ['6528a864939a5f4082579dc2', 'Domain-Controlled Prompt Learning'],\n",
       " ['656e8ee0939a5f4082875cc6',\n",
       "  'HGPROMPT: Bridging Homogeneous and Heterogeneous Graphs for Few-shot Prompt Learning'],\n",
       " ['6577c9ec939a5f40823110dc',\n",
       "  'ASWT-SGNN: Adaptive Spectral Wavelet Transform-based Self-Supervised Graph Neural Network'],\n",
       " ['6577caa7939a5f408235a875',\n",
       "  'AesFA: an Aesthetic Feature-Aware Arbitrary Neural Style Transfer'],\n",
       " ['657a6a77939a5f4082cf3c41',\n",
       "  'DTL: Disentangled Transfer Learning for Visual Recognition'],\n",
       " ['657a6ad1939a5f4082cf7ce2',\n",
       "  'LAMM: Label Alignment for Multi-Modal Prompt Learning'],\n",
       " ['657a6ad2939a5f4082cf7d8b',\n",
       "  'Curriculum-Enhanced Residual Soft An-Isotropic Normalization for Over-smoothness in Deep GNNs'],\n",
       " ['657a6ae9939a5f4082cf8f82',\n",
       "  'Patch-wise Graph Contrastive Learning for Image Translation'],\n",
       " ['657bbd09939a5f4082f12091',\n",
       "  'Towards Inductive Robustness: Distilling and Fostering Wave-induced Resonance in Transductive GCNs Against Graph Adversarial Attacks'],\n",
       " ['657fafa3939a5f4082c9d532',\n",
       "  'Prompt-based Distribution Alignment for Unsupervised Domain Adaptation'],\n",
       " ['65810e5b939a5f4082fcfd2d',\n",
       "  'Test-Time Domain Adaptation by Learning Domain-Aware Batch Normalization'],\n",
       " ['65810e63939a5f4082fd0333',\n",
       "  'PPIDSG: A Privacy-Preserving Image Distribution Sharing Scheme with GAN in Federated Learning'],\n",
       " ['65824f7d939a5f4082a82287',\n",
       "  'DePRL: Achieving Linear Convergence Speedup in Personalized Decentralized Learning with Shared Representations'],\n",
       " ['65824f86939a5f4082a849d4',\n",
       "  'Graph Invariant Learning with Subgraph Co-mixup for Out-Of-Distribution Generalization'],\n",
       " ['65824f86939a5f4082a84aeb',\n",
       "  'Leveraging Normalization Layer in Adapters with Progressive Learning and Adaptive Distillation for Cross-Domain Few-Shot Learning'],\n",
       " ['6583b18a939a5f408229add0',\n",
       "  'AdvST: Revisiting Data Augmentations for Single Domain Generalization'],\n",
       " ['6583b18a939a5f408229aed8', 'Doubly Perturbed Task Free Continual Learning'],\n",
       " ['6584ff1d939a5f408239ab88',\n",
       "  'MFABA: A More Faithful and Accelerated Boundary-based Attribution Method for Deep Neural Networks'],\n",
       " ['658e4adb939a5f4082dbe33c',\n",
       "  'Dynamic Sub-graph Distillation for Robust Semi-supervised Continual Learning'],\n",
       " ['658e4adb939a5f4082dbe3d7',\n",
       "  'VLCounter: Text-aware Visual Representation for Zero-Shot Object Counting'],\n",
       " ['658e4adc939a5f4082dbe4b0',\n",
       "  'Layer Attack Unlearning: Fast and Accurate Machine Unlearning Via Layer Level Attack and Knowledge Distillation'],\n",
       " ['6593917f939a5f4082e688e5',\n",
       "  'Towards Improved Proxy-based Deep Metric Learning Via Data-Augmented Domain Adaptation'],\n",
       " ['659f71a5939a5f408241fb9f',\n",
       "  'Inconsistency-Based Data-Centric Active Open-Set Annotation'],\n",
       " ['65a49a47939a5f4082eb88b7',\n",
       "  'Exploring Diverse Representations for Open Set Recognition'],\n",
       " ['65a747a9939a5f4082503f97',\n",
       "  'Make Prompts Adaptable: Bayesian Modeling for Vision-Language Prompt Learning with Data-Dependent Prior'],\n",
       " ['65b85e32939a5f4082c51a10', 'Data-Free Generalized Zero-Shot Learning'],\n",
       " ['65b9ac4b939a5f408221ddc8',\n",
       "  'One-Step Forward and Backtrack: Overcoming Zig-Zagging in Loss-Aware Quantization Training'],\n",
       " ['65c58949939a5f408229a0c4',\n",
       "  'Rethinking Propagation for Unsupervised Graph Domain Adaptation'],\n",
       " ['65d4162a939a5f4082e1488c',\n",
       "  'VQAttack: Transferable Adversarial Attacks on Visual Question Answering Via Pre-trained Models'],\n",
       " ['65e68afc13fb2c6cf6f6e3ba',\n",
       "  'NeRF-VPT: Learning Novel View Representations with Neural Radiance Fields Via View Prompt Tuning'],\n",
       " ['65ea7b2913fb2c6cf622a23e',\n",
       "  'Contrastive Continual Learning with Importance Sampling and Prototype-Instance Relation Distillation'],\n",
       " ['65ee659613fb2c6cf61acdd4', 'Agile Multi-Source-Free Domain Adaptation'],\n",
       " ['65f7a00f13fb2c6cf668df3d',\n",
       "  'Towards Diverse Perspective Learning with Selection over Multiple Temporal Poolings'],\n",
       " ['6600da6313fb2c6cf6bea196',\n",
       "  'Defying Imbalanced Forgetting in Class Incremental Learning'],\n",
       " ['6602558a13fb2c6cf6217fa9',\n",
       "  'Fast and Controllable Post-training Sparsity: Learning Optimal Sparsity Allocation with Global Constraint in Minutes'],\n",
       " ['6602681c13fb2c6cf61ecf53',\n",
       "  'A Separation and Alignment Framework for Black-Box Domain Adaptation'],\n",
       " ['636c6beb90e50fcafd2d3c86', 'Soft Augmentation for Image Classification'],\n",
       " ['637c3dd090e50fcafd77c754', 'Vision Transformer with Super Token Sampling.'],\n",
       " ['63a1750d90e50fcafd1f3b19',\n",
       "  'Fake It Till You Make It: Learning Transferable Representations from Synthetic ImageNet Clones'],\n",
       " ['63b39cbe90e50fcafdd1e47e',\n",
       "  'Bidirectional Cross-Modal Knowledge Exploration for Video Recognition with Pre-trained Vision-Language Models'],\n",
       " ['63ca06a190e50fcafd683889',\n",
       "  'Self-Supervised Learning from Images with a Joint-Embedding Predictive Architecture'],\n",
       " ['640fe64f90e50fcafd9e487a',\n",
       "  'Prototype-based Embedding Network for Scene Graph Generation'],\n",
       " ['6417d04190e50fcafd83dfa9',\n",
       "  'TeSLA: Test-Time Self-Learning with Automatic Adversarial Augmentation'],\n",
       " ['6419208d90e50fcafda92695',\n",
       "  'Trainable Projected Gradient Method for Robust Fine-tuning'],\n",
       " ['64225b7690e50fcafde12206',\n",
       "  'Generalization Matters: Loss Minima Flattening Via Parameter Hybridization for Efficient Online Knowledge Distillation'],\n",
       " ['642a43bb90e50fcafd9b1466', 'Siamese DETR'],\n",
       " ['6438c501d6db87a14654a545',\n",
       "  'Multi-Mode Online Knowledge Distillation for Self-Supervised Visual Representation Learning.'],\n",
       " ['6464af97d68f896efa353ac1',\n",
       "  'Endpoints Weight Fusion for Class Incremental Semantic Segmentation'],\n",
       " ['6344dede90e50fcafd24d377',\n",
       "  'Learning Robust Representations for Continual Relation Extraction Via Adversarial Class Augmentation'],\n",
       " ['628c4ce25aee126c0ff59b72',\n",
       "  'BBTv2: Towards a Gradient-Free Future with Large Language Models'],\n",
       " ['628d9e805aee126c0f9797ca',\n",
       "  'ATTEMPT: Parameter-Efficient Multi-task Tuning Via Attentional Mixtures of Soft Prompts.'],\n",
       " ['628d9e805aee126c0f9798d7',\n",
       "  'Hyper-X: A Unified Hypernetwork for Multi-Task Multilingual Transfer'],\n",
       " ['6350bc6c90e50fcafdecf065',\n",
       "  'CPL: Counterfactual Prompt Learning for Vision and Language Models'],\n",
       " ['6346305e90e50fcafda07ab8',\n",
       "  'Continual Training of Language Models for Few-Shot Learning.'],\n",
       " ['61baae695244ab9dcb644017',\n",
       "  'Large Dual Encoders Are Generalizable Retrievers.'],\n",
       " ['635753cc90e50fcafdddde31',\n",
       "  'MM-Align: Learning Optimal Transport-based Alignment Dynamics for Fast and Accurate Inference on Missing Modality Sequences'],\n",
       " ['63608e5090e50fcafdee1257',\n",
       "  'Parameter-Efficient Tuning Makes a Good Classification Head'],\n",
       " ['628d9e795aee126c0f979266',\n",
       "  'Analyzing the Mono- and Cross-Lingual Pretraining Dynamics of Multilingual Language Models.'],\n",
       " ['636482d790e50fcafdccaa99',\n",
       "  'Continual Learning of Neural Machine Translation Within Low Forgetting Risk Regions'],\n",
       " ['6375a68190e50fcafd3e49ab',\n",
       "  'On Measuring the Intrinsic Few-Shot Hardness of Datasets.'],\n",
       " ['635753cc90e50fcafddddd03',\n",
       "  'Unsupervised Non-transferable Text Classification'],\n",
       " ['63608e5290e50fcafdee154b',\n",
       "  'Reduce Catastrophic Forgetting of Dense Retrieval Training with Teleportation Negatives'],\n",
       " ['64f96aad3fda6d7f06d7c846',\n",
       "  'Universal Prototype Transport for Zero-Shot Action Recognition and Localization'],\n",
       " ['62ea18e05aee126c0fca2bf6',\n",
       "  'Few-shot Adaptation Works with UnpredicTable Data'],\n",
       " ['634781ff90e50fcafd2c258e', 'Are Sample-Efficient NLP Models More Robust?'],\n",
       " ['63a3cae990e50fcafdeb7456',\n",
       "  'MultiInstruct: Improving Multi-Modal Zero-Shot Learning Via Instruction Tuning.'],\n",
       " ['6459ac57d68f896efa657df8',\n",
       "  'Chain-of-Skills: A Configurable Model for Open-domain Question Answering.'],\n",
       " ['6459ac64d68f896efa658bcd',\n",
       "  'Unified Demonstration Retriever for In-Context Learning'],\n",
       " ['6466fafbd68f896efaeb7729',\n",
       "  'BERM: Training the Balanced and Extractable Representation for Matching to Improve Generalization Ability of Dense Retrieval'],\n",
       " ['646aeca9d68f896efa05a602',\n",
       "  'Do Models Really Learn to Follow Instructions? an Empirical Study of Instruction Tuning.'],\n",
       " ['646c3addd68f896efa5d16cc',\n",
       "  'MetaAdapt: Domain Adaptive Few-Shot Misinformation Detection Via Meta Learning'],\n",
       " ['646c3addd68f896efa5d1997',\n",
       "  'DiffusionNER: Boundary Diffusion for Named Entity Recognition.'],\n",
       " ['646edc9cd68f896efadda9b1',\n",
       "  'Finding the Pillars of Strength for Multi-Head Attention.'],\n",
       " ['646edca5d68f896efaddb069',\n",
       "  'Revisiting Token Dropping Strategy in Efficient BERT Pretraining'],\n",
       " ['647572d8d68f896efa7b7292',\n",
       "  'Federated Learning for Semantic Parsing: Task Formulation, Evaluation Setup, New Algorithms.'],\n",
       " ['647572d8d68f896efa7b7329',\n",
       "  'Towards Better Entity Linking with Multi-View Enhanced Distillation'],\n",
       " ['647817b2d68f896efa850e7f',\n",
       "  'LAIT: Efficient Multi-Segment Encoding in Transformers with Layer-Adjustable Interaction'],\n",
       " ['64796913d68f896efa13481d',\n",
       "  'ManagerTower: Aggregating the Insights of Uni-Modal Experts for Vision-Language Representation Learning.'],\n",
       " ['647eaf35d68f896efad408e7',\n",
       "  'TART: Improved Few-shot Text Classification Using Task-Adaptive Reference Transformation.'],\n",
       " ['647eaf35d68f896efad40903',\n",
       "  'Shrinking Embeddings for Hyper-Relational Knowledge Graphs.'],\n",
       " ['647eaf51d68f896efad41d2a',\n",
       "  'Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition and Constraints.'],\n",
       " ['64951856d68f896efa1ef336',\n",
       "  'Class-Incremental Learning Based on Label Generation.'],\n",
       " ['649e52c5d68f896efae4875a',\n",
       "  'Stop Pre-Training: Adapt Visual-Language Models to Unseen Languages.'],\n",
       " ['64a63bbad68f896efaec47f0',\n",
       "  'Prompt Tuning Pushes Farther, Contrastive Learning Pulls Closer: A Two-Stage Approach to Mitigate Social Biases'],\n",
       " ['64ae66623fda6d7f068425a2',\n",
       "  'Counterfactual Active Learning for Out-of-Distribution Generalization.'],\n",
       " ['61ea24995244ab9dcbabc692',\n",
       "  'Priors, Hierarchy, and Information Asymmetry for Skill Transfer in   Reinforcement Learning'],\n",
       " ['61f753235aee126c0f9c2548',\n",
       "  'Understanding Why Generalized Reweighting Does Not Improve over ERM'],\n",
       " ['6327dda590e50fcafd67de58',\n",
       "  'On the Soft-Subnetwork for Few-shot Class Incremental Learning'],\n",
       " ['633f98d290e50fcafd78df60',\n",
       "  'Effective Self-supervised Pre-training on Low-compute Networks Without Distillation.'],\n",
       " ['634f6ae590e50fcafdcb6897',\n",
       "  'Towards Understanding GD with Hard and Conjugate Pseudo-labels for Test-Time Adaptation'],\n",
       " ['63520de390e50fcafd60ec72',\n",
       "  'Does Learning from Decentralized Non-IID Unlabeled Data Benefit from Self Supervision?'],\n",
       " ['6363316f90e50fcafd4fbdcc',\n",
       "  'EquiMod: an Equivariance Module to Improve Self-Supervised Learning'],\n",
       " ['63dcdb422c26941cf00b60af',\n",
       "  'A Closer Look at Model Adaptation using Feature Distortion and Simplicity Bias'],\n",
       " ['63dcdb422c26941cf00b6101',\n",
       "  'Not All Tasks Are Born Equal: Understanding Zero-Shot Generalization'],\n",
       " ['63dcdb422c26941cf00b64a7',\n",
       "  'Multitask Prompt Tuning Enables Parameter-Efficient Transfer Learning'],\n",
       " ['623004305aee126c0f9b3299', 'GRAND+: Scalable Graph Random Neural Networks'],\n",
       " ['619715fd5244ab9dcb185a87', 'One-Shot Generative Domain Adaptation'],\n",
       " ['623a90055aee126c0f36c4e5',\n",
       "  'Representation Uncertainty in Self-Supervised Learning As Variational Inference'],\n",
       " ['625cd6b85aee126c0f3c6f6f',\n",
       "  'INSTA-BNN: Binary Neural Network with INSTAnce-aware Threshold'],\n",
       " ['628707335aee126c0f78c470',\n",
       "  'Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection'],\n",
       " ['62982a9a5aee126c0f6f5f33',\n",
       "  'CAFA: Class-Aware Feature Alignment for Test-Time Adaptation'],\n",
       " ['62982a9a5aee126c0f6f5fc6',\n",
       "  'Label-Efficient Online Continual Object Detection in Streaming Video'],\n",
       " ['62997c0b5aee126c0f77ce9f',\n",
       "  'ORC: Network Group-based Knowledge Distillation Using Online Role Change.'],\n",
       " ['6344dee590e50fcafd24e85a',\n",
       "  'FS-DETR: Few-Shot DEtection TRansformer with Prompting and Without Re-Training'],\n",
       " ['634781ff90e50fcafd2c258d',\n",
       "  'Token-Label Alignment for Vision Transformers.'],\n",
       " ['636b1a6590e50fcafdf41a24',\n",
       "  'BT^2: Backward-compatible Training with Basis Transformation'],\n",
       " ['6373035b90e50fcafd09ff20',\n",
       "  'ParCNetV2: Oversized Kernel with Enhanced Attention.'],\n",
       " ['637aec2190e50fcafd928cf3',\n",
       "  'DETRDistill: A Universal Knowledge Distillation Framework for DETR-families.'],\n",
       " ['637c3dd690e50fcafd77ccc1',\n",
       "  'MATE: Masked Autoencoders Are Online 3D Test-Time Learners'],\n",
       " ['6385787c90e50fcafdf47cf0',\n",
       "  'Residual Pattern Learning for Pixel-wise Out-of-Distribution Detection in Semantic Segmentation'],\n",
       " ['6386c9e790e50fcafdfa103b',\n",
       "  'SuS-X: Training-Free Name-Only Transfer of Vision-Language Models'],\n",
       " ['6390044c90e50fcafd837d65',\n",
       "  'Beyond Object Recognition: A New Benchmark Towards Object Concept Learning'],\n",
       " ['63969ba790e50fcafdcf1b62',\n",
       "  'Spurious Features Everywhere - Large-Scale Detection of Harmful Spurious Features in ImageNet.'],\n",
       " ['6397ed4390e50fcafdf42b44',\n",
       "  'A Soft Nearest-Neighbor Framework for Continual Semi-Supervised Learning'],\n",
       " ['63b24b2890e50fcafdd379f1',\n",
       "  'Scale-MAE: A Scale-Aware Masked Autoencoder for Multiscale Geospatial Representation Learning'],\n",
       " ['63b63fd190e50fcafd8f58a7',\n",
       "  'Rethinking Mobile Block for Efficient Attention-based Models'],\n",
       " ['63e5b5d190e50fcafd421ca6',\n",
       "  'Enhancing Modality-Agnostic Representations Via Meta-Learning for Brain Tumor Segmentation.'],\n",
       " ['63e5b5d190e50fcafd421cac', 'Q-Diffusion: Quantizing Diffusion Models'],\n",
       " ['63fd715990e50fcafd146d98',\n",
       "  'Knowledge Restore and Transfer for Multi-label Class-Incremental Learning'],\n",
       " ['640a9ff890e50fcafd03c6a7',\n",
       "  'SLCA: Slow Learner with Classifier Alignment for Continual Learning on a Pre-trained Model.'],\n",
       " ['640fe64690e50fcafd9e2345',\n",
       "  'DETA: Denoised Task Adaptation for Few-Shot Learning'],\n",
       " ['640fe64790e50fcafd9e252b',\n",
       "  'Gradient-Regulated Meta-Prompt Learning for Generalizable Vision-Language Models'],\n",
       " ['640fe64790e50fcafd9e25ad',\n",
       "  'Preventing Zero-Shot Transfer Degradation in Continual Learning of Vision-Language Models'],\n",
       " ['641137fe90e50fcafd17b992',\n",
       "  'ICICLE: Interpretable Class Incremental Continual Learning'],\n",
       " ['641137fe90e50fcafd17bc44',\n",
       "  'Editing Implicit Assumptions in Text-to-Image Diffusion Models'],\n",
       " ['6413dabe90e50fcafd3cd828',\n",
       "  'Focus on Your Target: A Dual Teacher-Student Framework for Domain-adaptive Semantic Segmentation'],\n",
       " ['6413dac290e50fcafd3ce20d',\n",
       "  'All4One: Symbiotic Neighbour Contrastive Learning Via Self-Attention and Redundancy Reduction.'],\n",
       " ['6413dac390e50fcafd3ce2f2',\n",
       "  'Efficient Diffusion Training Via Min-SNR Weighting Strategy.'],\n",
       " ['6417d04b90e50fcafd8407f3',\n",
       "  'No Fear of Classifier Biases: Neural Collapse Inspired Federated Learning with Synthetic and Fixed Classifier'],\n",
       " ['6417d04b90e50fcafd840839',\n",
       "  'A Unified Continual Learning Framework with General Parameter-Efficient Tuning'],\n",
       " ['6419208d90e50fcafda924f8',\n",
       "  'Multi-Modal Continual Test-Time Adaptation for 3D Semantic Segmentation.'],\n",
       " ['6419209390e50fcafda92fd3',\n",
       "  'SeiT: Storage-Efficient Vision Training with Tokens Using 1% of Pixel Storage.'],\n",
       " ['641a71f390e50fcafd71dcff',\n",
       "  'Linear-Covariance Loss for End-to-End Learning of 6D Pose Estimation'],\n",
       " ['641d14e590e50fcafdf75d9a',\n",
       "  'First Session Adaptation: A Strong Replay-Free Baseline for Class-Incremental Learning'],\n",
       " ['641d14e590e50fcafdf7628e',\n",
       "  'The Effectiveness of MAE Pre-Pretraining for Billion-Scale Pretraining'],\n",
       " ['6421096390e50fcafdb09438',\n",
       "  'FastViT: A Fast Hybrid Vision Transformer Using Structural Reparameterization'],\n",
       " ['6423ac7790e50fcafd55ed57',\n",
       "  'Complementary Domain Adaptation and Generalization for Unsupervised Continual Domain Shift Learning'],\n",
       " ['6423ac7890e50fcafd55f015',\n",
       "  'Unmasked Teacher: Towards Training-Efficient Video Foundation Models'],\n",
       " ['6423ac7890e50fcafd55f284',\n",
       "  'Your Diffusion Model is Secretly a Zero-Shot Classifier'],\n",
       " ['64264f7b90e50fcafd68da19',\n",
       "  'Task-Oriented Multi-Modal Mutual Learning for Vision-Language Models'],\n",
       " ['642a43bc90e50fcafd9b1576',\n",
       "  'DIME-FM: DIstilling Multimodal and Efficient Foundation Models'],\n",
       " ['642ce6fb90e50fcafde75a73',\n",
       "  'Black Box Few-Shot Adaptation for Vision-Language Models'],\n",
       " ['642ce6fb90e50fcafde75b7d',\n",
       "  'PODIA-3D: Domain Adaptation of 3D Generative Model Across Large Domain Gap Using Pose-Preserved Text-to-Image Diffusion'],\n",
       " ['6433f6a190e50fcafd6e78d1', 'Generalized Sum Pooling for Metric Learning'],\n",
       " ['6433f6aa90e50fcafd6ea9f2',\n",
       "  'Subclass-balancing Contrastive Learning for Long-tailed Recognition'],\n",
       " ['6434cfd690e50fcafd7a4458',\n",
       "  'HumanSD: A Native Skeleton-Guided Diffusion Model for Human Image Generation'],\n",
       " ['643621a290e50fcafd66670d',\n",
       "  'MOST: Multiple Object Localization with Self-supervised Transformers for Object Discovery'],\n",
       " ['6438c500d6db87a14654a22f',\n",
       "  'UniverSeg: Universal Medical Image Segmentation'],\n",
       " ['6438c504d6db87a14654ad3f',\n",
       "  'Single-Stage Diffusion NeRF: A Unified Approach to 3D Generation and Reconstruction.'],\n",
       " ['644744fa71ac66d2cbf9af99',\n",
       "  'Benchmarking Low-Shot Robustness to Natural Distribution Shifts.'],\n",
       " ['644744fb71ac66d2cbf9b874',\n",
       "  'Walking Your LiDOG: A Journey Through Multiple Domains for LiDAR Semantic Segmentation'],\n",
       " ['644744fb71ac66d2cbf9bacb',\n",
       "  'Universal Domain Adaptation Via Compressive Attention Matching'],\n",
       " ['644744ff71ac66d2cbf9d2f3',\n",
       "  'Distilling from Similar Tasks for Transfer Learning on a Budget.'],\n",
       " ['644b396c0ac864009830865f',\n",
       "  'MAPConNet: Self-supervised 3D Pose Transfer with Mesh and Point Contrastive Learning'],\n",
       " ['64a78f1fd68f896efa01eafe',\n",
       "  'Distilling Large Vision-Language Model with Out-of-Distribution Generalizability'],\n",
       " ['64ae259f3fda6d7f0658f61f',\n",
       "  'EgoVLPv2: Egocentric Video-Language Pre-training with Fusion in the Backbone'],\n",
       " ['64b0d4cc3fda6d7f06b43715',\n",
       "  'Self-regulating Prompts: Foundational Model Adaptation Without Forgetting'],\n",
       " ['64b4bd093fda6d7f0654f5d2',\n",
       "  'L-DAWA: Layer-wise Divergence Aware Weight Aggregation in Federated Self-Supervised Visual Representation Learning.'],\n",
       " ['64b4bd0d3fda6d7f0654fb9a',\n",
       "  'DreamTeacher: Pretraining Image Backbones with Deep Generative Models.'],\n",
       " ['64b60eaa3fda6d7f06eae95b',\n",
       "  'SINC: Self-Supervised In-Context Learning for Vision-Language Tasks.'],\n",
       " ['64b60eaa3fda6d7f06eaeaf0',\n",
       "  'Tangent Model Composition for Ensembling and Continual Fine-tuning.'],\n",
       " ['64b60eaa3fda6d7f06eaeb79', 'Random Boxes Are Open-world Object Detectors'],\n",
       " ['64b60eaa3fda6d7f06eaebad',\n",
       "  'ShiftNAS: Improving One-shot NAS Via Probability Shift'],\n",
       " ['64b60eaa3fda6d7f06eaec3a', 'DOT: A Distillation-Oriented Trainer.'],\n",
       " ['64b76c6a3fda6d7f068ee1db',\n",
       "  'Local or Global: Selective Knowledge Assimilation for Federated Learning with Limited Labels'],\n",
       " ['64b76c6a3fda6d7f068ee313',\n",
       "  'Ord2Seq: Regarding Ordinal Regression As Label Sequence Prediction'],\n",
       " ['64b8b1bd3fda6d7f062b9704',\n",
       "  'Adversarial Bayesian Augmentation for Single-Source Domain Generalization.'],\n",
       " ['64b8b1bd3fda6d7f062b9994',\n",
       "  'Unsupervised Accuracy Estimation of Deep Visual Models Using Domain-Adaptive Adversarial Perturbation Without Source Samples'],\n",
       " ['64ba03413fda6d7f06273341',\n",
       "  'See More and Know More: Zero-shot Point Cloud Segmentation Via Multi-modal Visual Data'],\n",
       " ['64bdf76d3fda6d7f06fbceeb',\n",
       "  'CLR: Channel-wise Lightweight Reprogramming for Continual Learning'],\n",
       " ['64bdf76d3fda6d7f06fbcf34',\n",
       "  'Strip-MLP: Efficient Token Interaction for Vision MLP'],\n",
       " ['64bdf76d3fda6d7f06fbcf3f',\n",
       "  'Distribution Shift Matters for Knowledge Distillation with Webly Collected Images'],\n",
       " ['64bdf76d3fda6d7f06fbcf8d',\n",
       "  'Bridging Vision and Language Encoders: Parameter-Efficient Tuning for Referring Image Segmentation'],\n",
       " ['64bf48f93fda6d7f0627475b',\n",
       "  'Hallucination Improves the Performance of Unsupervised Visual Representation Learning'],\n",
       " ['64bf48f93fda6d7f06274807',\n",
       "  'Learning Navigational Visual Representations with Semantic Map Supervision'],\n",
       " ['64bf48f93fda6d7f062748ad',\n",
       "  'TF-ICON: Diffusion-Based Training-Free Cross-Domain Image Composition'],\n",
       " ['64bf48f93fda6d7f062748b6',\n",
       "  'Cross Contrasting Feature Perturbation for Domain Generalization'],\n",
       " ['64bf48f93fda6d7f062748ff',\n",
       "  'A Good Student is Cooperative and Reliable: CNN-Transformer Collaborative Learning for Semantic Segmentation.'],\n",
       " ['64bf48f93fda6d7f06274902',\n",
       "  'PRIOR: Prototype Representation Joint Learning from Medical Images and Reports'],\n",
       " ['64bf48f93fda6d7f06274926',\n",
       "  'Less is More: Focus Attention for Efficient DETR'],\n",
       " ['64bf48f93fda6d7f0627492a',\n",
       "  'CTVIS: Consistent Training for Online Video Instance Segmentation'],\n",
       " ['64bf48f93fda6d7f0627499f',\n",
       "  'COCO-O: A Benchmark for Object Detectors under Natural Distribution Shifts'],\n",
       " ['64c1ec613fda6d7f0639732f',\n",
       "  'E^2VPT: an Effective and Efficient Approach for Visual Prompt Tuning'],\n",
       " ['64c1ec613fda6d7f0639745e',\n",
       "  'Set-level Guidance Attack: Boosting Adversarial Transferability of Vision-Language Pre-training Models'],\n",
       " ['64c33dc33fda6d7f069585e5',\n",
       "  'Taxonomy Adaptive Cross-Domain Adaptation in Medical Imaging Via Optimization Trajectory Distillation'],\n",
       " ['64c33dc33fda6d7f06958762',\n",
       "  'Regularized Mask Tuning: Uncovering Hidden Knowledge in Pre-trained Vision-Language Models'],\n",
       " ['64c33dc33fda6d7f06958773',\n",
       "  'To Adapt or Not to Adapt? Real-Time Adaptation for Semantic Segmentation'],\n",
       " ['64c731d03fda6d7f0682439d',\n",
       "  'PromptStyler: Prompt-driven Style Generation for Source-free Domain Generalization'],\n",
       " ['64c731d03fda6d7f0682442a',\n",
       "  'TaskExpert: Dynamically Assembling Multi-Task Representations with Memorial Mixture-of-Experts'],\n",
       " ['64c731d63fda6d7f06824a3c',\n",
       "  'Supervised Homography Learning with Realistic Dataset Generation'],\n",
       " ['64c88ca43fda6d7f06268aff',\n",
       "  'FULLER: Unified Multi-modality Multi-task 3D Perception Via Multi-level Gradient Calibration.'],\n",
       " ['64c9d51e3fda6d7f0637ca0c',\n",
       "  'Online Prototype Learning for Online Continual Learning'],\n",
       " ['64d1bde83fda6d7f06ec3b86',\n",
       "  'An Adaptive Model Ensemble Adversarial Attack for Boosting Adversarial Transferability'],\n",
       " ['64d1bde83fda6d7f06ec3bdf',\n",
       "  'Focus the Discrepancy: Intra- and Inter-Correlation Learning for Image Anomaly Detection'],\n",
       " ['64d1bde83fda6d7f06ec3c22',\n",
       "  'Prototypes-oriented Transductive Few-shot Learning with Conditional Transport'],\n",
       " ['64d1bde83fda6d7f06ec3d26',\n",
       "  'Multi-Label Self-Supervised Learning with Scene Images'],\n",
       " ['64d1bde83fda6d7f06ec3d81',\n",
       "  'Heterogeneous Forgetting Compensation for Class-Incremental Learning'],\n",
       " ['64d30f2d3fda6d7f06f6c383',\n",
       "  'Hierarchical Visual Primitive Experts for Compositional Zero-Shot Learning'],\n",
       " ['64d30f353fda6d7f06f6c9ef',\n",
       "  'Enhancing Adversarial Robustness in Low-Label Regime Via Adaptively Weighted Regularization and Knowledge Distillation'],\n",
       " ['64d465973fda6d7f06891436',\n",
       "  'Prototypical Kernel Learning and Open-set Foreground Perception for Generalized Few-shot Semantic Segmentation'],\n",
       " ['64d5b2153fda6d7f060d0068',\n",
       "  'RLSAC: Reinforcement Learning Enhanced Sample Consensus for End-to-End Robust Estimation'],\n",
       " ['64d5b2153fda6d7f060d00ba',\n",
       "  'Learning Gabor Texture Features for Fine-Grained Recognition'],\n",
       " ['64d5b21d3fda6d7f060d0cb8',\n",
       "  'Look at the Neighbor: Distortion-aware Unsupervised Domain Adaptation for Panoramic Semantic Segmentation'],\n",
       " ['64d9a6813fda6d7f061d3090',\n",
       "  'Diverse Data Augmentation with Diffusions for Effective Test-time Prompt Tuning'],\n",
       " ['64d9a6813fda6d7f061d309d',\n",
       "  'Towards Instance-adaptive Inference for Federated Learning'],\n",
       " ['64dafb293fda6d7f064e2b56',\n",
       "  \"BEV-DG: Cross-Modal Learning under Bird's-Eye View for Domain Generalization of 3D Semantic Segmentation\"],\n",
       " ['64dafb293fda6d7f064e2b57',\n",
       "  'SegPrompt: Boosting Open-world Segmentation Via Category-level Prompt Learning'],\n",
       " ['64dafb293fda6d7f064e2bf8',\n",
       "  'SimMatchV2: Semi-Supervised Learning with Graph Consistency'],\n",
       " ['64dafb293fda6d7f064e2bf9',\n",
       "  'Isomer: Isomerous Transformer for Zero-shot Video Object Segmentation'],\n",
       " ['64dafb293fda6d7f064e2c4e',\n",
       "  'Shrinking Class Space for Enhanced Certainty in Semi-Supervised Learning'],\n",
       " ['64dafb293fda6d7f064e2cb4',\n",
       "  'Towards Open-Set Test-Time Adaptation Utilizing the Wisdom of Crowds in Entropy Minimization'],\n",
       " ['64dafb293fda6d7f064e2ce3',\n",
       "  'CBA: Improving Online Continual Learning Via Continual Bias Adaptor'],\n",
       " ['64dc49903fda6d7f06389c70', 'EQ-Net: Elastic Quantization Neural Networks'],\n",
       " ['64dc49903fda6d7f06389c96',\n",
       "  'DiffGuard: Semantic Mismatch-Guided Out-of-Distribution Detection Using Pre-trained Diffusion Models'],\n",
       " ['64dd9b053fda6d7f0622e6d7',\n",
       "  'GPA-3D: Geometry-aware Prototype Alignment for Unsupervised Domain Adaptive 3D Object Detection from Point Clouds'],\n",
       " ['64dd9b053fda6d7f0622e799',\n",
       "  'Stable and Causal Inference for Discriminative Self-supervised Deep Visual Representations'],\n",
       " ['64dd9b053fda6d7f0622e7fd',\n",
       "  'Tem-adapter: Adapting Image-Text Pretraining for Video Question Answer'],\n",
       " ['64e2e14f3fda6d7f064663e2',\n",
       "  'Label Shift Adapter for Test-Time Adaptation under Covariate and Label Shifts'],\n",
       " ['64e2e14f3fda6d7f06466531',\n",
       "  'The Unreasonable Effectiveness of Large Language-Vision Models for Source-free Video Domain Adaptation'],\n",
       " ['64e2e14f3fda6d7f064665d0',\n",
       "  'NAPA-VQ: Neighborhood Aware Prototype Augmentation with Vector Quantization for Continual Learning.'],\n",
       " ['64e2e14f3fda6d7f064665d6',\n",
       "  'Online Class Incremental Learning on Stochastic Blurry Task Boundary Via Mask and Visual Prompt Tuning'],\n",
       " ['64e2e14f3fda6d7f064665d9',\n",
       "  'DiffDis: Empowering Generative Diffusion Model with Cross-Modal Discrimination Capability'],\n",
       " ['64e2e14f3fda6d7f06466631',\n",
       "  'Generalizable Decision Boundaries: Dualistic Meta-Learning for Open Set Domain Generalization'],\n",
       " ['64e2e15a3fda6d7f064669f6',\n",
       "  'Vision Relation Transformer for Unbiased Scene Graph Generation'],\n",
       " ['64e432bf3fda6d7f0600af2c',\n",
       "  'Disposable Transfer Learning for Selective Source Task Unlearning'],\n",
       " ['64e432bf3fda6d7f0600afef',\n",
       "  'Unilaterally Aggregated Contrastive Learning with Hierarchical Augmentation for Anomaly Detection'],\n",
       " ['64e432bf3fda6d7f0600b002',\n",
       "  'VLN-PETL: Parameter-Efficient Transfer Learning for Vision-and-Language Navigation'],\n",
       " ['64e432bf3fda6d7f0600b121',\n",
       "  'When Prompt-based Incremental Learning Does Not Meet Strong Pretraining'],\n",
       " ['64e432bf3fda6d7f0600b139',\n",
       "  'STEERER: Resolving Scale Variations for Counting and Localization Via Selective Inheritance Learning'],\n",
       " ['64e432bf3fda6d7f0600b1c1',\n",
       "  'Image-free Classifier Injection for Zero-Shot Classification'],\n",
       " ['64e432bf3fda6d7f0600b1c5',\n",
       "  'A Step Towards Understanding Why Classification Helps Regression'],\n",
       " ['64e5846c3fda6d7f063ac89c',\n",
       "  'MetaGCD: Learning to Continually Learn in Generalized Category Discovery'],\n",
       " ['64e5846c3fda6d7f063ac8bb',\n",
       "  'Video OWL-ViT: Temporally-consistent Open-World Localization in Video'],\n",
       " ['64e5849c3fda6d7f063af3c4',\n",
       "  'GrowCLIP: Data-aware Automatic Model Growing for Large-scale Contrastive Language-Image Pre-training'],\n",
       " ['64e5849c3fda6d7f063af3df',\n",
       "  'Exemplar-Free Continual Transformer with Convolutions'],\n",
       " ['64e6d5bd3fda6d7f0652c6bf',\n",
       "  'Understanding Hessian Alignment for Domain Generalization'],\n",
       " ['64e6d5bd3fda6d7f0652c72a',\n",
       "  'Semi-Supervised Learning Via Weight-aware Distillation under Class Distribution Mismatch'],\n",
       " ['64e6d5bd3fda6d7f0652c893',\n",
       "  'SG-Former: Self-guided Transformer with Evolving Token Reallocation'],\n",
       " ['64e826d03fda6d7f06c2e05b',\n",
       "  'Diffusion-based Image Translation with Label Guidance for Domain Adaptive Semantic Segmentation'],\n",
       " ['64e826d03fda6d7f06c2e06c',\n",
       "  'Continual Zero-Shot Learning Through Semantically Guided Generative Random Walks'],\n",
       " ['64e826d03fda6d7f06c2e072',\n",
       "  'Vision Transformer Adapters for Generalizable Multitask Learning'],\n",
       " ['64e826d03fda6d7f06c2e07e',\n",
       "  'With a Little Help from Your Own Past: Prototypical Memory Networks for Image Captioning'],\n",
       " ['64e826d03fda6d7f06c2e0ff',\n",
       "  'Masked Autoencoders Are Efficient Class Incremental Learners'],\n",
       " ['64e826d63fda6d7f06c31508',\n",
       "  'Motion-Guided Masking for Spatiotemporal Representation Learning'],\n",
       " ['64ec1b763fda6d7f0626f40d',\n",
       "  'Preserving Modality Structure Improves Multi-Modal Learning'],\n",
       " ['64ec1b763fda6d7f0626f468',\n",
       "  'IOMatch: Simplifying Open-Set Semi-Supervised Learning with Joint Inliers and Outliers Utilization'],\n",
       " ['64ec1b7e3fda6d7f062701d3',\n",
       "  'Dynamic Residual Classifier for Class Incremental Learning.'],\n",
       " ['64ec1b7e3fda6d7f062701e5',\n",
       "  'SVQNet: Sparse Voxel-Adjacent Query Network for 4D Spatio-Temporal LiDAR Semantic Segmentation'],\n",
       " ['64ed716d3fda6d7f0658a902',\n",
       "  'Prior-guided Source-free Domain Adaptation for Human Pose Estimation'],\n",
       " ['64ed716d3fda6d7f0658a949',\n",
       "  'Domain-Specificity Inducing Transformers for Source-Free Domain Adaptation'],\n",
       " ['64ed716d3fda6d7f0658aa58',\n",
       "  'Bridging Cross-task Protocol Inconsistency for Distillation in Dense Object Detection'],\n",
       " ['64ed716d3fda6d7f0658aab2',\n",
       "  'Online Continual Learning on Hierarchical Label Expansion'],\n",
       " ['64eebe873fda6d7f06760713', 'Learning to Upsample by Learning to Sample'],\n",
       " ['64f59fb33fda6d7f0648d2f8',\n",
       "  'Adversarial Finetuning with Latent Representation Constraint to Mitigate Accuracy-Robustness Tradeoff'],\n",
       " ['64f7f8e53fda6d7f06f25284',\n",
       "  'Domain Generalization Via Balancing Training Difficulty and Model Capability'],\n",
       " ['64f7f9443fda6d7f06f28b72',\n",
       "  'Towards Universal Image Embeddings: A Large-Scale Dataset and Challenge for Generic Image Representations'],\n",
       " ['64f7fc4f3fda6d7f06f4316c',\n",
       "  'Building a Winning Team: Selecting Source Model Ensembles Using a Submodular Transferability Estimation Approach'],\n",
       " ['64fa84403fda6d7f067007b0',\n",
       "  'Distribution-Aware Prompt Tuning for Vision-Language Models'],\n",
       " ['64fa84403fda6d7f0670098c',\n",
       "  'Box-based Refinement for Weakly Supervised and Unsupervised Localization Tasks'],\n",
       " ['64fa84403fda6d7f067009ac',\n",
       "  'Tracking Anything with Decoupled Video Segmentation'],\n",
       " ['64fe78f03fda6d7f06a43a18',\n",
       "  'Unsupervised Object Localization with Representer Point Selection'],\n",
       " ['64ffca703fda6d7f06cd896a',\n",
       "  'When to Learn What: Model-Adaptive Data Augmentation Curriculum'],\n",
       " ['64ffca703fda6d7f06cd8a1c',\n",
       "  'Effective Real Image Editing with Accelerated Iterative Diffusion Inversion'],\n",
       " ['6503bec83fda6d7f067c75cc', 'Efficiently Robustify Pre-trained Models'],\n",
       " ['6503bec83fda6d7f067c7792',\n",
       "  'Disentangling Spatial and Temporal Learning for Efficient Image-to-Video Transfer Learning'],\n",
       " ['650904db3fda6d7f06cd4813',\n",
       "  'In-Style: Bridging Text and Uncurated Videos with Style Transfer for Text-Video Retrieval'],\n",
       " ['650ba7c03fda6d7f06e61254',\n",
       "  'Contrastive Pseudo Learning for Open-World DeepFake Attribution'],\n",
       " ['650cf92d3fda6d7f06d445e2',\n",
       "  'TinyCLIP: CLIP Distillation Via Affinity Mimicking and Weight Inheritance'],\n",
       " ['6510eda73fda6d7f06b901b7',\n",
       "  'SCOB: Universal Text Understanding Via Character-wise Supervised Contrastive Learning with Online Text Rendering for Bridging Domain Gap.'],\n",
       " ['6510edb83fda6d7f06b90dd0',\n",
       "  'A Sentence Speaks a Thousand Images: Domain Generalization Through Distilling CLIP with Language Guidance'],\n",
       " ['6510edb83fda6d7f06b90ef2', 'Domain Adaptive Few-Shot Open-Set Learning'],\n",
       " ['65123f453fda6d7f06e548ce',\n",
       "  'Order-preserving Consistency Regularization for Domain Adaptation and Generalization'],\n",
       " ['65128fdf3fda6d7f062655f5',\n",
       "  'Informative Data Mining for One-Shot Cross-Domain Semantic Segmentation'],\n",
       " ['651390ac3fda6d7f06034ffc',\n",
       "  'Structure Invariant Transformation for Better Adversarial Transferability'],\n",
       " ['651390ac3fda6d7f060350c1',\n",
       "  'Nearest Neighbor Guidance for Out-of-Distribution Detection'],\n",
       " ['6514e2043fda6d7f062dca3e',\n",
       "  'Confidence-based Visual Dispersal for Few-shot Unsupervised Domain Adaptation'],\n",
       " ['6514e2043fda6d7f062dcafb',\n",
       "  'Rapid Network Adaptation: Learning to Adapt Neural Networks Using Test-Time Feedback'],\n",
       " ['6516338d3fda6d7f065e51ab',\n",
       "  'Learning to Transform for Generalizable Instance-wise Invariance'],\n",
       " ['651a282d3fda6d7f0600a33e',\n",
       "  'Multi-task View Synthesis with Neural Radiance Fields'],\n",
       " ['652528e4939a5f4082613dd2',\n",
       "  'Activate and Reject: Towards Safe Domain Generalization under Category Shift'],\n",
       " ['652622f1939a5f4082bb3b3f',\n",
       "  'CoinSeg: Contrast Inter- and Intra- Class Representations for Incremental Segmentation'],\n",
       " ['65275731939a5f4082a4503b',\n",
       "  'Domain Generalization Guided by Gradient Signal to Noise Ratio of Parameters'],\n",
       " ['6528c1ee939a5f4082995261',\n",
       "  'Periodically Exchange Teacher-Student for Source-Free Object Detection'],\n",
       " ['6528c1f8939a5f4082996fcf',\n",
       "  'Fully Attentional Networks with Self-emerging Token Labeling'],\n",
       " ['6528c1fa939a5f40829973b4', 'Pairwise Similarity Learning is SimPLE'],\n",
       " ['6528c201939a5f4082998f0b',\n",
       "  'SSB: Simple but Strong Baseline for Boosting Performance of Open-Set Semi-Supervised Learning'],\n",
       " ['6528c21d939a5f408299d993',\n",
       "  'Gramian Attention Heads Are Strong Yet Efficient Vision Learners'],\n",
       " ['6528c221939a5f408299e434', 'Quality Diversity for Visual Pre-Training'],\n",
       " ['6528c221939a5f408299e4cf',\n",
       "  'FerKD: Surgical Label Adaptation for Efficient Distillation'],\n",
       " ['6528c227939a5f408299f50c',\n",
       "  'ClusT3: Information Invariant Test-Time Training'],\n",
       " ['6528c22d939a5f40829a0612',\n",
       "  'CDAC: Cross-domain Attention Consistency in Transformer for Domain Adaptive Semantic Segmentation'],\n",
       " ['6528c23c939a5f40829a3695',\n",
       "  'Stable Cluster Discrimination for Deep Clustering'],\n",
       " ['6528c251939a5f40829a770c',\n",
       "  'Concept-wise Fine-tuning Matters in Preventing Negative Transfer'],\n",
       " ['647572d8d68f896efa7b7326',\n",
       "  'Instance-based Max-margin for Practical Few-shot Recognition'],\n",
       " ['64e2e15a3fda6d7f06466ae5',\n",
       "  'SimDA: Simple Diffusion Adapter for Efficient Video Generation.'],\n",
       " ['65499d90939a5f4082be9fdd',\n",
       "  'Asymmetric Masked Distillation for Pre-Training Small Foundation Models'],\n",
       " ['65518aaf939a5f4082a66ae7',\n",
       "  'Florence-2: Advancing a Unified Representation for a Variety of Vision Tasks'],\n",
       " ['659e222e939a5f408289b50a',\n",
       "  'Low-Resource Vision Challenges for Foundation Models'],\n",
       " ['65e7dcad13fb2c6cf6fdc8bb',\n",
       "  'Dynamic Adapter Meets Prompt Tuning: Parameter-Efficient Transfer Learning for Point Cloud Analysis'],\n",
       " ['65e9201d13fb2c6cf61fc6d1',\n",
       "  'LEAD: Learning Decomposition for Source-free Universal Domain Adaptation'],\n",
       " ['65ea7fa613fb2c6cf6260b0c',\n",
       "  'Delving into the Trajectory Long-tail Distribution for Muti-object Tracking'],\n",
       " ['65f3abe013fb2c6cf661337c',\n",
       "  'GroupContrast: Semantic-aware Self-supervised Representation Learning for 3D Understanding'],\n",
       " ['65f8f1b013fb2c6cf667445f',\n",
       "  'Boosting Order-Preserving and Transferability for Neural Architecture Search: a Joint Architecture Refined Search and Fine-tuning Approach'],\n",
       " ['65fc055d13fb2c6cf6df2276',\n",
       "  'GLID: Pre-training a Generalist Encoder-Decoder Vision Model']]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "/query_by_keyword\n",
    "通过关键词查询论文ID和标题。\n",
    "返回值: 论文ID和标题列表。 每个元素是一个元组 (paper_id, title)。\n",
    "\"\"\"\n",
    "results = kb.query_by_keyword(\"Transfer Learning\")\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env-mlx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
