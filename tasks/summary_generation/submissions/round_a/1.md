# Survey on Loss Functions in Machine Learning and Deep Learning

Loss functions are fundamental to the training and performance of machine learning and deep learning models. They serve as a guide for the learning process, ensuring that the model achieves the desired objectives. This survey explores various perspectives on loss functions, focusing on their formulations, approximations, and applications in different contexts.

## Wasserstein GAN with Gradient Penalty

The Wasserstein GAN with Gradient Penalty (WGAN-GP) is an approach that combines the Wasserstein GAN loss with a gradient penalty to stabilize training. The losses for the generator ($\mathcal{L}_{G}$) and discriminators ($\mathcal{L}_{F_{p}}$ and $\mathcal{L}_{F}$) are carefully designed to balance the generation and discrimination processes, ensuring that the gradients remain within a bounded range<sup>1</sup>.

## Cost Volume, Disparity, and Disparity Gradient Losses

In the context of stereo matching, the loss function comprises three main components: cost volume loss ($\mathcal{L}_{c v}$), disparity loss ($\mathcal{L}_{d i s p}$), and disparity gradient loss ($\mathcal{L}_{g r a d}$). The cost volume loss uses a contrastive loss to maximize the similarity for ground truth disparities while penalizing incorrect disparities. The disparity loss further refines this by incorporating completion, space refinement, and gradient-guided propagation losses<sup>2</sup>.

## Resolvent Moments in Loss Functional

For kernel ridge regression (KRR), the loss functional is derived using resolvent moments. The loss is computed for both noiseless and noisy scenarios, providing a comprehensive framework for understanding the impact of noise on the learning process<sup>3</sup>.

## CondInst Loss Function

The CondInst model employs a composite loss function that includes the original FCOS loss ($L_{f c o s}$), instance mask loss ($L_{m a s k}$), and panoptic segmentation loss ($L_{p a n o}$). Each component is designed to address specific aspects of the segmentation task, ensuring accurate classification, bounding box regression, and mask generation<sup>4</sup>.

## Importance of Each Loss Term

The proposed loss function in this context is broken down into multiple terms, each penalizing different aspects of the model's performance. This includes terms for suboptimal solutions, success rates, consistency, and regularization, highlighting the importance of a well-structured loss function in achieving robust model performance<sup>5</sup>.

## CAST NATURAL GRADIENT CL INTO THE GENERAL FRAMEWORK

The paper discusses the approximation of loss terms in a natural gradient framework. The first loss term, $\mathcal{L}_{C E}(\pmb{\theta})$, is approximated using a first-order Taylor expansion. The second loss term involves the cross entropy loss on previously learned tasks, approximated using a second-order Taylor expansion. The third loss term is a regularization term given by $\Psi=\frac{1}{2}||\pmb{\theta}||^{2}$. These approximations help in simplifying the optimization process by focusing on the most significant terms<sup>6</sup>.

## A BLATION STUDY ON LOSS TERMS

The study investigates the impact of different loss terms: pixel-wise $\ell_{2}$ distance (MSE), pixel-wise $\ell_{1}$ distance (MAE), and perceptual loss. Experiments are conducted on FlatFault using 24,000 unlabeled data. Quantitative results show that combining these loss terms improves performance metrics such as VelocityError and SeismicError. Visual results illustrate the differences in reconstructed seismic data and predicted velocity maps, highlighting the reduction in artifacts when using combined loss functions<sup>7</sup>.

## BOUND ON LOSS

The paper presents a bound on the loss function expressed as a weighted sum of 2-norms. Lemma and proof demonstrate that setting $\eta_{k}\leq\frac{4}{n m_{k}}$ or $\sum_{k}\eta_{k}\leq\frac{4}{\bar{m}}$ ensures $0\leq\mathcal{L}(x)\leq1$ for all $x\in\mathscr{X}$. This bound is crucial for maintaining the loss within a manageable range, facilitating easier analysis and optimization<sup>8</sup>.

## FISHER SAM ILLUSTRATION: TOY 2D EXPERIMENTS

The paper introduces a synthetic 2D parameter space to demonstrate the advantages of the proposed Fisher SAM (FSAM) over previous SAM and ASAM. The model is a univariate Gaussian $p(x;\theta)=\mathcal N(x;\mu,\sigma^{2})$ with $\theta=(\mu,\sigma)\in\mathbb{R}\times\mathbb{R}_{+}\subset\mathbb{R}^{2}$. The loss function is defined as a negative log-mixture of two KL-driven energy models. The contour map of $l(\theta)$ reveals two minima, with a preference for the flat minimum due to its stability<sup>9</sup>.

## LOSS FUNCTION IN IMAGE FUSION

The total loss function for image fusion combines multiple loss types: image fusion loss $\mathcal{L}_{\sf MSE}$, gradient loss $\mathcal{L}_{\mathrm{grad}}$, and segmentation loss $\mathcal{L}_{\mathrm{s}}$. The structural similarity index (SSIM) is incorporated to preserve overall structures. The saliency-based pixel loss is formulated as $\mathcal{L}_{\mathtt{MSE}}=\|\mathbf{u}-m_{1}\mathbf{x}\|_{2}^{2}+\|\mathbf{u}-m_{2}\mathbf{y}\|_{2}^{2}$. Gradient information is used to characterize texture details. The combined loss function $\mathcal{L}_{\math}$ ensures that the fused image retains vital intensity and structural details from the source images<sup>10</sup>.

## References

[1]: Point Cloud Part Editing: Segmentation, Generation, Assembly, and Selection, chunk 3
[2]: Temporally Consistent Stereo Matching, chunk 3
[3]: Generalization Error of Spectral Algorithms., chunk 16
[4]: Instance and Panoptic Segmentation Using Conditional Convolutions, chunk 5
[5]: Improving Test-Time Adaptation Via Shift-agnostic Weight Regularization and Nearest Source Prototypes, chunk 7
[6]: A Unified and General Framework for Continual Learning, chunk 7
[7]: Unsupervised Learning of Full-Waveform Inversion: Connecting CNN and Partial Differential Equation in a Loop, chunk 4
[8]: Approximating Nash Equilibria in Normal-Form Games Via Stochastic Optimization, chunk 7
[9]: Fisher SAM: Information Geometry and Sharpness Aware Minimisation., chunk 3
[10]: Multi-interactive Feature Learning and a Full-time Multi-modality Benchmark for Image Fusion and Segmentation, chunk 2