# Enhancing Planning Capabilities of Large Models: A Comparative Survey

## Introduction

The rapid advancement in artificial intelligence has led to the development of large models with exceptional capabilities. However, planning, which is a crucial aspect of decision-making and task execution, remains a challenge. This survey explores various methods proposed to enhance the planning capabilities of large models, discussing their respective advantages and disadvantages.

## Internal Working Memory Module

One approach to mitigate the forgetting phenomenon inherent in large language models (LLMs) is the internal working memory module, inspired by the human brain's distributed memory storage. This module is designed to store, blend, and retrieve information for different downstream tasks, thereby improving training efficiency and generalization. It has been demonstrated effective in Atari games and meta-world object manipulation tasks. However, the complexity of implementing and integrating such a memory module could be a potential drawback <sup>1</sup>.

## Plan-Seq-Learn (PSL)

PSL is a modular approach that uses motion planning to bridge the gap between abstract language and learned low-level control for solving long-horizon robotics tasks. It leverages internet-scale knowledge from LLMs for high-level policies, guiding reinforcement learning (RL) policies. PSL achieves state-of-the-art results on over 25 challenging robotics tasks and solves long-horizon tasks from raw visual input with high success rates. However, the integration of multiple modules could introduce complexity in implementation and tuning <sup>2</sup>.

## Adapting Black-Box LMs with Domain Experts

This method involves fine-tuning a small domain expert white-box LM and combining it with a large black-box LM at the parameter level during inference. It enhances the performance of black-box LMs in specific domains and is adaptable to existing powerful models. However, it relies on the availability and quality of domain-specific data for fine-tuning the small LM and could pose challenges in integrating outputs from two different models <sup>3</sup>.

## Reading Comprehension for Domain-Specific Adaptation

This method involves continued pre-training on domain-specific corpora, transforming raw texts into reading comprehension texts enriched with tasks related to their content. It mimics human learning via reading comprehension and consistently enhances performance across various tasks in different domains. However, it requires extensive domain-specific data for pre-training and could be resource-intensive <sup>4</sup>.

## AGVM (Adaptive Gradient Virtual Memory)

AGVM enables module-wise learning rate scaling, allowing for large-batch training without performance drop in dense visual prediction tasks. It successfully scales batch size to over 10K while maintaining generalization performance and reduces training time significantly. However, it requires careful module partitioning, which may not be straightforward for all types of pipelines <sup>5</sup>.

## Composing Anchor and Augmenting Models

This approach involves combining an anchor model with a domain-specific augmenting model to enable new capabilities. It leverages existing specialized models but is computationally expensive and requires significant resources for pre-training or fine-tuning <sup>6</sup>.

## Model Compression Techniques

Model compression techniques like unstructured model pruning are used to reduce the size of neural networks while preserving their capabilities. Unstructured pruning shows strong potential to maintain model performance even with high compression rates. However, it may still show performance gaps on complex datasets and requires careful tuning to balance compression and performance <sup>7</sup>.

## Structured Pruning

Structured pruning is used to reduce training time and memory requirements, allowing the use of larger models while saving computational resources. However, it may not fully close the accuracy gap with unstructured pruning on complex datasets <sup>8</sup>.

## Analyzing Different Embedding Modules

Analyzing different embedding modules in MLP models can optimize model size and performance. However, variability in parameter counts can complicate model tuning and selection <sup>9</sup>.

## Comparing Aggressive Pruning and Scaling

Comparing the effects of aggressive pruning and scaling on model performance and computational efficiency reveals that scaling is often better for large models in terms of accuracy and throughput. However, pruning can achieve higher accuracy for smaller models and reduces model size, but aggressive pruning leads to underperformance in large models and complexity in determining the optimal pruning rate <sup>10</sup>.

## Conclusion

The methods proposed to enhance the planning capabilities of large models encompass a variety of techniques, each with its unique advantages and limitations. The choice of method depends on the specific requirements and constraints of the task at hand, balancing the trade-offs between performance, resource usage, and implementation complexity.

## References

[1]: Think Before You Act: Decision Transformers with Internal Working Memory., chunk 0
[2]: Plan-Seq-Learn: Language Model Guided RL for Solving Long Horizon Robotics Tasks, chunk 0
[3]: CombLM: Adapting Black-Box Language Models Through Small Fine-Tuned Models, chunk 4
[4]: Adapting Large Language Models Via Reading Comprehension, chunk 0
[5]: Large-batch Optimization for Dense Visual Predictions, chunk 7
[6]: LLM Augmented LLMs: Expanding Capabilities Through Composition, chunk 1
[7]: ECoFLaP: Efficient Coarse-to-Fine Layer-Wise Pruning for Vision-Language Models, chunk 1
[8]: Winning the Lottery Ahead of Time: Efficient Early Network Pruning., chunk 5
[9]: On Embeddings for Numerical Features in Tabular Deep Learning, chunk 6
[10]: Zero-TPrune: Zero-Shot Token Pruning Through Leveraging of the Attention Graph in Pre-Trained Transformers, chunk 9