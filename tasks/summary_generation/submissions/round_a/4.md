# Technological Development Path of Multimodal Large Models

The technological development path of multimodal large models encompasses a wide range of advancements and methodologies aimed at enhancing their capabilities across various domains. This survey delves into the key aspects of this development, focusing on the progress in large language models (LLMs), DPF frameworks, deep neural networks (DNNs), and large foundation models.

## Large Language Models (LLMs)

### Progress in LLMs
Recent years have witnessed significant progress in LLMs, with models like GPT-3, Bloom, PaLM, Megatron-Turing-530B, and Chinchilla achieving remarkable performance in complex reasoning tasks. These models, with their hundreds of billions of parameters, demonstrate human-like proficiency in various domains <sup>1</sup>. However, the closed-source nature of many powerful LLMs has limited accessibility and hindered further progress in the field. To address this issue, Meta's LLaMA provides a set of open-source models, promoting transparency and collaboration <sup>1</sup>.

### Instruction Tuning and Multimodal LLMs
Models like Alpaca, Vicuna, and LMFlow have shown impressive performance in instruction following and dialogue applications after instruction tuning, highlighting the potential of LLMs in interactive scenarios <sup>1</sup>. LLAVA and MiniGPT-4 have made breakthroughs in image and multimedia processing, enabling image-based robot system interactions and opening doors for applications in diverse fields <sup>1</sup>. These advancements underscore the evolving landscape of LLMs, moving towards increased openness, versatility, and multimedia capabilities <sup>1</sup>.

## DPF Frameworks

### Advancements in DPF
The DPF framework has evolved into a continuous, multi-frame affinity field model, enhancing spatiotemporal regularity and expressiveness without compromising compactness <sup>2</sup>. Incorporating the one-dimensional time domain into the network's input layer ensures temporal regularity, while the output layer's Degrees of Freedom (DOFs) manipulate the model's expressiveness without altering hidden variables <sup>2</sup>. Experimental results demonstrate the framework's effectiveness in point motion prediction and guided mesh alignment, surpassing baseline methods <sup>2</sup>. Future work includes exploring loss weight balancing, applying the method to dynamic scene reconstruction, and extending it to models simulating complex dynamics like fluid fields, with potential applications in medicine, aerodynamics, and physics <sup>2</sup>. This progress signifies a crucial step towards more complex and adaptable multimodal models <sup>2</sup>.

## Deep Neural Networks (DNNs)

### Effective Scaling of DNNs
While DNNs, particularly Transformers, have shown scalability, training larger models from scratch is inefficient and resource-intensive <sup>3</sup>. Knowledge inheritance and model expansion have been proposed to leverage the knowledge of smaller pre-trained models for effective scaling <sup>3</sup>. Knowledge inheritance allows large models to learn from smaller ones, while model expansion directly expands pre-trained models without performance degradation <sup>3</sup>. These methods significantly reduce training time and computational costs, addressing the inefficiency and environmental impact of training large models from scratch <sup>3</sup>. These approaches represent a strategic shift towards more sustainable and effective model scaling practices <sup>3</sup>.

## Large Foundation Models

### Domain-Specific Large Models
Large foundation models have gained attention across various domains, with pre-trained models like BERT, T5, GPT series, DINOV2, MAE, and ViT-22B demonstrating exceptional performance in natural language processing and visual understanding <sup>4</sup>. Efficient fine-tuning techniques have transferred these pre-trained models to various downstream tasks, consistently delivering high performance <sup>4</sup>. Although three-dimensional visual understanding holds significant importance, the lack of large-scale models in this area presents a challenge. However, recent progress in this domain, such as the development of LLAVA and MiniGPT-4, promises to bridge this gap and further advance multimodal large models <sup>1</sup>.

## Conclusion

The technological development path of multimodal large models is a multifaceted journey that integrates various innovative approaches and methodologies to enhance their capabilities. The progress in LLMs, DPF frameworks, DNNs, and large foundation models highlights the evolving landscape of multimodal AI research, moving towards more robust, efficient, and generalized models capable of handling complex tasks in diverse environments. Each step in this development path builds upon the previous, contributing to a comprehensive and evolving landscape of multimodal AI research.

## References

[1]: DetGPT: Detect What You Need Via Reasoning, chunk 1
[2]: Degrees of Freedom Matter: Inferring Dynamics from Point Trajectories, chunk 6
[3]: LEMON: Lossless Model Expansion., chunk 1
[4]: Any2Point: Empowering Any-modality Large Models for Efficient 3D Understanding, chunk 1